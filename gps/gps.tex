\section{Gaussian Processes}\label{Chapter1}
The aim of this chapter is to build up the theory behind GPs from the ground up. First, we shall review some essential theory from functional analysis on kernels and reproducing kernel Hilbert spaces which are not used in GPs but are used in a vast array of machine learning models, aptly named kernel machines. Afterward, we shall go through the underlying statistics that drive GP prediction and use it to form algorithms for both regression and classification tasks. Note that most of the theory presented here is only for real-values data sets although most the time complex-valued generalizations do exist.

\input{gps/kernels.tex}

\input{gps/rkhs.tex}

\input{gps/rbfk.tex}

\input{gps/kern_mach.tex}

\input{gps/gp_reg.tex}

\input{gps/gp_cls.tex}