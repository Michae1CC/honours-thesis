\section*{Gaussian Processes}\label{Chapter1}
The aim of this chapter is to explore the theory behind GPs. First, some essential theory from functional analysis on kernels and reproducing kernel Hilbert spaces will be reviewed which are not only used in GPs but are found in a vast array of machine learning models, aptly named kernel machines. Afterward, we shall go through the underlying statistics that drive GP prediction and use it to form algorithms for both regression and classification tasks. Note that most of the theory presented here is only for real-values data sets although most the time complex-valued generalizations do exist.

\input{gps/kernels.tex}

\input{gps/rkhs.tex}

\input{gps/rbfk.tex}

\input{gps/kern_mach.tex}

\input{gps/gp_reg.tex}

\input{gps/gp_cls.tex}