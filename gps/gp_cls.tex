\subsection{Gaussian Processes for Classification}\label{Section1.6}

As with most classification models, the Gaussian processes classifier (GPC) seeks an estimate for the joint probability $p \left( y , \bm{x} \right)$ where $\bm{x} \in \RR^d$ is an input, as in the regression case, but $y$ is now a class taking on a discrete and finite number of values $\left\{ \calC_i \right\}_{i=1}^C$. Using Baye's theorem the joint probability density can be decomposed into either $p \left( y \right) p \left( \bm{x} \mid y \right)$ or $p \left( \bm{x} \right) p \left( \bm{y} \mid \bm{x} \right)$ giving rise to the {\it generative} and {\it discriminative} approaches respectively \cite{RasmussenCarlEdward2006Gpfm}*{page 34}. The generative approach models the prior probabilities of each class, $p \left( \calC_i \right)$, as well as the class conditional probabilities for each input $p \left( \bm{x} \mid \calC_i \right)$ and computes the posterior as
\[
    p \left( y \mid \bm{x} \right) = \frac{ p \left( y \right) p \left( \bm{x} \mid y \right) }{ \sum_{i=1}^{C} p \left( \calC_i \right) p \left( \bm{x} \mid \calC_i \right) }.
\]
On the other hand, the discriminative method focuses on modelling $p \left( y \mid \bm{x} \right)$ directly. With both these paradigms at our disposal, which one would be preferred for our GPC? While there are strengths and weaknesses associated with both models, the discriminative approach is usually chosen as it has a rather attractive property of directly modeling what we require, that is $p \left( y \mid \bm{x} \right)$. Aditionally, the density estimation of $p \left( \bm{x} \mid \calC_i \right)$ using in the generative model presents a number of difficulties, especially for larger values of $d$. If we are only focused on classifying inputs, the generative approach could mean trying to solve a harder problem than what is necessary. For this reason we focus on GPCs that adopt the discriminative approach.

\subsubsection{Linear Models for Classification}\label{Section1.6.1}

We can start by reviewing linear models for the simplist form of classification, that is binary classification. Adopting the notation from SVM (see \Cref{Section1.4.1}) literature, the binary classification problem involves assigning an input $\bm{x}$ to a class of either $-1$ or $+1$. For a linear model likelihood can be formulated as
\begin{equation} \label{eq: GPC-lin-model-1}
    p \left( y=+1 \mid \bm{x} , \bm{w} \right) = \sigma \left( \langle \bm{x} , \bm{w} \rangle \right)
\end{equation}
given a weight vector $\bm{w}$ and where $\sigma (\bm{z})$ is chosen to be any sigmoid function, see \Cref{defe: sigmoid-function}.
\begin{defe}[Sigmoid Function] \label{defe: sigmoid-function}
    A sigmoid function is a monotonically increasing function mapping from $\RR$ to $\left[ 0,1 \right]$ \cite{RasmussenCarlEdward2006Gpfm}*{page 35}.
\end{defe}
In this text, the commonly used logistic function
\begin{equation} \label{eq: logistic-function}
    \sigma (z) = \frac{1}{1 + \exp (-z)}
\end{equation}
will take the role of the sigmoid function in equation \ref{eq: GPC-lin-model-1}, graphed in Figure \ref{fig: logistic-func-and-probit}. This type of model is aptly named the logistic regression.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[>=latex, scale=1.1]
        \begin{axis}[
                axis line style = thick,
                xlabel={$x$},
                ylabel={$y$},
                every axis x label/.style={
                        at={(ticklabel* cs:1.01)},
                        anchor=west,
                    },
                xmin=-5.5,xmax=5.5,
                ymin=-0.125,ymax=1.125,
                axis y line =middle,
                axis x line =bottom,
                every axis y label/.style={
                        at={(ticklabel* cs:1.01)},
                        anchor=south,
                    },
                % axis line style={->},
                xticklabels={$-5$, $5$},
                xtick={-5,5},
                yticklabels={$0$, $1$},
                ytick={-0.05,0.95},
                % axis line style={draw=none},
                ytick style={draw=none},
            ]
            \addplot[smooth, color=red, thick] {1/(1+exp(-x))};
            \addplot [smooth, blue, thick, dashed] {normcdf((0.6266 * x),0,1)};
            \addplot[smooth, color=black, semithick, dotted] {0};
            \addplot[smooth, color=black, semithick, dotted] {1};
        \end{axis}
    \end{tikzpicture}
    \caption{The logistic function from equation \ref{eq: logistic-function} (solid red) juxtaposed with a close approximation, the scaled probit function (dashed blue).}
    \label{fig: logistic-func-and-probit}
\end{figure}
Unlike GPR, the likelihood is no longer a Gaussian distribution. Instead it follows the Bernoulli distribution
\begin{equation*}
    p \left( y \mid \bm{x} , \bm{w} \right) = \sigma \left( \langle \bm{x} , \bm{w} \rangle \right)^{y} \left( 1 - \sigma \left( \langle \bm{x} , \bm{w} \rangle \right) \right)^{\frac{1 - y}{2}}
\end{equation*}
which for symmeteric likelihood functions can be written more concisely as
\begin{equation*}
    p \left( y_i \mid \bm{x}_i , \bm{w} \right) = \sigma \left( y_i f_i \right)
\end{equation*}
where
\begin{equation} \label{eq: GPC-lin-latent-func}
    f_i \triangleq f \left( \bm{x}_i \right) = \langle \bm{x} , \bm{w} \rangle .
\end{equation}
Thus, the logistic regression model can be written as the log ratio of the likelihoods of the input belonging to either class, that is
\begin{equation*}
    \operatorname{logit} \left( \bm{x} \right) \triangleq \langle \bm{x} , \bm{w} \rangle = \log \left( \frac{p \left( y = +1 \right)}{p \left( y = -1 \right)} \right)
\end{equation*}
where $\operatorname{logit}$ is commonly referred to as the logit transformation \cite{RasmussenCarlEdward2006Gpfm}*{page 37}. For a given dataset $\calD = \left\{ \left( x_i , y_i \right) \right\}_{i=1}^{n}$ we assume each observation is independently generated conditioned over $f \left( \bm{x} \right)$. Similar to GPR, a Gaussian prior is used for the weights so that $\bm{w} \sim \calN \left( \bm{0} , \sigma_p \right)$ giving an un-normalised log posterior of
\begin{equation*}
    \log p \left( \bm{w} \mid \bm{X} , \bm{y} \right) \propto - \frac{1}{2} \bm{w}^{\intercal} \Sigma_p^{-1} \bm{w} + \sum_{i=1}^{n} \log \sigma \left( y_i f_i \right).
\end{equation*}

However, unlike GPR an analytic form for the mean and variance for the posterior is not available due to the non-Gaussian nature of the likelihood, although, when using the logistic function it is easy enough to show that the log likelihood is concave as a function of $\bm{w}$ for a fixed dataset. This means a number of numerical optimization techniques, such as Newton's method or the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm \cite{FletcherR2000PMoO} can be used to solve these values.

The idea behind Gaussian process classification for binary classes is that a Gaussian process prior is placed over a latent function $f \left( \bm{x} \right)$ where the output is then "squashed" through a sigmoid function to obtain a prior on
\begin{equation*}
    \pi \left( \bm{x} \right) \triangleq p \left( y=+1 \mid \bm{x} \right) = \sigma \left( f \left( \bm{x} \right) \right).
\end{equation*}
This construction is illustrated in Figure \ref{fig: latent-func-and-sig-trans} and provides a natural extension to the linear logistic regression model.

\begin{filecontents*}{./data/gpr_latent_fig1.csv}
    x,f,sig
    0.0,-0.716636450452456,0.3281340890410802
    0.05555555555555555,-0.3872677518321085,0.4043752064908497
    0.1111111111111111,-0.04169388251654786,0.48957803910434766
    0.16666666666666666,0.30748545475025907,0.5762713707991717
    0.2222222222222222,0.6464740783985357,0.6562154650063955
    0.2777777777777778,0.961561586658563,0.7234343518272932
    0.3333333333333333,1.2404054048798008,0.7756345729728175
    0.38888888888888884,1.4731593678003678,0.8135371194857097
    0.4444444444444444,1.653300565910448,0.8393366325299798
    0.5,1.77804434211388,0.8554552137616092
    0.5555555555555556,1.8482947284219486,0.8639267602259872
    0.611111111111111,1.8681477880300634,0.8662438156048382
    0.6666666666666666,1.844024635176618,0.8634239990093878
    0.7222222222222222,1.7835778456043871,0.8561380953611345
    0.7777777777777777,1.6945293075464642,0.8448188796360699
    0.8333333333333333,1.5836227815051651,0.8297169804398394
    0.8888888888888888,1.4558461620643777,0.8108965338173831
    0.9444444444444444,1.3140273314869115,0.7881862938214962
    1.0,1.1588551707012498,0.7611246311602722
    1.0555555555555556,0.9893037805660674,0.7289503840440278
    1.1111111111111112,0.8033735795719339,0.690695659627058
    1.1666666666666665,0.5990205172125318,0.6454321840431138
    1.222222222222222,0.37510931242033974,0.5926929891152688
    1.2777777777777777,0.13223323848313873,0.5330102232552971
    1.3333333333333333,-0.12672784735928108,0.46836037106151224
    1.3888888888888888,-0.3964103237476957,0.40217510289406855
    1.4444444444444444,-0.6691517706610377,0.33868679954793734
    1.5,-0.9355408280853343,0.28180195391419693
    1.5555555555555554,-1.1852457987537584,0.23411029892781032
    1.611111111111111,-1.408010271880984,0.196548078418454
    1.6666666666666665,-1.5946766330839104,0.16872694438891356
    1.722222222222222,-1.7381092175623238,0.1495532583157629
    1.7777777777777777,-1.8339034326105361,0.13777391959330848
    1.8333333333333333,-1.8808066358911115,0.13229624915295843
    1.8888888888888888,-1.8808148704868526,0.13229530387403557
    1.9444444444444444,-1.8389575421436446,0.1371746288812035
    2.0,-1.762808219338528,0.14643897890898566
    2.0555555555555554,-1.6617924461232692,0.15952152909513476
    2.111111111111111,-1.5463718876794204,0.17561089816790565
    2.1666666666666665,-1.4271799785358956,0.1935384565574492
    2.2222222222222223,-1.3141764578300106,0.21178881080529863
    2.2777777777777777,-1.2158759418012686,0.22866302240021388
    2.333333333333333,-1.1386790564398872,0.24256297036803995
    2.388888888888889,-1.0863393391090705,0.2523082312707144
    2.444444444444444,-1.0595738916176571,0.2573908929832913
    2.5,-1.0558426697384702,0.25810472715213173
    2.5555555555555554,-1.0693030523966423,0.2555356469279933
    2.611111111111111,-1.0909711640146635,0.25143544599371426
    2.6666666666666665,-1.1090929165853411,0.24804003562062307
    2.722222222222222,-1.1097459725310912,0.24791825016606753
    2.7777777777777777,-1.0776532758488337,0.25395036925471315
    2.833333333333333,-0.997190360006935,0.26949418860886254
    2.888888888888889,-0.853523705726157,0.2986942026351552
    2.944444444444444,-0.6338078056665624,0.34664763152253814
    3.0,-0.3283465008205865,0.4186429992846845
    3.0555555555555554,0.06838484205618314,0.5170895511116412
    3.1111111111111107,0.5569529512258093,0.6357472192793857
    3.1666666666666665,1.132487738026538,0.7562977100837649
    3.222222222222222,1.7845801849827891,0.8562615050851906
    3.2777777777777777,2.49755753518847,0.9239704166376049
    3.333333333333333,3.2511324784312694,0.9627137853569252
    3.388888888888889,4.021376394857483,0.9823874902884194
    3.444444444444444,4.781943394168808,0.9916899377360553
    3.5,5.505458755235512,0.995951929944337
    3.5555555555555554,6.164984744717579,0.9979026576630112
    3.6111111111111107,6.735466982435726,0.9988133894314339
    3.6666666666666665,7.1950796833818895,0.9992502941814919
    3.722222222222222,7.526406822593906,0.9994616196879862
    3.7777777777777777,7.717385130530989,0.9995551751969357
    3.833333333333333,7.761974187627159,0.9995745655656185
    3.888888888888889,7.660512373107247,0.9995291558112748
    3.944444444444444,7.419735623560392,0.9994010513870635
    4.0,7.052442559921735,0.999135455240024
    4.055555555555555,6.5768311951578315,0.9986096815828182
    4.111111111111111,6.0155205200837125,0.9975653642534463
    4.166666666666666,5.394318761388515,0.9954782255735908
    4.222222222222222,4.740803814055081,0.991343956562032
    4.277777777777778,4.082820560277176,0.9834196970094687
    4.333333333333333,3.446989970396544,0.9691412482325584
    4.388888888888888,2.857339498962172,0.9456968331619086
    4.444444444444445,2.334160434608953,0.911666952206589
    4.5,1.8931622621329902,0.8691156694032487
    4.555555555555555,1.544984634180779,0.8241881763838215
    4.611111111111111,1.295071693838633,0.7850043888236089
    4.666666666666666,1.1439101905755478,0.7583968311600912
    4.722222222222222,1.087555379614477,0.7479211041915879
    4.777777777777778,1.1183780186296264,0.7536877316301696
    4.833333333333333,1.2259536925758674,0.7731095912803282
    4.888888888888888,1.3979718665845051,0.8018618571626841
    4.944444444444444,1.6211061459040699,0.8349476243586275
    5.0,1.8817742817340295,0.8678147912195052
    5.055555555555555,2.1667472795664557,0.8972234089235315
    5.111111111111111,2.4636123746267415,0.9215512159171637
    5.166666666666666,2.761092162073636,0.9405367451793998
    5.222222222222222,3.049269824821768,0.9547509923004637
    5.277777777777778,3.3197364525186464,0.9650997156829392
    5.333333333333333,3.5657178376098226,0.9725009030361754
    5.388888888888888,3.7821806293058837,0.9777340834061939
    5.444444444444444,3.9659265896685927,0.9814019721867842
    5.5,4.115652418864887,0.983946623009968
\end{filecontents*}

\begin{figure}[h]
    \centering
    \subfloat[]{
        \begin{adjustbox}{width=0.48\textwidth}
            \begin{tikzpicture}[>=latex]
                \begin{axis}[
                        xmin=-0.0,xmax=6.5,
                        ymin=-4.0,ymax=10.0,
                        axis line style={draw=none},
                        tick style={draw=none},
                        yticklabels=\empty,
                        xticklabels=\empty,
                    ]
                    \addplot[smooth, color=red, thick] table [x=x, y=f, col sep=comma, mark=none] {./data/gpr_latent_fig1.csv};
                \end{axis}
                \draw[->,thick] (0,0.5)--(0,5.5) node[above]{$f(x)$};
                \draw[->,thick] (0,0.5)--(6,0.5) node[right]{$x$};

                \draw[dotted] (5.8,0.7)--(0,0.7) node[left]{$-4$};
                \draw[dotted] (5.8,5.1)--(0,5.1) node[left]{$9$};
            \end{tikzpicture}
        \end{adjustbox}
    }
    \subfloat[]{
        \begin{adjustbox}{width=0.48\textwidth}
            \begin{tikzpicture}[>=latex]
                \begin{axis}[
                        xmin=-0.0,xmax=6.5,
                        ymin=-0.05,ymax=1.15,
                        axis line style={draw=none},
                        tick style={draw=none},
                        yticklabels=\empty,
                        xticklabels=\empty,
                    ]
                    \addplot[smooth, color=red, thick] table [x=x, y=sig, col sep=comma, mark=none] {./data/gpr_latent_fig1.csv};
                \end{axis}
                \draw[->,thick] (0,0.5)--(0,5.5) node[above]{$\sigma(f(x)$)};
                \draw[->,thick] (0,0.5)--(6,0.5) node[right]{$x$};

                \draw[dotted] (5.8,0.7)--(0,0.7) node[left]{$0$};
                \draw[dotted] (5.8,5.1)--(0,5.1) node[left]{$1$};
            \end{tikzpicture}
        \end{adjustbox}
    }
    \caption{The latent function $f$, panel (A), is transformed using a sigmoid function, panel (B), to provide a probabilistic interpretation of $x$ belonging to the class $+1$.}
    \label{fig: latent-func-and-sig-trans}
\end{figure}
Specifically, the linear model from equation \ref{eq: GPC-lin-latent-func} is replaced with a GPR model and the Gaussian prior on the weights with a GPR weight prior with
\begin{equation*}
    p \left(
    \begin{bmatrix}
            \bm{f} \\
            f_{\star}
        \end{bmatrix}
    \right)
    =
    \calN \left( \bm{0} ,
    \begin{bmatrix}
        \bm{K_{XX}}         & \bm{K_{x^{\star}X}}^{\intercal}                 \\
        \bm{K_{x^{\star}X}} & k \left( \bm{x}_{\star}, \bm{x}_{\star} \right)
    \end{bmatrix}
    \right)
\end{equation*}
where $f_{\star} = f ( \bm{x}_{\star} )$ and $\bm{f} = f \left( \bm{X} \right)$. For classification tasks, we assume that each observation has received the correct label which is why no noise is added to the covariance matrix.

Note that values of $f$ are also never observed within the phenomena we are modelling, nor are we particularly interested in them. The function $f$ serves the role of a {\it nuisance function} and acts solely as a convenience tool within our formulations. The ultimate goal is to make predictions for $\pi$, not $f$, and the goal of the coming sections will be to eventually integrate out $f$.

Subsequently, predictions for $\pi_{\star} = \pi \left( \bm{x}_{\star} \right)$ are made by average over all possible latent functions weighted by the posterior giving the prediction
\begin{equation} \label{eq: GP-pred-1}
    \overline{\pi_{\star}} \triangleq p \left( y_{\star} = +1 \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right) = \int \sigma \left( f_{\star} \right) p \left( f_{\star} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right) \; d f_{\star}
\end{equation}
While this is a sound model, computing predictions is not so straight forward since the integral in \ref{eq: GP-pred-1} is not analytically tractable for the same reason as the linear binary classifier. Later on we will see how we can make use of our numerical toolbox to derive a good approximation for $\overline{\pi_{\star}}$.

\subsubsection{Lapace Approximation for Posterior}\label{Section1.6.2}

We saw that the integral in \ref{eq: GP-pred-1} could not be used to make predictions for $\overline{\pi_{\star}}$ analytically. In this section we shall address how the distribution for the latent process, $p \left( f_{\star} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right)$, can be approximated to provide a numerically tractable succedaneum. Using Baye's theorem
\begin{align*}
    p \left( f_{\star} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right)
     & = \int p \left( f_{\star} , \bm{f} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right) \; d \bm{f}                                                                                                                                                       \\
     & = \frac{1}{p \left( \bm{y} \mid \bm{X} , \bm{x}_{\star} \right)} \int p \left( f_{\star} \mid \bm{X} , \bm{x}_{\star}, \bm{f} \right) p \left( \bm{f} \mid \bm{X} \right) p \left( \bm{y} \mid \bm{X} , \bm{x}_{\star}, \bm{f} \right) \; d \bm{f} \\
     & = \int p \left( f_{\star} \mid \bm{X} , \bm{x}_{\star}, \bm{f} \right) p \left( \bm{f} \mid \bm{X} , \bm{y} \right) \; d \bm{f}
\end{align*}
using the fact that $p \left( \bm{y} \mid \bm{X} , \bm{x}_{\star}, \bm{f}, f_{\star} \right) = p \left( \bm{y} \mid \bm{X} , \bm{x}_{\star}, \bm{f} \right)$ \cite{BishopChristopherM2006Pram, RasmussenCarlEdward2006Gpfm}. The conditional distribution $p \left( f_{\star} \mid \bm{X} , \bm{x}_{\star}, \bm{f} \right)$ can be derived as
\begin{equation*}
    p \left( f_{\star} \mid \bm{X} , \bm{x}_{\star}, \bm{f} \right) = \calN \left( f_{\star} \mid \bm{K}_{\bm{x}_{\star}\bm{X}} \bm{K}_{\bm{X} \bm{X}}^{-1} \bm{y}, k \left( \bm{x}_{\star}, \bm{x}_{\star} \right) - \bm{K}_{\bm{x}_{\star}\bm{X}} \bm{K}_{\bm{X} \bm{X}}^{-1} \bm{K}_{\bm{x}_{\star} \bm{X}}^{\intercal} \right)
\end{equation*}
through the use of equation \ref{eq: GP_train_distr2_mean} and \ref{eq: GP_train_distr2_var}. Unfortunately
\begin{equation*}
    p \left( \bm{f} \mid \bm{X} , \bm{y} \right) = \frac{p \left( \bm{y} \mid \bm{f} \right) p \left( \bm{f} \mid \bm{X} \right) }{p \left( \bm{y} \mid \bm{X} \right)}
\end{equation*}
does not follow a Gaussian distribution. Instead we can use a Lapace approximation to estimate $p \left( \bm{f} \mid \bm{X} , \bm{y} \right)$ as a Gaussian distribution. Breifly, the Lapace approximation works by assuming the distribution at hand, $p \left( \bm{z} \right)$, can be modelled as
\begin{equation*}
    p \left( \bm{z} \right) = \frac{1}{c} q \left( \bm{z} \right)
\end{equation*}
where $q \left( \bm{z} \right)$ is multivariate Gaussian and $c$ is some normalization constant \cite{BishopChristopherM2006Pram}*{page 214}. To do this, first the centre of $q \left( \bm{z} \right)$ is placed at the mode of $p \left( \bm{z} \right)$. The mode of $p \left( \bm{z} \right)$ is
\begin{equation*}
    \bm{z}_0 = \argmin_{\bm{z}} p \left( \bm{z} \right)
\end{equation*}
which can be computed by solving
\begin{equation} \label{eq: lapace-grad-zero}
    \nabla p \left( \bm{z}_0 \right) = \bm{0}.
\end{equation}
To ensure the covariance of the synthesized multivariate Gaussian behaves similar to the original distribution we can make use of an important property of the Gaussian distribution which is its logarithm being is a quadratic function of its inputs. Taking the Taylor series expansion of $\ln q \left( \bm{z} \right)$ centered at $\bm{z}_0$ yields
\begin{equation*}
    \ln q \left( \bm{z} \right) \simeq \ln q \left( \bm{z}_0 \right) - \frac{1}{2} \left( \bm{z} - \bm{z}_0 \right)^{\intercal} \bm{A} \left( \bm{z} - \bm{z}_0 \right)
\end{equation*}
where
\begin{equation*}
    \bm{A} = - \nabla \nabla \left. \ln q \left( \bm{z} \right) \right|_{\bm{z} = \bm{z}_0}.
\end{equation*}
Expotentiating both sides gives
\begin{align}
    q \left( \bm{z} \right)
     & \simeq q \left( \bm{z}_0 \right) \exp \left( - \frac{1}{2} \left( \bm{z} - \bm{z}_0 \right)^{\intercal} \bm{A} \left( \bm{z} - \bm{z}_0 \right) \right) \nonumber \\
     & \propto \calN \left( \bm{z} \mid \bm{z}_0 , \bm{A}^{-1} \right) \label{eq: lapace-gauss-apprx} .
\end{align}
Returning to our original problem of estimating $p \left( \bm{f} \mid \bm{X} , \bm{y} \right) \propto p \left( \bm{y} \mid \bm{f} \right) p \left( \bm{f} \mid \bm{X} \right)$ as a Gaussian distribution, the prior $p \left( \bm{f} \mid \bm{X} \right)$ follows a Gaussian distribution with zero mean and covariance $\bm{K}_{\bm{X} \bm{X}}$ and the distribution of $p \left( \bm{y} \mid \bm{f} \right)$ (assuming independence of samples) can be written as
\begin{equation*}
    p \left( \bm{y} \mid \bm{f} \right) = \prod_{i=1}^{n} \sigma \left( y_i f_i \right).
\end{equation*}
To find a Laplace approximation for $p \left( \bm{f} \mid \bm{X} , \bm{y} \right)$ we only need to consider an unnormalized posterior when maximizing with respect to $\bm{f}$ since $p \left( \bm{y} \mid \bm{f} \right)$ does not depend on $\bm{f}$. Thus, the log of the unnormalized posterior is
\begin{align*}
    \Psi \left( \bm{f} \right)
     & \triangleq \ln p \left( \bm{y} \mid \bm{f} \right) + \ln p \left( \bm{f} \mid \bm{X} \right)                                                                                                                                \\
     & = - \sum_{i=1}^{n} \ln \left( 1 + \exp \left( y_i f_i \right) \right) - \frac{1}{2} \bm{f}^{\intercal} \bm{K}_{\bm{X} \bm{X}}^{-1} \bm{f} - \frac{1}{2} \ln \left| \bm{K}_{\bm{X} \bm{X}} \right| - \frac{n}{2} \ln 2 \pi .
\end{align*}
The gradient and Hessian of the unnormalized posterior then becomes
\begin{align*}
    \nabla \Psi \left( \bm{f} \right)        & = \nabla \ln p \left( \bm{y} \mid \bm{f} \right) - \bm{K}_{\bm{X} \bm{X}}^{-1} \bm{f} = \left( \bm{t} - \bm{\pi} \right) - \bm{K}_{\bm{X} \bm{X}}^{-1} \bm{f} \\
    \nabla \nabla \Psi \left( \bm{f} \right) & = \nabla \nabla \ln p \left( \bm{y} \mid \bm{f} \right) - \bm{K}_{\bm{X} \bm{X}}^{-1} = - \bm{W} - \bm{K}_{\bm{X} \bm{X}}^{-1}
\end{align*}
where $\pi_i = p \left( y_i = +1 \mid f_i \right) = \sigma ( f_i )$, $\bm{t} = \left( \bm{y} + \bm{1} \right) / 2 \in \RR^{n}$ and $\bm{W} \triangleq - \nabla \nabla \ln p \left( \bm{y} \mid \bm{f} \right)$ is a diagonal matrix (since the distribution of $y_i$ only depends on $f_i$ and not $f_{j \neq i}$) with entries $\bm{W}_{ii} = \sigma \left( y_i f_i \right)$ \cite{BishopChristopherM2006Pram, RasmussenCarlEdward2006Gpfm}. From equation \ref{eq: lapace-grad-zero}, the mode of $\hat{\bm{f}}$ of $\bm{\Psi}$ can be computed as
\begin{align}
    \nabla \Psi \left( \hat{\bm{f}} \right) & = \bm{0} = \left( \bm{t} - \bm{\pi} \right) - \bm{K}_{\bm{X} \bm{X}}^{-1} \hat{\bm{f}} \nonumber \\
    \iff \hat{\bm{f}}                       & = \bm{K}_{\bm{X} \bm{X}} \left( \bm{t} - \bm{\pi} \right) \label{eq: expr-for-mode-lapace} .
\end{align}
Since $\bm{t} - \bm{\pi}$ is a non-linear function, a non-linear optimization technique method is required to solve $\hat{\bm{f}}$ in \ref{eq: expr-for-mode-lapace}. Since the Hessian of $\Psi \left( \bm{f} \right)$ is available, Newton's method is typically employed as fast iterative method to approximate $\hat{\bm{f}}$ where $\hat{\bm{f}}$ is updated as
\begin{equation*}
    \hat{\bm{f}}^{\; \text{new}} = \bm{K}_{\bm{X} \bm{X}} \left( \Id_{n \times n} + \bm{W} \bm{K}_{\bm{X} \bm{X}} \right)^{-1} \left( \bm{W} \hat{\bm{f}}^{\; \text{old}} + \nabla \ln \left( \bm{y} \mid \hat{\bm{f}}^{\; \text{old}} \right) \right).
\end{equation*}
Once a suitable mode is found, using equation \ref{eq: lapace-gauss-apprx}, the Lapacian approximation for $p \left( \bm{f} \mid \bm{X} , \bm{y} \right)$ becomes
\begin{equation} \label{eq: lapace-apprx}
    p \left( \bm{f} \mid \bm{X} , \bm{y} \right) \simeq q \left( \bm{f} \mid \bm{X} , \bm{y} \right) = \calN \left( \hat{\bm{f}} , \left( \bm{K}_{\bm{X} \bm{X}}^{-1} + \bm{W} \right)^{-1} \right).
\end{equation}

\subsubsection{Predictions}\label{Section1.6.3}

With the Lapace approximation for $p \left( \bm{f} \mid \bm{X} , \bm{y} \right)$ (equation \ref{eq: lapace-apprx}) and an exact probability distribution for $p \left( f_{\star} \mid \bm{X} , \bm{x}_{\star}, \bm{f} \right)$, a mean for the latent process, $p \left( f_{\star} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right)$, can now be computed by invoking \ref{eq: GP_train_distr2_mean} to give
\begin{align}
    \mu_{f_{\star}} = \EE \left[ f_{\star} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right]
     & = \bm{K}_{\bm{x}_{\star} \bm{X}} \bm{K}_{\bm{X} \bm{X}}^{-1} \hat{\bm{f}}           \nonumber               \\
     & = \bm{K}_{\bm{x}_{\star} \bm{X}} \nabla \ln \left( \bm{y} \mid \hat{\bm{f}} \right) \nonumber               \\
     & = \bm{K}_{\bm{x}_{\star} \bm{X}} \left( \bm{t} - \bm{\pi} \right) \label{eq: latent-process-mean-apprx-1} .
\end{align}
Similarly, the variance can be computed using equation \ref{eq: GP_train_distr2_var} to give
\begin{align} \label{eq: latent-process-var-apprx-1}
    \sigma_{f_{\star}}^2 = \VV \left[ f_{\star} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right]
     & = k \left( \bm{x}_{\star} , \bm{x}_{\star} \right) - \bm{K}_{\bm{x}_{\star} \bm{X}} \left( \bm{K}_{\bm{X} \bm{X}} + \bm{W}^{-1} \right)^{-1} \bm{K}_{\bm{x}_{\star} \bm{X}}^{\intercal}.
\end{align}
Using equation \ref{eq: GP-pred-1}, predictions can now be made as
\begin{equation} \label{eq: pred-apprx-1}
    \overline{\pi_{\star}} \simeq \int \sigma \left( f_{\star} \right) q \left( f_{\star} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right) \; d f_{\star}
\end{equation}
where $q \left( f_{\star} \mid \bm{X} , \bm{y} , \bm{x}_{\star} \right)$ is a multivariate Gaussian distribution with mean and variance given by equations \ref{eq: latent-process-mean-apprx-1} and \ref{eq: latent-process-var-apprx-1} respectively. Notice that the prediction given in \ref{eq: pred-apprx-1} is a convolution of a Gaussian and logistic function which unfortunately cannot be evaluated analytically. However, Spiegelhalter and Lauritzen \cite{spiegelhalter1990sequential} show that a good approximation can be found by replacing the sigmoid function with the probit function $\Phi \left( \lambda a \right)$ which is simply the cumulative distribution function (CDF) of the standard Gaussian distribution. To get the best approximation using the probit function, the constant factor $\lambda$ is adjusted to equate their slopes at the origin. The value of $\lambda$ that gives this equality is $\lambda = \sqrt{\pi / 8}$. The similarity between the sigmoid function and probit function rescaled by a factor of $\sqrt{\pi / 8}$ is illustrated in Figure \ref{fig: logistic-func-and-probit}. The reason for replacing the sigmoid function with a probit function is that the convolution of a Gaussian distribution and probit function can be analytically evaluated as
\begin{equation} \label{eq: probit-int-apprx-1}
    \int \Phi \left( \lambda a \right) \calN \left( a \mid \mu , \sigma^2 \right) \; da = \Phi \left( \frac{\mu}{\left( \lambda^{-2} + \sigma^2 \right)^{\frac{1}{2}}} \right).
\end{equation}
Again apply the approximation $\sigma \left( a \right) \simeq \Phi \left( \lambda a \right)$ to left hand side of \ref{eq: probit-int-apprx-1} gives the following estimate for the convolution of a Gaussian and sigmoid function
\begin{equation} \label{eq: pred-apprx-2}
    \int \sigma \left( a \right) \calN \left( a \mid \mu , \sigma^2 \right) \; da \simeq \sigma \left( \frac{\mu}{\left( 1 + \pi \sigma^2 / 8 \right)^{\frac{1}{2}}} \right)
\end{equation}
\cite{BishopChristopherM2006Pram}*{page 219}. The integral used to approximate $\overline{\pi_{\star}}$ in \ref{eq: pred-apprx-1} can now be estimated using \ref{eq: pred-apprx-2} to give
\begin{equation*} \label{eq: pred-apprx-3}
    \overline{\pi_{\star}} = \sigma \left( \frac{\mu_{f_{\star}}}{\left( 1 + \pi \sigma_{f_{\star}}^2 / 8 \right)^{\frac{1}{2}}} \right).
\end{equation*}
This theory justifies Algorithm \ref{alg: Unoptimized_GPC} which creates predictions based on the GPC method.

    {\centering
        \begin{minipage}{.85\linewidth}
            \begin{algorithm}[H]
                \caption{Unoptimized GPC}
                \label{alg: Unoptimized_GPC}
                \SetAlgoLined
                \DontPrintSemicolon
                \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

                \Input{Observations $\bm{X}, \bm{y}$ and a test input $\bm{x}^{\star}$.}
                \Output{A prediction $\overline{f_{\star}} $ with its corresponding variance $ \VV \left[ f_{\star} \right]$.}
                \BlankLine
                $\bm{t} = \left( \bm{y} + \bm{1} \right) / 2$\;
                $\bm{f} = \bm{0}$\;
                \Repeat{convergence}{
                    $\bm{W} = \operatorname{diag} \left( \sigma \left( \bm{y} .^{\ast} \bm{f} \right) \right)$\;
                    $\bm{\alpha} = \operatorname{lin-solve} \left( \Id_{n \times n} + \bm{W} \bm{K}_{\bm{X} \bm{X}}, \bm{K}_{\bm{X} \bm{X}} \right)$\;
                    $\bm{f} = \bm{\alpha} \left( \bm{t} - \sigma (\bm{f}) + \bm{W} \bm{f} \right)$\;
                }
                $\mu_{f_{\star}} = \bm{K}_{\bm{x}_{\star} \bm{X}} \left( \bm{t} - \sigma (\bm{f}) \right)$\;
                $\sigma_{f_{\star}}^2 = k \left( \bm{x}_{\star} , \bm{x}_{\star} \right) - \bm{K}_{\bm{x}_{\star} \bm{X}} \left( \bm{K}_{\bm{X} \bm{X}} + \bm{W}^{-1} \right)^{-1} \bm{K}_{\bm{x}_{\star} \bm{X}}^{\intercal}$\;
                $\overline{\pi_{\star}} = \sigma \left( \mu_{f_{\star}} / {\left( 1 + \pi \sigma_{f_{\star}}^2 / 8 \right)^{\frac{1}{2}}} \right)$\;
                \Return{$\overline{\pi_{\star}} , \mu_{f_{\star}} , \sigma_{f_{\star}}^2$}
                \BlankLine
            \end{algorithm}
        \end{minipage}
        \par
    }