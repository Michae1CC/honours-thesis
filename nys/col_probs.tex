\subsection{Column Probabilities}\label{Section2.2}

Recall that the Nystrom method from \Cref{alg: nys-col-samp} is largely dependent on the random matrix multiplication algorithm (see \Cref{alg: rand-mat-mult}) to produce a suitable sketching matrix. Moreover, improvements in the sketching matrix produced by the random matrix multiplication algorithm are reflected as smaller errors in the Nystrom approximation. Now, consider using the random matrix multiplication algorithm to approximate $\bm{A} \bm{A}^{\intercal}$ by setting $\bm{B} = \bm{A}$. The output is an approximation of the form
\begin{equation*}
    \bm{A} \bm{A}^{\intercal} \simeq \bm{C} \bm{C}^{\intercal} = \bm{C} \bm{R}.
\end{equation*}
The probability distribution
\begin{equation*} \label{eq: col-probs}
    p_i = \frac{\norm{\bm{A}_{(:,i)}}_2^2}{\norm{\bm{A}}_F}.
\end{equation*}
aims to minimize the error between $\bm{A} \bm{A}^{\intercal}$ and the approximation $\bm{C} \bm{C}^{\intercal}$. As a result, we should expect that $\bm{C}$ becomes a better estimate for $\bm{A} \bm{S}$, implying that the sketching matrix, $\bm{S}$, is using a better sampling and landmark selection criteria. Drineas and Mahoney give a precise bound on this error presented in \Cref{thm: col-pro-bounds} \cite{JMLR:v6:drineas05a}*{page 2158}.

\begin{thm} \label{thm: col-pro-bounds}
    Given $\bm{A} \in \RR^{n \times I}$, $1 \leq c \leq I$ and the probability distribution $\left\{ p_i \right\}_{i=1}^{I}$ described in \Cref{eq: col-probs}. Construct $\bm{C}$ using \Cref{alg: rand-mat-mult}, then
    \[
        \EE \left[ \norm{\bm{A} \bm{A}^{\intercal} - \bm{C} \bm{C}^{\intercal}}_{F} \right] \leq \frac{1}{\sqrt{c}} \norm{\bm{A}}^2_{F}
    \]
    \cite{JMLR:v6:drineas05a}*{page 2158}.
\end{thm}

To show \Cref{thm: col-pro-bounds}, we can actually prove something a little more general.

\begin{lem} \label{lem: col-pro-bounds-gen}
    Given $\bm{A} \in \RR^{n \times I}$, $\bm{B} \in \RR^{I \times m}$, $1 \leq c \leq I$ and the probability distribution $\left\{ p_i \right\}_{i=1}^{I}$ as follows
    \[
        p_i = \frac{\norm{\bm{A}_{(:,i)}}_2 \norm{\bm{B}_{(i,:)}}_2}{\sum_{j=1}^{I} \norm{\bm{A}_{(:,j)}}_2 \norm{\bm{B}_{(j,:)}}}.
    \]
    Construct $\bm{C}$ using \Cref{alg: rand-mat-mult}, using the probability distribution described above, then
    \[
        \EE \left[ \norm{\bm{A} \bm{B} - \bm{C} \bm{R}}_{F} \right] \leq \frac{1}{\sqrt{c}} \norm{\bm{A}}^2_{F} \norm{\bm{B}}^2_{F}.
    \]
    This choice of probability distribution minimises $\EE \left[ \norm{\bm{A} \bm{B} - \bm{C} \bm{R}}_{F} \right]$ among all possible sampling probabilites \cite{doi:10.1137/S0097539704442684}*{pages 9-12}.
\end{lem}

\begin{proof}
    First note that
    \[
        \EE \left[ \norm{\bm{A} \bm{B} - \bm{C} \bm{R}}_{F}^{2} \right]
        = \sum_{k=1}^{n} \sum_{j=1}^{m} \EE \left[ \left( \bm{A} \bm{B} - \bm{C} \bm{R}\right)_{kj}^{2} \right]
        = \sum_{k=1}^{n} \sum_{j=1}^{m} \VV \left[ \left( \bm{C} \bm{R} \right)_{kj} \right].
    \]
    Thus from \Cref{lem: rmm-exp-var-bds}, it follows that
    \begin{align*}
          & \EE \left[ \norm{\bm{A} \bm{B} - \bm{C} \bm{R}}^2_F \right]                                                                                                                     \\
        = & \frac{1}{c} \sum_{i=1}^{I} \frac{1}{p_i} \left( \sum_{k=1}^{n} \bm{A}_{ki}^{2} \right) \left( \sum_{j=1}^{m} \bm{B}_{ij}^{2} \right) - \frac{1}{c} \norm{\bm{A} \bm{B}}_{F}^{2} \\
        = & \frac{1}{c} \sum_{i=1}^{I} \frac{1}{p_i} \norm{\bm{A}_{(:,i)}}_{2}^{2} \norm{\bm{B}_{(i,:)}}_{2}^{2} - \frac{1}{c} \norm{\bm{A} \bm{B}}_{F}^{2}.
    \end{align*}
    Substituting in a probability of
    \[
        p_i = \frac{\norm{\bm{A}_{(:,i)}}_2 \norm{\bm{B}_{(i,:)}}_2}{\sum_{j=1}^{I} \norm{\bm{A}_{(j,:)}}_2 \norm{\bm{B}_{(:,j)}}}.
    \]
    yields
    \begin{align*}
        \EE \left[ \norm{\bm{A} \bm{B} - \bm{C} \bm{R}}^2_F \right] = & \frac{1}{c} \left( \sum_{i=1}^{I} \norm{\bm{A}_{(:,i)}}_{2} \norm{\bm{B}_{(i,:)}}_{2} \right)^{2} - \frac{1}{c} \norm{\bm{A} \bm{B}}_{F}^{2} \\
        \leq                                                          & \frac{1}{c} \norm{\bm{A}}^2_{F} \norm{\bm{B}}^2_{F}.
    \end{align*}
    To verify that this choice of probability distribution minimises $\EE \left[ \norm{\bm{A} \bm{B} - \bm{C} \bm{R}}_{F} \right]$ define the function
    \[
        f \left( p_1 , \ldots , p_n \right) = \sum_{i=1}^{I} \frac{1}{p_i} \norm{\bm{A}_{(:,i)}}_{2}^{2} \cdot \norm{\bm{B}_{(i,:)}}_{2}^{2}
    \]
    which characterises the dependence of $\EE \left[ \norm{\bm{A} \bm{B} - \bm{C} \bm{R}}_{F} \right]$ on the probability distribution. To minimise $f$ subject to $\sum_{i=1}^{I} p_i = 1$, we introduce the Lagrange multiplier $\lambda$ and define the function
    \[
        g \left( p_1 , \ldots , p_n \right) = f \left( p_i , \ldots , p_n \right) + \lambda \left( \sum_{i=1}^{I} p_i - 1 \right).
    \]
    The minimum is then
    \[
        0 = \frac{\partial g}{\partial p_i} = - \frac{1}{p_i^2} \norm{\bm{A}_{(:,i)}}_{2}^{2} \cdot \norm{\bm{B}_{(i,:)}}_{2}^{2} + \lambda.
    \]
    Thus
    \[
        p_i = \frac{\norm{\bm{A}_{(:,i)}}_{2} \cdot \norm{\bm{B}_{(i,:)}}_{2}}{\sqrt{\lambda}} = \frac{\norm{\bm{A}_{(:,i)}}_{2} \cdot \norm{\bm{B}_{(i,:)}}_{2}}{\sum_{j=1}^{I} \norm{\bm{A}_{(j,:)}}_{2} \norm{\bm{B}_{(:,j)}}_{2}}
    \]
    where the second equality comes from solving for $\sqrt{\lambda}$ in $\sum_{i=1}^{I-1} p_i = 1$. These probabilities are indeed minimizing since $\frac{\partial^2 g}{\partial p_i^2} > 0$ for every $i$ such that $\norm{\bm{A}_{(:,i)}}_{2}^{2} \cdot \norm{\bm{B}_{(i,:)}}_{2}^{2} > 0$.
\end{proof}