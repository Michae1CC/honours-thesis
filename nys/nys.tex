\section{The Nystrom Method}\label{Chapter2}
In chapter \ref{Chapter1} we saw that GP regression and classification relied on a Gram matrix (see definition \ref{defe: Gram_Matrix}) to produce predictions. Unfortunately, from a computational perspective, constructing the Gram matrix for a data set $\calD = \left\{ \left( \bm{x}_i , y_{i} \right) \right\}_{i=1}^{n}$ brings about a nasty bottle neck owed by the $\calO \left( n^2 \right)$ kernel evaluations. Even before the rise of ML, there has been a lot of research devoted to creating numerical methods that quickly construct a low rank approximation of large matrices, $\bm{A}$, which ordinarily are a computational burdened to build exactly. These methods are centered around the idea of capturing the columns space of the matrix that best describes the the action of $\bm{A}$ as an operator. For lack of a better explanation, Mahoney gives a fantastic summary as to why the column space is of paramount importance in these approximation techniques
\begin{center}
    \emph{"To understand why sampling columns (or rows) from a matrix is of interest,recall that matrices are “about” their columns and rows that is, linear combinations are taken with respect to them; one all but understands a given matrix if one understands its column space, row space, and null
        spaces; and understanding the subspace structure of a matrix sheds a great deal of light on the linear transformation that the matrix represents."} \cite{DBLP:journals/corr/abs-1104-5557}*{page 13}
\end{center}
Moreover, this class of algorithms lend very nice forms when $\bm{A}$ possess positive definite structure, which is exactly the case for our Gram matrix.

\input{nys/nys_method.tex}

\input{nys/col_probs.tex}

\input{nys/lev_scrs.tex}