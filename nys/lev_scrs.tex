\subsection{Leverage Scores}\label{Section2.3}

\subsubsection{Statistical Leverage Scores}
Our next distribution originates from the least-squares problem. Breifly, in an over constrained least-squares problem, where $\bm{A} \in \RR^{n \times m}$, $\bm{b} \in \RR^{n}$, for $m \ll n$ there usually are not any $\bm{x} \in \RR^{m}$ for which $\bm{A} \bm{x} = \bm{b}$. Instead, alternative criteria must be employed to seek a $\bm{x}$ which in some way comes "closest" to satisfying this equality. Perhaps one of the more popular criterion is to minimize the $\ell^2-$norm, that is
\[
    \bm{x}_{opt} = \argmin_{x} \norm{\bm{A} \bm{x} - \bm{b}}
\]
\cite{DBLP:journals/corr/abs-1104-5557}*{page 19-21}. The optimal value for $\bm{x}$ can be solved as $\bm{x}_{opt} = \left( \bm{A}^{\intercal} \bm{A} \right)^{-1} \bm{A}^{\intercal} \bm{b}$. The least-squares solution is commonly used to find the best weight vector (in this case $\bm{x}$) for a linear model, given a dataset. Fitted or predicted values are usually obtained from $\hat{\bm{b}} = \bm{H} \bm{b}$ where the projector onto the column space of $\bm{A}$
\[
    \bm{H} = \bm{A} \left( \bm{A}^{\intercal} \bm{A} \right)^{-1} \bm{A}^{\intercal}
\]
is sometimes referred to as the {\it hat matrix}. The element $\bm{H}_{ij}$ has the direct interpretation as the influence or statistical leverage exerted on $\hat{\bm{b}}_i$. Thus, examining the hat matrix can reveal to us columns of $\bm{A}$ which bear a significant impact on $\hat{\bm{b}}$ \cite{HoaglinDavidC1978THMi}*{page 17}. Relatedly, if the element $\bm{H}_{ii}$ is particularly large this is indicative of the $i^{th}$ column of $\bm{A}$ having a strong influence over values of $\hat{\bm{b}}$, justifying the interpretation of $\bm{H}_{ii}$ as {\it statistical leverage scores}.

The statistical leverage scores are maximised when $\bm{A}_{(:,i)}$ is linearly independent from $\bm{A}$'s other columns and decreases when it aligns with many other columns or when the value of $\norm{\bm{A}_{(:,i)}}$ is small \cite{DBLP:journals/corr/CohenMM15}*{page 5}. To compute the statistical leverage scores, if $\bm{A} = \bm{U} \bm{\Sigma} \bm{V}^{\intercal}$ is the SVD of $\bm{A}$, then
\begin{align*}
    \bm{H}_{ii} & = \left( \bm{A} \left( \bm{A}^{\intercal} \bm{A} \right)^{-1} \bm{A}^{\intercal} \right)_{ii} \\
                & = \left( \bm{U} \bm{\Sigma}^{2} \left( \bm{\Sigma}^{2} \right)^{-1} \bm{U} \right)_{ii}       \\
                & = \norm{\bm{U}_{(i,:)}}_2^2.
\end{align*}
Note that $\bm{H}_{ii}$ may not constitute as a probability distribution, as may the other leverage scores which we will soon discuss. This is easily remedied through normalisation, in this case dividing each statistical leverage score by $\operatorname{tr} \left( \bm{H} \right)$. The idea behind using statistical leverage scores as a probability distribution in the Nystrom method is that they help prioritize selection of columns that are more linearly independent from other columns so that the range of our approximate better aligns with the range of our original $\bm{A}$.

\subsubsection{Rank$-k$ Statistical Leverage Scores}

We can generalize this notion of statistical leverage scores to include lower rank approximations. Let $\bm{A} = \bm{U} \bm{\Sigma} \bm{V}^{\intercal}$ be the compact SVD of a $\bm{A}$ real $n \times m$ matrix. Setting $r = \min \left\{ n,m \right\}$, the compact SVD can be partitioned as
\begin{equation*}
    \bm{U} = \left[ \bm{U}_1 , \bm{U}_2 \right] \in \RR^{n \times r}, \qquad \bm{\Sigma} =
    \begin{bmatrix}
        \bm{\Sigma}_1 &               \\
                      & \bm{\Sigma}_2
    \end{bmatrix}
    \in \RR^{r \times r},
    \qquad
    \bm{V} = \left[ \bm{V}_1 , \bm{V}_2 \right] \in \RR^{m \times r}.
\end{equation*}
Here $\bm{U}_1$ contains the first $k \leq r$ columns of $\bm{U}$, $\bm{V}_1$ the first $k$ rows of $\bm{V}$ and $\bm{\Sigma}_1$ is a $k \times k$ matrix containing the top $k$ singular values across its diagonal. The matrix $\bm{A}_k = \bm{U}_1 \bm{\Sigma}_1 \bm{V}_1$ serves as the best rank$-k$ approximation to $\bm{A}$. The statistical leverage scores relative to the best rank$-k$ approximation are again $\bm{H}_{ii}$, but this time $\bm{H}$ is computed only using the best rank$-k$ approximation of $\bm{A}$, that is $\bm{A}_k$. These low rank scores can be evaluated as
\begin{equation*} \label{eq: lev-scrs-1}
    \ell_i^k \triangleq \left( \bm{A}_{k} \left( \bm{A}_{k}^{\intercal} \bm{A}_{k} \right)^{-1} \bm{A}_{k}^{\intercal} \right)_{ii} = \norm{\left( \bm{U}_1 \right)_{(i,:)}}_2^2.
\end{equation*}
What makes low-rank statistical leverage scores particularly appealing is that they can be approximated quickly with a truncated SVD \cite{DBLP:journals/corr/abs-1303-1849}*{pages 3-4}.

\subsubsection{Ridge Leverage Scores}

The low rank leverage scores we saw in equation \ref{eq: lev-scrs-1} will not always be unique and can be sensitive to perturbations \cite{DBLP:journals/corr/CohenMM15}*{page 6}. Consequently these scores can vary drastically when $\bm{A}$ is modified slightly or when we only have access to partial information on the matrix. This undermines the the possibility of computing good quality low rank approximations from statistical leverage scores. This shortcoming is addressed in the next class of leverage score, that is, ridge leverage scores. Ridge leverage scores are similar to statistical leverage scores although a ridge regression term (hence the name) is added to the hat matrix with a regularization parameter $\lambda$. The $\lambda-$ridge leverage score is defined as
\begin{equation*}
    r_{i}^{\lambda} \triangleq \left( \bm{A} \left( \bm{A}^{\intercal} \bm{A} + \lambda \Id_{n \times n} \right)^{-1} \bm{A}^{\intercal} \right)_{ii}.
\end{equation*}
A regularization parameter of
\begin{equation*} \label{eq: rid-lev-reg-param}
    \lambda = \frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k}
\end{equation*}
is typically used since this choice of $\lambda$ will guarantee that the sum of the ridge leverage scores (keep in mind that the raw ridge leverage scores do not necessarily form a probability distribution) is bounded by $2k$, stated more formally in \Cref{lem: rid-lev-reg-param-bound}.
\begin{lem} \label{lem: rid-lev-reg-param-bound}
    When using a regularization parameter of $\lambda = \frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k}$ we have $\sum_{i=1}^{n} r_{i}^{\lambda} \leq 2k$ \cite{DBLP:journals/corr/CohenMM15}*{pages 6-7}.
\end{lem}
\begin{proof}
    Writing $r_{i}^{\lambda}$ using the SVD of $\bm{A}$ where $\lambda = \frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k}$ gives
    \begin{align*}
        r_{i}^{\lambda} & = \bm{A}_{(i,:)} \left( \bm{U} \bm{\Sigma} \bm{U}^{\intercal} + \frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k} \bm{U} \bm{U}^{\intercal} \right)^{-1} \bm{A}_{(i,:)}^{\intercal} \\
                        & = \bm{A}_{(i,:)} \left( \bm{U} \overline{\bm{\Sigma}}^2 \bm{U}^{\intercal} \right)^{-1} \bm{A}_{(i,:)}^{\intercal}                                                         \\
                        & = \bm{A}_{(i,:)} \left( \bm{U} \overline{\bm{\Sigma}}^{-2} \bm{U}^{\intercal} \right) \bm{A}_{(i,:)}^{\intercal}
    \end{align*}
    where $\overline{\bm{\Sigma}}^{2}_{ii} = \sigma_{i}^{2} \left( \bm{A} \right) + \frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k}$. Then
    \begin{align*}
        \sum_{i=1}^{n} r_{i}^{\lambda} & = \operatorname{tr} \left( \bm{A}^{\intercal} \bm{U} \overline{\bm{\Sigma}}^{-2} \bm{U}^{\intercal} \bm{A} \right) \\
                                       & = \operatorname{tr} \left( \bm{V} \bm{\Sigma} \overline{\bm{\Sigma}}^{-2} \bm{\Sigma} \bm{V}^{\intercal} \right)   \\
                                       & = \operatorname{tr} \left( \bm{\Sigma}^{2} \overline{\bm{\Sigma}}^{-2} \right).
    \end{align*}
    Here we have
    \[
        \left( \bm{\Sigma}^{2} \overline{\bm{\Sigma}}^{-2} \right)_{ii} = \frac{\sigma_{i}^{2} \left( \bm{A} \right)}{\sigma_{i}^{2} \left( \bm{A} \right) + \frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k}}.
    \]
    For $i \leq k$ we simply upper bound this by $1$, yielding
    \begin{equation*}
        \operatorname{tr} \left( \bm{\Sigma}^{2} \overline{\bm{\Sigma}}^{-2} \right) =
        k + \sum_{i=k+1}^{n} \frac{\sigma_{i}^{2} \left( \bm{A} \right)}{\sigma_{i}^{2} \left( \bm{A} \right) + \frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k}}
        \leq k + \sum_{i=k+1}^{n} \frac{\sigma_{i}^{2} \left( \bm{A} \right)}{\frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k}}
        = k + \frac{\sum_{i=k+1}^{n} \sigma_{i}^{2} \left( \bm{A} \right)}{\frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k}}
        \leq k + k.
    \end{equation*}
\end{proof}
From now on (unless otherwise stated) the regularization parameter seen in \ref{eq: rid-lev-reg-param} will always be used for ridge leverage scores where the notation
\begin{equation*}
    r_{i}^{k} \triangleq \left( \bm{A} \left( \bm{A}^{\intercal} \bm{A} + \left( \frac{\norm{\bm{A} - \bm{A}_k}_F^{2}}{k} \right) \Id_{n \times n} \right)^{-1} \bm{A}^{\intercal} \right)_{ii}
\end{equation*}
is employed to show that the best rank$-k$ matrix is utilized in the regularization parameter. Adding regularization to the hat matrix offers a smoother alternative which 'washes out' small singular directions meaning they are sampled with proportionally lower probability \cite{DBLP:journals/corr/CohenMM15}*{page 6}. Alaoui and Mahoney \cite{NIPS2015_f3f27a32} prove that ridge leverage scores provide theoretically better bounds over uniform sampling techniques when the number of sampled columns is proportional to $\operatorname{tr} \left( \bm{H}_{\lambda} \right) \cdot \ln \left( n \right)$ where $\bm{H}_{\lambda}$ is the hat matrix with added regularization, that is $\bm{H}_{\lambda} = \bm{A} \left( \bm{A}^{\intercal} \bm{A} + \lambda \Id_{n \times n} \right)^{-1} \bm{A}^{\intercal}$. With the rising popularity of ridged leverage scores, a number of iterative methods have been devised (and continue to be developed) that take advantage of parallel computing to provide fast approximations \cite{martinsson2021randomized}*{page 90}.