\subsection{Data Probabilities}\label{Section2.3}

Here we present two novel Nystrom probability sampling distributions that make use of the data matrix, $\bm{X}$, directly without requiring any knowledge of the associated kernel matrix. The first of these two methods is the data-column sampling distribution defined simply as
\begin{equation*}
    p_{i} = \norm{\bm{X}_{(i,:)}}_2 / \norm{\bm{X}}_{F}^2.
\end{equation*}
This sampling distribution can be thought of as the data-column dual of the kernel column sampling probabilities seen in \Cref{Section2.2}. The second sampling method is the $Q$ matrix column sampling distribution defined as
\begin{equation*}
    p_{i} = \norm{\bm{Q}_{(i,:)}}_2 / \norm{\bm{Q}}_{F}^2.
\end{equation*}
where $\bm{X} = \bm{Q} \bm{R}$ is the $QR$ factorisation (see \Cref{theorem: QR_general_existence}) of $\bm{X}$. This sampling technique can be though of as the data matrix equivalent of leverage score sampling. There are numerous benefits computing the sampling probabilities using the data matrix directly without needing any knowledge about the corresponding matrix. To start, the obvious benefit is that no properties or elements (along with their corresponding calculations) of the kernel matrix are required saving large amounts of computation time. Second, if the parameters of the kernel function are changed, or an entirely different kernel function is used altogether, then the data-column sampling probabilities need not be re-evaluted.

The main grounds for the theoretical justification for using the data matrix directly to produce a sampling distribution comes from a result shown in \cite{KarouiNoureddineEl2010TSOK}, presented below.

\renewcommand{\labelenumi}{(\alph{enumi})}

\begin{thm}[Inner Product of Kernel Matrices] \label{thm: inner-prof-kern-mat}
    Let us assume that we observe n i.i.d. random vectors, $X_i \in \RR^{p}$. let us consider the kernel matrix $\bm{M}$ with entries
    \begin{equation*}
        \bm{M}_{ij} = f \left( \frac{\bm{X}_{i}^{\intercal} \bm{X}_{i}}{p} \right).
    \end{equation*}
    We assume that
    \begin{enumerate}
        \item We have $n/p$ and $p/n$ remain bounded as $n \to \infty$.
        \item $\bm{\Sigma}_{p}$ is a positive semi-definite $p \times p$ matrix, and $\norm{\bm{\Sigma}_{p}}_2 = \sigma_1 \left( \bm{\Sigma}_{p} \right)$ remains bounded in $p$, that is, there exists $K > 0$, such that $\sigma_1 \left( \bm{\Sigma}_{p} \right) \leq K$, for all p.
        \item $\bm{\Sigma}_{p} / p$ has a finite limit, that is, there exists $l \in \RR$ such that $\lim_{p \to \infty} \operatorname{tr} \left( \bm{\Sigma}_{p} \right) / p = l$.
        \item $\bm{X}_{i} = \bm{\Sigma}_{p}^{1/2} \bm{Y}_{i}$.
        \item The entries of $\bm{Y}_{i}$, a $p-$dimensional random vector, are i.i.d. Also, denoting by $\bm{Y}_{i} (k)$ the $k^{th}$ entry of $\bm{Y}_{i}$, we assume that $\EE \left( \bm{Y}_{i} (k) \right) = 0, \; \VV \left( \bm{Y}_{i} (k) \right) = 1$ and $\EE \left( \left| \bm{Y}_{i} (k) \right|^{4 + \varepsilon} \right) < \infty$ for some $\varepsilon > 0$.
        \item $f$ is a $C^{1}$ function is a neighborhood of $l = \lim_{p \to \infty} \operatorname{tr} \left( \bm{\Sigma}_{p} \right) / p$ and a $C^3$ function in a neighborhood of $0$.
    \end{enumerate}
    Under these assumptions, the kernel matrix $\bm{M}$ can (in probability) be approximated consistently in the operator norm, when $p$ and $n$ tend to $\infty$, by the matrix $\bm{K}$, where
    \begin{equation*}
        \bm{K} = \left( f(0) + f''(0) \frac{\operatorname{tr} \left( \bm{\Sigma}_{p}^{2} \right)}{2p^2} \right) 11' + f'(0) \frac{\bm{X} \bm{X}^{\intercal}}{p} + v_{p} \Id,
    \end{equation*}
    and where
    \begin{equation*}
        v_{p} = f \left( \frac{\operatorname{tr} \left( \bm{\Sigma}_{p} \right)}{p} \right) - f(0) - f'(0) \frac{\operatorname{tr} \left( \bm{\Sigma}_{p} \right)}{p}.
    \end{equation*}
    In other words,
    \begin{equation*}
        \norm{\bm{M} - \bm{K}}_2 \to 0
    \end{equation*}
    in probability, when $p \to \infty$ \cite{KarouiNoureddineEl2010TSOK}*{page 9}.
\end{thm}

When using the RBF kernel these assumptions typically hold. This theorem shows that as the size of the kernel matrix becomes large, it begins to behave like the underpinning data matrix, up to a factor of a constant.