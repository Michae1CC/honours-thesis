\section*{Introduction}
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}
The problem of time series prediction (and related regressional tasks) have been a long standing subject of high interest in many disciplines of science and mathematics. The problem itself is fairly straight forward, given a data set of $n$ observations $\calD = \left\{ \left( x_i , y_i \right) \right\}_{i=1}^{n}$, where each input $x_i \in \RR_{>0}$ is a time value and $y_i \in \RR$ is a that acts a function of time, the question then is how do we go about predicting a value $y_{\star}$ for the phenomena being modeled at time $x_{\star}$? With computing power becoming ever more affordable and advanced, many have taken to Machine Learning (ML) to develope sophisticated models to address the problem of developing accurate time series predictors. ML, broadly speaking, is any class of heuristic algorithm that attempts to refine and develope functionality by learning through some form of input. The idea of ML is fonded on the idea that any form of task learning is done through sensory input from the world around them. More formally speaking, in ML we are trying to develope a function $f : X \to Y$ for some input set $X$ and observation set $Y$ were the outputs of $f$ closely align to actual observations. Evidently, ML can be applied the problem of time series prediction in a fairly straight forward manner, by simply teaching a ML algorithm $\calM$ the time series data set $\calD$ to hopefully produce a function $f$ that provides a good model for event predicting.

\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%