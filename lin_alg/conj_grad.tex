\subsection{Conjugate Gradient Algorithm}\label{Section4.5}

From \Cref{Section4.4} that the Petrov-Galerkin conditions for the CG algorithm used an orthogonal projection and the matrix $\bm{A}$ was assumed to be positive definite. To derive the CG algorithm we can start be using some machinery that the Lanczos algorithm provides us with. Recall, the Lanczos algorithm produces the form $\bm{A}\bm{V}_{k} = \bm{V}_{k} \bm{T}_k + \bm{v}_{k+1} \bm{t}_{k}^{\intercal}$ where $\bm{t}_{k} \triangleq \left[ 0,0, \ldots , 0, \beta_k \right]^{\intercal} \in \KK^k$ and the columns of $\bm{V}_{k}$ span $\calK_k$. Recall that $\bm{x}_k$ can be expressed as $\bm{x}_k = \bm{x}_0 + \bm{K}_k \left( \bm{W}_k^{\intercal} \bm{A} \bm{K}_k \right)^{-1} \bm{W}_k \bm{r}_0$ (\Cref{eq: expr_x_Petrov_Galerkin_1}) when $\bm{W}_k^{\intercal} \bm{A} \bm{K}_k$ is invertible. For the CG algorithm $\calK = \calW$ and $\bm{A} \succ \bm{0}$. Under these conditions we can easily show that $\bm{W}_k^{\intercal} \bm{A} \bm{K}_k$ is indeed invertible. Thus the approximate vector can be expressed as $\bm{x}_k = \bm{x}_0 + \bm{z}_k$ where $\bm{z}_k \in \calK_k$. In terms of the Petrov-Galerkin conditions this means that $\bm{z}_k$ must satisfy $\bm{r}_0 - \bm{A} \bm{z}_k \perp \calW_k$. Furthermore since $\calK_k = \operatorname{Range} \left( \bm{V}_{k} \right)$ where $\bm{V}_{k}$ has full column rank then $\bm{z}_k$ can be represented as $\bm{z}_k = \bm{V}_{k} \bm{y}$ for a unique $\bm{y} \in \RR^k$ so that
\begin{equation} \label{eq: x_eq_Qky}
    \bm{x}_k = \bm{x}_0 + \bm{V}_{k} \bm{y}.
\end{equation}
Coupling this with the Petrov-Galerkin conditions ensures
\begin{align} \label{eq: Tky_eq_normr0e1}
    \bm{V}_{k}^{\intercal} \left( \bm{r}_0 - \bm{A} \bm{V}_{k} \bm{y} \right) & = \bm{0}                        \nonumber   \\
    \bm{V}_{k}^{\intercal} \bm{A} \bm{V}_{k} \bm{y}                           & = \bm{V}_{k}^{\intercal} \bm{r}_0 \nonumber \\
    \bm{T}_k \bm{y}                                                           & = \| \bm{r}_0 \| \bm{e}_1.
\end{align}
In the CG algorithm $\bm{x}_{k+1}$ is computed as the recurrance of the following three sets of vectors
\begin{enumerate}
    \item The approximate solutions $\bm{x}_{k}$
    \item The residual vectors $\bm{r}_{k}$
    \item The conjugate gradient vectors $\bm{p}_k$
\end{enumerate}
The conjugate gradient vectors include the word 'gradient' since the attempt to find the direction of steepest descent that minimizes $\| \bm{r}_{k} \|_{\bm{A}^{-1}}$. It also includes the word 'conjugate' since $\langle \bm{p}_k, \bm{A} \bm{p}_j \rangle = 0$ for $i \neq j$, that is, vectors $\bm{p}_i$ and $\bm{p}_j$ are mutally $A$-conjugate (shown in \Cref{lemma: Pk_cols_A_conj}).

Since $\bm{A}$ is symmetric positive definite then so is $\bm{T}_k  = \bm{V}_{k} \bm{A} \bm{V}_{k}$. We can take the Cholesky decomposition of $\bm{T}_k$ to get
\begin{equation} \label{eq: Tk_Cholesky}
    \bm{T}_k = \bm{L}_k \bm{D}_k \bm{L}_k^{\intercal}
\end{equation}
where $\bm{L}_k$ is a unit lower bidiagonal matrix and $\bm{D}_k$ is diagonal written as
\[
    \bm{L}_k =
    \begin{bmatrix}
        1   &        &         &   \\
        l_1 & \ddots &         &   \\
            & \ddots & \ddots  &   \\
            &        & l_{k-1} & 1
    \end{bmatrix}, \quad
    \bm{D}_k =
    \begin{bmatrix}
        d_1 &     &        &     \\
            & d_2 &        &     \\
            &     & \ddots &     \\
            &     &        & d_k
    \end{bmatrix}.
\]
Combining equations \Cref{eq: x_eq_Qky}, \Cref{eq: Tky_eq_normr0e1} and \Cref{eq: Tk_Cholesky}
\begin{align*}
    \bm{x}_k & = \bm{x}_0 + \bm{V}_{k} \bm{y}                                                                                                  \\
    \bm{x}_k & = \bm{x}_0 + \| \bm{r}_0 \| \bm{V}_{k} \bm{T}_k^{-1} \bm{e}_1                                                                   \\
    \bm{x}_k & = \bm{x}_0 + \| \bm{r}_0 \| \bm{V}_{k} \left( \bm{L}_k \bm{D}_k \bm{L}_k^{\intercal} \right)^{-1} \bm{e}_1                      \\
    \bm{x}_k & = \bm{x}_0 + \left( \bm{V}_{k} \bm{L}_k^{-\intercal} \right) \left( \| \bm{r}_0 \| \bm{D}_k^{-1} \bm{L}_k^{-1} \bm{e}_1 \right) \\
    \bm{x}_k & \triangleq \bm{x}_0 + \tilde{\bm{P}}_k \tilde{\bm{y}}_k
\end{align*}
where $\tilde{\bm{P}}_k = \bm{V}_{k} \bm{L}_k^{-\intercal}$ and $\tilde{\bm{y}}_k = \| \bm{r}_0 \| \bm{D}_k^{-1} \bm{L}_k^{-1} \bm{e}_1$. The matrix $\tilde{\bm{P}}_k$ can be written as
$\tilde{\bm{P}}_k = \left[ \tilde{\bm{p}}_1 , \tilde{\bm{p}}_2 , \ldots , \tilde{\bm{p}}_k \right]$. \Cref{lemma: Pk_cols_A_conj} shows that the columns of $\tilde{\bm{P}}_k$ are $A$-conjugate.

\begin{lem} \label{lemma: Pk_cols_A_conj}
    The columns of $\tilde{\bm{P}}_k$ are $A$-conjugate, in otherwords $\tilde{\bm{P}}_k^{\intercal} \bm{A} \tilde{\bm{P}}_k$ is diagonal.
\end{lem}

\begin{proof}
    We compute
    \begin{align*}
        \tilde{\bm{P}}_k^{\intercal} \bm{A} \tilde{\bm{P}}_k
         & = \left( \bm{V}_{k} \bm{L}_k^{-\intercal} \right)^{\intercal} \bm{A} \left( \bm{V}_{k} \bm{L}_k^{-\intercal} \right)      \\
         & = \bm{L}_k^{-1} \left( \bm{V}_{k}^{\intercal} \bm{A} \bm{V}_{k} \right) \bm{L}_k^{-\intercal}                             \\
         & = \bm{L}_k^{-1} \left( \bm{T}_k \right) \bm{L}_k^{-\intercal}                                                             \\
         & = \bm{L}_k^{-1} \left( \bm{L}_k \bm{D}_k \bm{L}_k^{\intercal} \right) \bm{L}_k^{-\intercal} \tag*{\Cref{eq: Tk_Cholesky}} \\
         & = \bm{D}_k
    \end{align*}
    as wanted.
\end{proof}

Since $\bm{L}_k$ is a lower bidiagonal, setting $\bm{a} \triangleq l_{k-1} \bm{e}_{k-1}$, it can be written in the form
\[
    \bm{L}_k =
    \begin{bmatrix}
        \bm{L}_{k-1}       & \bm{0} \\
        \bm{a}^{\intercal} & 1
    \end{bmatrix}
\]
meaning
\[
    \bm{L}_k^{-1} =
    \begin{bmatrix}
        \bm{L}_{k-1}^{-1} & \bm{0} \\
        \star             & 1
    \end{bmatrix}
\]

where the elements of $\star$ are of no importance. With this a recurrance for the columns of $\tilde{\bm{P}}_k$ can now be derived in terms of $\bm{y}_k$. To start we can show that the first $k-1$ entries of $\tilde{\bm{y}}_{k}$ shares the first $k-1$ entires with $\tilde{\bm{y}}_{k-1}$ and that $\tilde{\bm{P}}_k$ and $\tilde{\bm{P}}_{k-1}$ share the same first $k-1$ columns. We can begin by computing a recurrance for $\tilde{\bm{y}}_{k}$ as follows
\begin{align*}
    \tilde{\bm{y}}_{k} & = \| \bm{r}_0 \| \bm{D}_k^{-1} \bm{L}_k^{-1} \bm{e}_1^k \\
                       & = \| \bm{r}_0 \|
    \begin{bmatrix}
        \bm{D}_{k-1}^{-1} & \bm{0}   \\
        \bm{0}            & d_k^{-1}
    \end{bmatrix}
    \begin{bmatrix}
        \bm{L}_{k-1}^{-1} & \bm{0} \\
        \star             & 1
    \end{bmatrix}
    \bm{e}_1^k                                                                   \\
                       & = \| \bm{r}_0 \|
    \begin{bmatrix}
        \bm{D}_{k-1}^{-1} \bm{L}_{k-1}^{-1} & \bm{0}   \\
        \star                               & d_k^{-1}
    \end{bmatrix}
    \begin{bmatrix}
        \bm{e}_1^k \\
        0
    \end{bmatrix}                                                   \\
                       & =
    \begin{bmatrix}
        \tilde{\bm{y}}_{k-1} \\
        \eta_k
    \end{bmatrix}
\end{align*}

where $\bm{e}_i^k$ is the $i^{th}$ unit vector with $k$ dimensions. To get a recurrance for the columns of $\tilde{\bm{P}}_{k-1} = \left[ \tilde{\bm{p}}_1 , \tilde{\bm{p}}_2 , \ldots , \tilde{\bm{p}}_k \right]$ since $\bm{L}_{k-1}^{\intercal}$ is upper triangular then so is $\bm{L}_{k-1}^{-\intercal}$, thus forming the leading $(k-1) \times (k-1)$ submatrix of $\bm{L}_{k}^{-\intercal}$. In effect, $\tilde{\bm{P}}_{k-1}$ is identical to the leading $k-1$ columns of

\[
    \tilde{\bm{P}}_{k} = \bm{V}_{k} \bm{L}_k^{-\intercal} = \left[ \bm{V}_{k-1} , \bm{v}_k \right]
    \begin{bmatrix}
        \bm{L}_{k-1}^{-1} & \bm{0} \\
        \star             & 1
    \end{bmatrix}
    = \left[ \bm{V}_{k-1} \bm{L}_{k-1}^{-1} , \tilde{\bm{p}}_{k} \right]
    = \left[ \tilde{\bm{P}}_{k-1} , \tilde{\bm{p}}_{k} \right].
\]
Moreover rearranging $\tilde{\bm{P}}_{k} = \bm{V}_{k} \bm{L}_k^{-\intercal}$ we get $\tilde{\bm{P}}_{k} \bm{L}_k^{\intercal} = \bm{V}_{k}$. Equating the $k^{th}$ column yields
\begin{equation} \label{eq: pk_rec}
    \tilde{\bm{p}}_{k} = \bm{v}_k - l_{k-1} \tilde{\bm{p}}_{k-1}.
\end{equation}
Finally we can use
\begin{equation} \label{eq: xk_rec}
    \bm{x}_k = \bm{x}_0 + \tilde{\bm{P}}_{k} \tilde{\bm{y}}_{k}                                 \\
    = \bm{x}_0 + \left[ \tilde{\bm{P}}_{k-1} , \tilde{\bm{p}}_{k} \right]
    \begin{bmatrix}
        \tilde{\bm{y}}_{k-1} \\
        \eta_k
    \end{bmatrix}                                                                    \\
    = \bm{x}_0 + \tilde{\bm{P}}_{k-1} \tilde{\bm{y}}_{k-1} + \eta_k \tilde{\bm{p}}_{k} \\
    = \bm{x}_{k-1} + \eta_k \tilde{\bm{p}}_{k}
\end{equation}
as a recurrance for $\bm{x}_k$. A recurrance for $\bm{r}_k$ is easily computed as
\begin{equation} \label{eq: rk_rec}
    \bm{r}_{k} = b - \bm{A} \bm{x}_k = b - \bm{A} \left( \bm{x}_{k-1} + \eta_k \tilde{\bm{p}}_{k} \right) = \left( b - \bm{A} \bm{x}_{k-1} \right) - \eta_k \bm{A} \tilde{\bm{p}}_{k} = \bm{r}_{k-1} - \eta_k \bm{A} \tilde{\bm{p}}_{k}
\end{equation}
Altogether we are left with recurrences for $\bm{v}_k$ from Lanczos, $\tilde{\bm{p}}_{k}$ \Cref{eq: pk_rec}, the residual $\bm{r}_k$ \Cref{eq: pk_rec},  and for the approximate solution $\bm{x}_k$ \Cref{eq: xk_rec}. However, futher simplifications can be made to bring about a more efficient algorithm. Recall from \Cref{Section4.3} that $\bm{A} \bm{V}_{k} =  \bm{V}_{k} \bm{T}_k + \bm{v}_{k+1} \bm{t}_{k}^{\intercal}$ where $\bm{t}_k = \left[ 0,0, \ldots , 0, \beta_k \right]^{\intercal} \in \RR^k$ meaning
\[
    \bm{r}_k = \bm{r}_0 - \bm{A} \bm{V}_{k} \bm{y}_k = \bm{r}_0 - \bm{V}_{k} \bm{T}_k \bm{y}_k - \langle \bm{t}_k , \bm{y} \rangle \bm{v}_{k+1} = - \beta_k y_k \bm{v}_{k+1}.
\]
This tells us that $\bm{r}_k$ is parallel to $\bm{v}_{k+1}$ and orthogonal to all $\bm{v}_{i}, \; 1 \leq i \leq k$. This further implies that $\bm{r}_k$ is orthogonal to all $\bm{r}_i, \; 1 \leq i \leq k-1$ since they are just $\bm{v}_{i}$ scaled by some constant factor. So replacing $\bm{r}_{k-1}$ with $\bm{v}_k / \eta_k$ and defining $\bm{p}_k \triangleq \tilde{\bm{p}}_k / \gamma_k$ gives us a new set of recurrences
\begin{align*}
    \bm{x}_k & = \bm{x}_{k-1} + \alpha_k \bm{p}_k        \\
    \bm{r}_k & = \bm{r}_{k-1} - \alpha_k \bm{A} \bm{p}_k \\
    \bm{p}_k & = \bm{r}_{k-1} + \beta_k \bm{p}_{k-1}
\end{align*}
where $\alpha_k = \eta_k / \gamma_k$. From \Cref{lemma: Pk_cols_A_conj} we have shown that the columns of $\tilde{\bm{P}}_k$ are $A$-conjugate (that is $\langle \tilde{\bm{p}}_i , \bm{A} \tilde{\bm{p}}_j \rangle = 0, \; i \neq j$) and that $\tilde{\bm{P}}_k^{\intercal} \bm{A} \tilde{\bm{P}}_k = \bm{D}_k$. This also means that $\langle \bm{r}_i , \bm{r}_j \rangle = 0, \; i \neq j$. Note that from our recurrence for $\bm{p}_k = \bm{r}_{k-1} + \beta_k \bm{p}_{k-1}$ we have
\[
    \langle \bm{A} \bm{p}_k ,\bm{p}_k \rangle = \langle \bm{A} \bm{p}_k , \bm{r}_{k-1} + \beta_k \bm{p}_{k-1} \rangle = \langle \bm{A} \bm{p}_k , \bm{r}_{k-1} \rangle.
\]
We can now find an expression for $\alpha_k$ as
\begin{align*}
    \langle \bm{r}_{k-1} , \bm{r}_{k} \rangle & = \langle \bm{r}_{k-1} , \bm{r}_{k-1} - \alpha_k \bm{A} \bm{p}_k \rangle                           \\
    \langle \bm{r}_{k-1} -1 \rangle           & = \langle \bm{r}_{k-1} , \bm{r}_{k-1} \rangle - \alpha_k \langle \bm{p}_k, \bm{A} \bm{p}_k \rangle \\
    \alpha_k                                  & = \frac{\langle \bm{r}_{k-1} , \bm{r}_{k-1} \rangle}{\langle \bm{p}_k, \bm{A} \bm{p}_k \rangle}.
\end{align*}
Similarly, using the recurrence for $\bm{p}_k$, an expression for $\beta_k$ can be computed as
\begin{align*}
    \langle \bm{A} \bm{p}_{k-1} , \bm{p}_k \rangle & = \langle \bm{A} \bm{p}_{k-1}, \bm{r}_{k-1} + \beta_k \bm{p}_{k-1} \rangle                                       \\
    \langle \bm{A} \bm{p}_{k-1} , \bm{p}_k \rangle & = \langle \bm{A} \bm{p}_{k-1}, \bm{r}_{k-1} \rangle + \beta_k \langle \bm{A} \bm{p}_{k-1}, \bm{p}_{k-1} \rangle  \\
    \beta_k                                        & = - \frac{\langle \bm{A} \bm{p}_{k-1}, \bm{r}_{k-1} \rangle}{\langle \bm{A} \bm{p}_{k-1}, \bm{p}_{k-1} \rangle}.
\end{align*}
This formula requires an additional dot product which was not present before. Fortunately, this dot product can be eliminated using our recurrence for $\bm{r}_k$
\begin{align*}
    \langle \bm{r}_k , \bm{r}_k \rangle & = \langle \bm{r}_k , \bm{r}_{k-1} - \alpha_k \bm{A} \bm{p}_k \rangle                            \\
    \langle \bm{r}_k , \bm{r}_k \rangle & = \langle \bm{r}_k , \bm{r}_{k-1} \rangle - \alpha_k \langle \bm{r}_k , \bm{A} \bm{p}_k \rangle \\
    \alpha_k                            & = - \frac{\langle \bm{r}_k , \bm{r}_k \rangle}{\langle \bm{r}_k , \bm{A} \bm{p}_k \rangle}.
\end{align*}
Equating the two expressions for $\alpha_k$ affords
\begin{align*}
    - \frac{\langle \bm{r}_k , \bm{r}_k \rangle}{\langle \bm{r}_k , \bm{A} \bm{p}_k \rangle}  & = \frac{\langle \bm{r}_{k-1} , \bm{r}_{k-1} \rangle}{\langle \bm{p}_k, \bm{A} \bm{p}_k \rangle} \\
    - \frac{\langle \bm{r}_k , \bm{r}_k \rangle}{\langle \bm{r}_{k-1} , \bm{r}_{k-1} \rangle} & = \frac{\langle \bm{r}_k , \bm{A} \bm{p}_k \rangle}{\langle \bm{p}_k, \bm{A} \bm{p}_k \rangle}.
\end{align*}
This means that
\[
    \beta_k = \frac{\langle \bm{r}_{k-1} , \bm{r}_{k-1} \rangle}{\langle \bm{r}_{k-2} , \bm{r}_{k-2} \rangle}.
\]
These recurrences are computed iteratively to form the basis of the CG algorithm, seen in Algorithm \ref{alg: CG}.

{\centering
\begin{minipage}{.85\linewidth}
    \begin{algorithm}[H]
        \caption{CG Algorithm}
        \label{alg: CG}
        \SetAlgoLined
        \DontPrintSemicolon
        \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

        \Input{$\bm{A} \succ \bm{0}$, $\bm{b}$ and an initial guess $\bm{x}_0$.}
        \Output{An approximation of $\bm{x}^{\ast}$, $\bm{x}_k$.}
        \BlankLine
        $\bm{r}_0 = \bm{b} - \bm{A} \bm{x}_0$, $\bm{p}_1 = \bm{r}_0$\;
        \For{$k = 1 , \ldots $ \Until $\| r_{k-1} \| \leq \tau$}{
            $\alpha_k = \frac{\langle \bm{r}_{k-1} , \bm{r}_{k-1} \rangle}{\langle \bm{p}_k, \bm{A} \bm{p}_k \rangle}$ \;
            $\bm{x}_k = \bm{x}_{k-1} + \alpha_k \bm{p}_k$ \;
            $\bm{r}_k = \bm{r}_{k-1} - \alpha_k \bm{A} \bm{p}_k$ \;
            $\beta_{k+1} = \frac{\langle \bm{r}_{k} , \bm{r}_{k} \rangle}{\langle \bm{r}_{k-1} , \bm{r}_{k-1} \rangle}$ \;
            $\bm{p}_{k+1} = \bm{r}_{k} + \beta_{k+1} \bm{p}_k$ \;
        }
        \Return{$\bm{x}_k$}
        \BlankLine
    \end{algorithm}
\end{minipage}
\par
}

The next few theorems pertain to the rate of convergence of the CG algorithm.

\begin{thm} \label{theorem: equiv-CG-spaces}
    Let the CG iteration (\Cref{alg: CG}) be applied to a symmetric positive definite matrix problem $\bm{A} \bm{x} = \bm{b}$. As long as the iteration has not yet converged (that is, $\bm{r}_{n-1} \neq 0$), the algorithm proceeds without divisions by zero, and we have the following identities of subspaces:
    \begin{equation} \label{eq: equiv-CG-spaces}
        \calK_n                                                  = \operatorname{l.s} \left\{ \bm{x}_i \right\}_{i=1}^{n} = \operatorname{l.s} \left\{ \bm{p}_i \right\}_{i=0}^{n-1} = \operatorname{l.s} \left\{ \bm{r}_i \right\}_{i=0}^{n-1} = \operatorname{l.s} \left\{ \bm{A}^{i} \bm{b} \right\}_{i=0}^{n-1}.
    \end{equation}
    Moreover the residuals are orthogonal,
    \begin{equation*}
        \langle \bm{r}_{n} , \bm{r}_{j} \rangle = 0 \quad (j<n)
    \end{equation*}
    and the search directions are $\bm{A}-$conjugate \cite{TrefethenLloydN.LloydNicholas1997Nla/}*{page 295}.
\end{thm}

\begin{proof}
    The orthogonality of the residuals has already been shown within the derivation of the CG algorithm. Seeing that the search directions are $\bm{A}-$conjugate is a very straight forward application of \Cref{lemma: Pk_cols_A_conj}. To show \Cref{eq: equiv-CG-spaces} we can induct on $k$. From the initial guess $\bm{x}_0$ and the expression $\bm{x}_k = \bm{x}_{k-1} + \alpha_k \bm{p}_k$, using an induction argument it follows that $\bm{x}_k$ belongs to $\operatorname{l.s} \left\{ \bm{p}_i \right\}_{i=0}^{k-1}$. Similarly, from $\bm{p}_k = \bm{r}_{k-1} + \beta_k \bm{p}_{k-1}$ it follows that this belongs to $\operatorname{l.s} \left\{ \bm{r}_i \right\}_{i=0}^{n-1}$. Finally, from $\bm{r}_k = \bm{r}_{k-1} - \alpha_k \bm{A} \bm{p}_k$ that this belongs to $\operatorname{l.s} \left\{ \bm{A}^{i} \bm{b} \right\}_{i=0}^{n-1}$.
\end{proof}

It is fairly straight forward to confirm that CG minimizes error at each step.

\begin{thm} \label{theorem: CG-error-min}
    Let the CG iteration be applied to a symmertic positive definite matrix $\bm{A} \bm{x} = \bm{b}$. If the iteration has not already converged (that is, $\| \bm{r}_{k-1} \| > 0 $), then $\bm{x}_{k}$ is the unique point in $\calK_{k}$ that minimizes $\| \bm{x}^{\star} - \bm{x}_{k} \|_{\bm{A}}$. The convergence is the monotonic,
    \[
        \| \bm{x}^{\star} - \bm{x}_{k} \|_{\bm{A}} \leq \| \bm{x}^{\star} - \bm{x}_{k-1} \|_{\bm{A}}
    \]
    and $\| \bm{x}^{\star} - \bm{x}_{k} \|_{\bm{A}} = 0$ is achieved for some $k \leq n$ \cite{TrefethenLloydN.LloydNicholas1997Nla/}*{page 296}.
\end{thm}

\begin{proof}
    From \Cref{theorem: equiv-CG-spaces} we know that $\bm{x}_{k}$ belongs to $\calK_{k}$. To show that it is the unique point in $\calK_{k}$ that minimizes the error, consider an arbitrary point $\bm{z} = \bm{x}_k - \Delta \bm{z} \in \calK_{k}$, with error $\bm{e} = \bm{x}_{\star} - \bm{z} = \bm{e}_{k} + \Delta \bm{x}$. Note in this context $\bm{e}_{k} = \bm{x}_{\star} - \bm{x}_{k}$, and does {\it not} represent standard basis vectors. We calculate
    \begin{align*}
        \| \bm{e} \|_{\bm{A}}^2 \
         & = \left( \bm{e}_{k} + \Delta \bm{x} \right)^{\intercal} \bm{A} \left( \bm{e}_{k} + \Delta \bm{x} \right)                                                                                  \\
         & = \bm{e}_{k}^{\intercal} \bm{A} \bm{e}_{k} + \left( \Delta \bm{x} \right)^{\intercal} \bm{A} \left( \Delta \bm{x} \right) + 2 \bm{e}_{k}^{\intercal} \bm{A} \left( \Delta \bm{x} \right).
    \end{align*}
    The final term of this expression is $2 \bm{r}_{k}^{\intercal} \left( \Delta \bm{x} \right)$, an inner product of $\bm{r}_{k}$ with a vector in $\calK_k$, and by \Cref{theorem: CG-error-min}, any such inner product is zero. Effectively, this means
    \[
        \| \bm{e} \|_{\bm{A}}^{2} = \bm{e}_{k}^{\intercal} \bm{A} \bm{e}_{k} + \left( \Delta \bm{x} \right)^{\intercal} \bm{A} \left( \Delta \bm{x} \right).
    \]
    Only the second of these terms depends on $\Delta \bm{x}$, since $\bm{A}$ is positive definite, that term is greater than or equal to $0$, attaining the value of $0$ if and only if $\Delta \bm{x} = \bm{0}$, that is, $\bm{x}_k = \bm{x}$. Thus $\| \bm{e} \|_{\bm{A}}$ is minimal if and only if $\bm{x}_{k} = \bm{x}$, as claimed.
\end{proof}

As seen in \Cref{eq: x_ast_via_cayley} Krylov subspace methods, in some sense, aim to find a polynomial $p_n \in P_n$ which minimizes $\| p_n \left( \bm{A} \right) \| = \| \sum_{k=0}^{n-1} \alpha_{k + 1} \bm{A}^{k} \bm{r}_{0} \|$, where $P_n$ is the set of polynomials with degree less than or equal to $n$ with $p(\bm{0}) = \Id$. In the context of the CG algorithm, this is equivalent to finding a polynomial $p_n \in P_n$ which instead minimizes
\begin{equation} \label{eq: CG-poly-min}
    \| p_n \left( \bm{A} \right) \bm{e}_{0} \|_{\bm{A}}.
\end{equation}
From \Cref{theorem: equiv-CG-spaces}, we can derive the following convergence theorem.

\begin{thm} \label{theorem: CG-convg}
    If the CG iteration has not already converged before step $k$ (that is, $\| \bm{r}_{k-1} \| > 0 $), then \Cref{eq: CG-poly-min} has a unique solution $p_n \in P_n$, and the iterate $\bm{x}_{k}$ has error $\bm{e}_k = p_n \left( \bm{A} \right) \bm{e}_{0}$ for this same polynomial $p_n$. Consequently we have
    \begin{equation} \label{eq: CG-err-bounds}
        \frac{\| \bm{e}_k \|_{\bm{A}}}{\| \bm{e}_{0} \|_{\bm{A}}} = \inf_{p \in P_n} \frac{\| p \left( \bm{A} \right) \bm{e}_{0} \|_{\bm{A}}}{\| \bm{e}_{0} \|_{\bm{A}}} \leq \inf_{p \in P_n} \max_{\lambda \in \Lambda \left( \bm{A} \right)} \left| p (\lambda) \right|
    \end{equation}
    \cite{TrefethenLloydN.LloydNicholas1997Nla/}*{page 298}.
\end{thm}

\begin{proof}
    From \Cref{theorem: equiv-CG-spaces} it follows that $\bm{e}_{k} = p \left( \bm{A} \right) \bm{e}_{0}$ for some $p \in P_n$. The equality in \Cref{eq: CG-err-bounds} is a consequence of this and \Cref{theorem: CG-error-min}. As for the inequality in \Cref{eq: CG-err-bounds}, if $\bm{e}_{0} = \sum_{j=1}^{m} a_j \bm{v}_j$ is an expansion of $\bm{e}_{0}$ in orthonormal eigenvectors of $\bm{A}$, then we have $p \left( \bm{A} \right) \bm{e}_{0} = \sum_{j=1}^{m} a_j p \left( \lambda_j \right) \bm{v}_{j}$ and thus
    \[
        \| \bm{e}_{0} \|_{\bm{A}}^{2} = \sum_{j=1}^{m} a_{j}^{2} \lambda_j, \qquad \| p \left( \bm{A} \right) \bm{e}_0 \|_{\bm{A}}^{2} = \sum_{j=1}^{m} a_{j}^{2} \lambda_j \left( p \left( \lambda_j \right) \right)^{2}.
    \]
    These identities indicate $\| p \left( \bm{A} \right) \bm{e}_{0} \|_{\bm{A}}^{2} / \| \bm{e}_{0} \|_{\bm{A}}^{2} \leq \max_{\lambda \in \Lambda \left( \bm{A} \right)} \left| p (\lambda) \right|^{2}$, which implies the inequality in question.
\end{proof}