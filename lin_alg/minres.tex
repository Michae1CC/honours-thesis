\subsection{Minimum Residual}\label{Section4.6}
In contrast to CG, MINRES is able applicable to a wider range of linear systems, namely symmertic indefinite systems. From \Cref{Section4.4} we saw that MINRES minimizes the residual $\bm{r}_k$ with respect to the Euclidean norm at each iteration (hence the name), that is $\bm{x}_k$ is chosen so that
\begin{equation} \label{eq: minres_opt_cond_1}
    \bm{x}_k = \argmin_{\bm{x} \in \bm{x}_0 + \calK_k} \norm{\bm{A} \bm {x} - \bm{b}}_2.
\end{equation}
It can be shown that this is equivalent setting $\calW = \bm{A} \calK_k$ in the Petrov-Galerkin conditions where $\det \left( \bm{A} \right) \neq 0$, in other words MINRES is an oblique projection method. We can also show that when $\det \left( \bm{A} \right) \neq 0$ and $\calW = \bm{A} \calK_k$ the matrix $\bm{W}^{\intercal} \bm{A} \bm{K}$ is non-singular. So under the conditions of MINRES, using a similar argument used for CG, $\bm{x}_k$ can be expressed as $\bm{x}_k = \bm{x}_0 + \bm{V}_k \bm{y}_k$. Using equation \ref{eq: AVk_eq_VkTk1k} produced by the Lanczos algorithm in \ref{Section4.3} we can manipulate our optimality condition \ref{eq: minres_opt_cond_1} as follows
\begin{align*}
    \bm{x}_k      & = \argmin_{\bm{x} \in \bm{x}_0 + \calK_k} \norm{\bm{A} \bm {x} - \bm{b}}_2                                 \\
    \iff \bm{y}_k & = \argmin_{\bm{y} \in \RR^k} \norm{\bm{A} \left( \bm{x}_0 + \bm{V}_k \bm{y} \right) - \bm{b}}_2            \\
                  & = \argmin_{\bm{y} \in \RR^k} \norm{ - \left( \bm{b} - \bm{A} \bm{x}_0 \right) + \bm{A} \bm{V}_k \bm{y} }_2 \\
                  & = \argmin_{\bm{y} \in \RR^k} \norm{ \bm{V}_{k} \bm{T}_{k+1,k} \bm{y} - \bm{r}_0 }_2                        \\
                  & = \argmin_{\bm{y} \in \RR^k} \norm{ \bm{T}_{k+1,k} \bm{y} - \bm{V}_{k}^{\intercal} \bm{r}_0 }_2            \\
                  & = \argmin_{\bm{y} \in \RR^k} \norm{ \bm{T}_{k+1,k} \bm{y} - \beta_0 \bm{e}_1 }_2
\end{align*}

\cite{GreenbaumAnne1997Imfs}*{page 43}. Using the general project method procedure \ref{alg: General_Projection} as a guide, this gives the following high-level description of the MINRES algorithm \cite{TrefethenLloydN.LloydNicholas1997Nla/}*{page 268}.

{\centering
\begin{minipage}{.85\linewidth}
    \begin{algorithm}[H]
        \caption{High-Level MINRES}
        \label{alg: highlev_minres}
        \SetAlgoLined
        \DontPrintSemicolon
        \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

        \Output{An approximation of $\bm{x}^{\ast}$, $\bm{x}_k$.}
        \BlankLine
        \For{$k = 1 , \ldots $ \Until convergence}{
            $\operatorname{Lanczos-Step} \left( \bm{A}, \bm{v}_{k}, \bm{v}_{k-1}, \beta_{k} \right) \to \alpha_{k}, \beta_{k+1}, \bm{v}_{k+1}$ \;
            Find $\bm{y}$ to minimize $\norm{ \bm{T}_{k+1,k} \bm{y} - \beta_0 \bm{e}_1 }_2$ \;
            $\bm{x}_k = \bm{V}_k \bm{y}$ \;
        }
        \Return{$\bm{x}_k$}
        \BlankLine
    \end{algorithm}
\end{minipage}
\par
}

At each step solving $\bm{y}$ is a matter of solving a $(n+1) \times n$ least squares problem with Hessenberg structure. This can be done by performing a QR-factorisation on the successive $\bm{T}_{k+1,k}$ matrices. Whats more using a clever bit of thinking, we can actually compute the QR-factorisation of $\bm{T}_{k+1,k}$ from the QR-factorisation of $\bm{T}_{k,k-1}$ using a inexpensive $\calO \left( n \right)$ Householder reflection \cite{TrefethenLloydN.LloydNicholas1997Nla/}*{page 268}. Computing the QR-factorisation of $\bm{T}_{k,k-1}$ yields
\begin{equation}
    \bm{Q}_k \bm{T}_{k,k+1}
    =
    \begin{bmatrix}
        \bm{R}_k \\ 0
    \end{bmatrix}
    =
    \begin{bmatrix}
        \gamma_{1}^{(1)} & \delta_{2}^{(1)} & \varepsilon_{3}^{(1)} &                       &        &                       \\
                         & \gamma_{1}^{(2)} & \delta_{3}^{(2)}      & \varepsilon_{4}^{(1)} &        &                       \\
                         &                  & \ddots                & \ddots                & \ddots &                       \\
                         &                  &                       & \ddots                & \ddots & \varepsilon_{k}^{(1)} \\
                         &                  &                       &                       & \ddots & \gamma_{k}^{(2)}      \\
                         &                  &                       &                       &        & \delta_{k}^{(2)}      \\
                         &                  &                       &                       &        & 0                     \\
    \end{bmatrix}
\end{equation}
where $\bm{Q}_k = \prod_{i=1}^{k} \bm{Q}_{i,i+1}$ is the product of Householder reflections designed to annihilate the $\beta_i$s in the subdiagonal of $\bm{T}_{k,k+1}$ \cite{ChoiSou-ChengTerrya2007Imfs}*{page 25} where each $\bm{Q}_{i,i+1}$ is defined as
\begin{equation*}
    \bm{Q}_{i,i+1} \triangleq
    \begin{bmatrix}
        \Id_{(i-1) \times (i-1)} &       &        &                          \\
                                 & c_{i} & s_{i}  &                          \\
                                 & s_{i} & -c_{i} &                          \\
                                 &       &        & \Id_{(k-i) \times (k-i)}
    \end{bmatrix}
\end{equation*}
\cite{ChoiSou-ChengTerrya2007Imfs}*{page 22}. As mentioned for each $i$, $\bm{Q}_{i,i+1}$ is orthogonal, symmetric and constructed to annihilate $\beta_{k+1}$ that is the bottom right element of $\bm{T}_{k,k+1}$. To see this better an alternative way of writing $\bm{Q}_k \bm{T}_{k,k+1}$ is $\bm{Q}_{k,k+1} \cdot \bm{Q}_{2,3}\bm{Q}_{1,2} \bm{T}_{k,k+1}$. Notice however that $\bm{Q}_{k,k+1}$ is the only rotation matrix that will involve $\beta_{k+1}$ in its matrix multiplication with $\bm{T}_{k,k+1}$. To study the influence of $\bm{Q}_{k,k+1}$ in a more compact way we only need to consider the matrix product
\begin{equation} \label{eq: iso_Qkk1_mat_vec_prod_1}
    \begin{bmatrix}
        c_k & s_k  \\
        s_k & -c_k
    \end{bmatrix}
    \begin{bmatrix}
        \gamma_{k}^{(1)} & \delta_{k+1}^{(1)} & 0           \\
        \beta_{k+1}      & \alpha_{k+1}       & \beta_{k+1}
    \end{bmatrix}.
\end{equation}
To annihilate $\beta_{k+1}$, we find the appropriate choice of $c_k$ and $s_k$ are
\begin{equation*}
    \rho_k = \sqrt{{\gamma_k^{(1)}}^{2} + \beta_{k+1}^{2}}, \quad c_k \triangleq \frac{\gamma_{k}^{(1)}}{\rho_k}, \quad s_k \triangleq \frac{\beta_{k+1}}{\rho_{k}}
\end{equation*}
so that \ref{eq: iso_Qkk1_mat_vec_prod_1} becomes
\begin{equation*}
    \begin{bmatrix}
        c_k & s_k  \\
        s_k & -c_k
    \end{bmatrix}
    \begin{bmatrix}
        \gamma_{k}^{(1)} & \delta_{k+1}^{(1)} & 0           \\
        \beta_{k+1}      & \alpha_{k+1}       & \beta_{k+1}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \gamma_{k}^{(2)} & \delta_{k+1}^{(2)} & \varepsilon_{k+2}^{(1)} \\
        0                & \gamma_{k+1}^{(1)} & \delta_{k+2}^{(1)}
    \end{bmatrix}.
\end{equation*}
Furthermore, if we set
\begin{align*}
    \bm{Q}_k \left( \beta_0 \bm{e}_1 \right) & = \prod_{i=1}^{k} \bm{Q}_{i,i+1} \left( \beta_0 \bm{e}_1 \right) \\
                                             & =
    \beta_0 \bm{Q}_{k,k+1} \cdots \bm{Q}_{2,3}
    \begin{bmatrix}
        c_1 \\ s_1 \\ \bm{0}_{k-1}
    \end{bmatrix}                                                                                  \\
                                             & =
    \beta_0 \bm{Q}_{k,k+1} \cdots \bm{Q}_{3,4}
    \begin{bmatrix}
        c_1 \\ s_1 c_1 \\ s_1 s_2 \\ \bm{0}_{k-2}
    \end{bmatrix}                                                                                  \\
                                             & =
    \beta_0 \bm{Q}_{k,k+1}
    \begin{bmatrix}
        c_1 \\ s_1 c_1 \\ \vdots \\ s_1 \cdots s_{k-1} \\ 0
    \end{bmatrix}                                                                                  \\
                                             & =
    \bm{Q}_{k,k+1}
    \begin{bmatrix}
        \bm{t}_{k-1} \\ \phi_{k-1} \\ 0
    \end{bmatrix}
    =
    \begin{bmatrix}
        \bm{t}_{k} \\ \phi_{k-1}
    \end{bmatrix}
\end{align*}
where $\bm{t}_k = \left[ \tau_1 , \tau_2 , \cdots , \tau_k \right]^{\intercal}$ then our optimality condition becomes
\begin{equation*}
    \bm{y}_k = \argmin_{\bm{y} \in \RR^k} \norm{
        \begin{bmatrix}
            \bm{R}_k \\ 0
        \end{bmatrix} \bm{y} -
        \begin{bmatrix}
            \bm{t}_{k} \\ \phi_{k-1}
        \end{bmatrix}
    }_2
\end{equation*}
which can be used to formulate subproblems \cite{ChoiSou-ChengTerrya2007Imfs}*{page 25}. From the new optimality condition it obvious that the optimal solution will satisfy $\bm{R}_k \bm{y}_k = \bm{t}_k$. However, instead of solving for $\bm{y}_k$ directly, MINRES solves
\begin{equation}\label{eq: MINRES_xk_recurr_1}
    \bm{R}_{k}^{\intercal} \bm{D}_{k}^{\intercal} = \bm{V}_{k}^{\intercal}
\end{equation}
where $\bm{D}_{k} =
    \left[ \bm{d}_1 , \bm{d}_2 , \ldots , \bm{d}_k \right] \triangleq \bm{V}_k \bm{R}_k^{-1}$. This is done by forward substitution obtaining the last column $\bm{d}_k$ of $\bm{D}_k$ at iteration $k$. At the same time $\bm{x}_k$ is updated as
\begin{align} \label{eq: MINRES_xk_recurr_2}
    \bm{x}_k & = \bm{V}_k \bm{y}_k = \bm{D}_k \bm{R}_k \bm{y}_k = \bm{D}_k \bm{t}_k \nonumber \\
             & = \left[ \bm{D}_{k-1} , \bm{d}_k \right]
    \begin{bmatrix}
        \bm{t}_{k-1} \\ \tau_{t}
    \end{bmatrix} \nonumber                                                      \\
             & = \bm{x}_{k-1} + \tau_{k} \bm{d}_{k}.
\end{align}
The $d_j$ in equation \ref{eq: MINRES_xk_recurr_2} can be found as
\begin{equation*}
    \left\{
    \begin{array}{l}
        \bm{d}_1 = \bm{v}_1 / \gamma_1                                                                                                        \\
        \bm{d}_2 = \left( \bm{v}_2 - \delta_2 \bm{d}_1 \right) / \gamma_{2}^{(2)}                                                             \\
        \bm{d}_j = \left( \bm{v}_j - \delta_j^{(2)} \bm{d}_{j-1} -\varepsilon_{j} \bm{d}_{j-2} \right) / \gamma_{j}^{(2)}, \quad j=3,\ldots k \\
    \end{array}
    \right.
\end{equation*}
from equation \ref{eq: MINRES_xk_recurr_1}  \cite{CHOISou-ChengT2011MAKS}*{page 4}. The final form of the MINRES in presented in algorithm \ref{alg: MINRES}.

{\centering
\begin{minipage}{.85\linewidth}
    \begin{algorithm}[H]
        \caption{MINRES Algorithm}
        \label{alg: MINRES}
        \SetAlgoLined
        \DontPrintSemicolon
        \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

        \Input{$\bm{A}$ where $\bm{A} = \bm{A}^{\intercal}$, $\bm{b}$ and an initial guess $\bm{x}_0$.}
        \Output{An approximation of $\bm{x}^{\ast}$, $\bm{x}_k$.}
        \BlankLine
        $\bm{r}_0 = \bm{b} - \bm{A} \bm{x}_0$, $\tau_0 = \beta_0 = \norm{\bm{r}_0}, \bm{v}_1 = \bm{r}_0 / \beta_0, \delta_{1}^{(1)} = 0, \bm{v}_0 = \bm{d}_{0} = \bm{d}_{-1} = \bm{0}$\;
        \For{$k = 1 , \ldots $ \Until convergence}{
            $\operatorname{Lanczos-Step} \left( \bm{A}, \bm{v}_{k}, \bm{v}_{k-1}, \beta_{k} \right) \to \alpha_{k}, \beta_{k+1}, \bm{v}_{k+1}$ \;
            \(
            \begin{bmatrix}
                \delta_{k}^{(2)} & \varepsilon_{k+1}^{(1)} \\
                \gamma_{k}^{(1)} & \delta_{k+1}^{(1)}
            \end{bmatrix}
            =
            \begin{bmatrix}
                c_{k-1} & s_{k-1}  \\
                s_{k-1} & -c_{k-1}
            \end{bmatrix}
            \begin{bmatrix}
                \delta_{k}^{(1)} & 0           \\
                \alpha_{k}       & \beta_{k+1}
            \end{bmatrix}
            \)\;
            $\rho_k = \sqrt{{\gamma_k^{(1)}}^{2} + \beta_{k+1}^{2}}, \quad c_k = \frac{\gamma_{k}^{(1)}}{\rho_k}, \quad s_k = \frac{\beta_{k+1}}{\rho_{k}}$\;
            $\gamma_k^{(2)} = c_k \gamma_k^{(1)}$\;
            $\tau_k = c_k \phi_{k-1}, \quad \phi_k = s_k \phi_{k-1}$\;
            $\bm{d}_k = \left( \bm{v}_k - \delta_k^{(2)} \bm{d}_{k-1} -\varepsilon_{k} \bm{d}_{k-2} \right) / \gamma_{k}^{(2)}$\;
            $\bm{x}_k + \tau_{k} \bm{d}_{k}$\;
        }
        \Return{$\bm{x}_k$}
        \BlankLine
    \end{algorithm}
\end{minipage}
\par
}