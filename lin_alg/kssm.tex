
\subsection{Krylov Subspaces}\label{Section4.1}

We will motivate the Krylov subspaces by observing their usefullness in solving linear systems. To this end, consider the problem of solving the linear system
\begin{equation}\label{eq: lin_sys_1}
    \bm{A} \bm{x^{\star}} = \bm{b}
\end{equation}
where no explicit form of $\bm{A}$ is available and instead one must draw information from $\bm{A}$ solely through a routine that can evaluate $\bm{A} \bm{v}$ for any $\bm{v}$. How could this routine be utilized in such a manner to provide with a solution to equation \ref{eq: lin_sys_1}? Before answering this, consider the following theorem

\begin{thm} \label{theorem: invert_mat_norm}
    For $\bm{A} \in \CC^{n \times n}$ if $\| \bm{A} \| = q < 1$ then $\Id - \bm{A}$ is invertible and its inverse admits the following representation
    \begin{equation*} \label{eq: convg-of-part}
        \left( \Id - \bm{A} \right)^{-1} = \sum_{k=0}^{\infty} \bm{A}^k
    \end{equation*}
    \cite{BerezanskyMakarovich1996FaV1}*{page 287}.
\end{thm}

\begin{proof}
    Let us show that the sequence $\bm{S}_{n} = \sum_{k=0}^{n} \bm{A}^k$ (where $n$ is some natural number) of partial sums of the series on the right-hand side of \Cref{eq: convg-of-part} is fundamental in $\CC^{n \times n}$. Indeed, by using the fact that $\| \bm{A B} \| \leq \| \bm{A} \| \| \bm{B} \|$ for any $\bm{A} , \bm{B} \in \CC^{n \times n}$, we conclude that
    \begin{equation*}
        \| \bm{S}_{n+p} - \bm{S}_{n} \| \leq \| \bm{A}^{n+1} \| + \cdots + \| \bm{A}^{n+p} \| \leq {q}^{n+1} + \cdots + {q}^{n+p}
    \end{equation*}
    for all $n,p \in \NN$. Since $q < 1$, this yields the desired result. Since $\CC^{n \times n}$ is a Banach space, the sequence $\left\{ \bm{S}_{n} \right\}_{n}^{\infty}$ uniformly converges to an operator $\bm{S} \in \CC^{n \times n}$. Let us show that $\bm{S} \left( \Id_{n \times n} - \bm{A} \right) = \left( \Id_{n \times n} - \bm{A} \right) \bm{S} = \Id_{n \times n}$. To this end, we note that
    \[
        \| \left( \Id_{n \times n} - \bm{A} \right) \bm{S}_n - \left( \Id_{n \times n} - \bm{A} \right) \bm{S} \| \leq \|  \Id_{n \times n} - \bm{A} \| \cdot \| \bm{S} - \bm{S}_{n} \| \to 0
    \]
    as $n \to \infty$. Therefore, it suffices to show that the sequence $\left\{ \left( \Id_{n \times n} - \bm{A} \right) \bm{S}_n \right\}_{n=1}^{\infty}$ uniformly converges to the identity operator. We have
    \[
        \bm{S}_{n} \left( \Id_{n \times n} - \bm{A} \right) = \left( \Id_{n \times n} - \bm{A} \right) \bm{S}_{n} = \sum_{k=0}^{n} \bm{A}^k - \sum_{k=1}^{n+1} \bm{A}^k = \Id_{n \times n} - \bm{A}^{n+1},
    \]
    that is, $\| \left( \Id_{n \times n} - \bm{A} \right) \bm{S}_{n} - \Id_{n \times n} \| = \| \bm{A}^{n+1} \| \leq q^{n+1}$, which vanishes as $n \to \infty$. Thus $\bm{S} = \left( \Id_{n \times n} - \bm{A} \right)^{-1}$.
\end{proof}

Consider a matrix for which $\| \bm{A} \| < 2$, it follows that $\| \Id_{n \times n} - \bm{A} \| < 1$ meaning $\Id_{n \times n} - \left( \Id_{n \times n} - \bm{A} \right)$ is invertible and $\bm{A}^{-1} = \left( \Id_{n \times n} - \left( \Id_{n \times n} - \bm{A} \right) \right)^{-1} = \sum_{k=0}^{\infty} \left( \Id_{n \times n} - \bm{A} \right)^{k}$. Thinking back to equation \ref{eq: lin_sys_1} for any $\bm{x}_0 \in \RR^{n}$ we have
\begin{align*}
    \bm{x^{\star}} & = \bm{A}^{-1} \bm{b} = \bm{A}^{-1} \left( \bm{A} \bm{x^{\star}} - \bm{A} \bm{x_0} + \bm{A} \bm{x_0} \right) \\
                   & = \bm{x_0} + \bm{A}^{-1} \bm{r_0}                                                                           \\
                   & = \bm{x_0} + \sum_{k=0}^{\infty} \left( \Id_{n \times n} - \bm{A} \right)^k \bm{r_0}
\end{align*}
where $\bm{r_0} = \bm{A} \bm{x^{\star}} - \bm{A} \bm{x_0}$. A natural question that arises is that can we find a closed form solution of the above equation? To answer this question we need to enlist the help of the Cayley-Hamilton theorem.
\begin{thm}[Cayley-Hamilton] \label{theorem: cayley_amilton}
    Let $p_n \left( \lambda \right) = \sum_{i=0}^{n} c_i \lambda^{i}$ be the characteristic polynomial of the matrix $\bm{A} \in \CC^{n \times n}$, then $p_n \left( \bm{A} \right) = \bm{0}$. {\color{red} \textbf{THIS NEEDS A CITATION}}
\end{thm}
The Cayley-Hamilton theorem implies that
\begin{align*}
    \bm{0}      & = c_0 + c_1 \bm{A} + \ldots + c_{n-1} \bm{A}^{n-1} + c_{n} \bm{A}^{n}                      \\
    \bm{0}      & = \bm{A}^{-1} c_0 + c_1 + \ldots + c_{n-1} \bm{A}^{n-2} + c_{n} \bm{A}^{n-1}               \\
    \bm{A}^{-1} & = \alpha_1 \Id_{n \times n} + \ldots + \alpha_{n-1} \bm{A}^{n-2} + \alpha_{n} \bm{A}^{n-1}
\end{align*}
where $\alpha_i = -c_i / c_0$. The above demonstrates that $\bm{A}^{-1}$ can be represented as a matrix polynomial of degree $n-1$. This means that $\sum_{k=0}^{\infty} \left( \Id - \bm{A} \right)^k$ indeed possess a closed form solution namely
\begin{equation} \label{eq: x_ast_via_cayley}
    \bm{x^{\star}} = \bm{x}_0 + \bm{A}^{-1} \bm{r}_0 = \bm{x}_0 + \alpha_1 \bm{r}_0 + \ldots + \alpha_{n-1} \bm{A}^{n-2} \bm{r}_0 + \alpha_{n} \bm{A}^{n-1} \bm{r}_0.
\end{equation}
This also shows that $\bm{x^{\star}} \in \bm{x}_0 + \operatorname{l.s} \left\{ \bm{r}_0, \bm{A} \bm{r}_0, \bm{A}^2 \bm{r}_0, \ldots , \bm{A}^{n-1} \bm{r}_0 \right\}$. One idea for finding a solution to equation \ref{eq: lin_sys_1} is to use our routine for evaluting $\bm{A} \bm{v}$ to iteratively compute new basis elements for the space generated by $\left\{ \bm{r}_0, \bm{A} \bm{r}_0, \bm{A}^2 \bm{r}_0, \ldots , \bm{A}^{n-1} \bm{r}_0 \right\}$ and at each step carefully choosing a $\bm{x_k}$ such that $\bm{x_k}$ approaches $\bm{x^{\star}}$, in some form. The subspace constructed using this technique is so important that is has its own name.
\begin{defe}[Krylov Subspace] \label{defe: krylov_subspace}
    The Krylov Subspace of order $k$ generated by the matrix $\bm{A} \in \CC^{n \times n}$ and the vector $\bm{v} \in \CC^{n}$ is defined as
    \[
        \calK_{k} \left( \bm{A},\bm{v} \right) = \operatorname{l.s} \left\{ \bm{r}_0, \bm{A} \bm{r}_0, \bm{A}^2 \bm{r}_0, \ldots , \bm{A}^{k-1} \bm{r}_0 \right\}
    \]
    for $k \geq 1$ and $\calK_{0} \left( \bm{A},\bm{v} \right) = \left\{ \bm{0} \right\}$ \cite{TrefethenLloydN.LloydNicholas1997Nla/}*{page 245}.
\end{defe}
For the purposes of solving equation \ref{eq: lin_sys_1} it is of much interest to understand how $\calK_{k} \left( \bm{A},\bm{v} \right)$ grows for larger and larger $k$ since a solution for equation \ref{eq: lin_sys_1} will be present in a Krylov Subspace that cannot be grown any larger, according to equation \ref{eq: x_ast_via_cayley}. In other words, an exact solution can be constructed once we have extracted all the information from $\bm{A}$ through multiplication of $\bm{r_0}$. The following theorem provides information on how exactly the Krylov Subspace grows as $k$ increases.
\begin{thm} \label{theorem: grade_of_v}
    There is a positive called the grade of $\bm{v}$ with respect to $\bm{A}$, denoted $t_{\bm{v}, \bm{A}}$, where
    \[
        \operatorname{dim} \left( \calK_{k} \left( \bm{A} , \bm{v} \right) \right) = \left\{
        \begin{matrix}
            k,                  & k \leq t_{\bm{v}, \bm{A}} \\
            t_{\bm{v}, \bm{A}}, & k \geq t_{\bm{v}, \bm{A}}
        \end{matrix}
        \right.
    \]
\end{thm}
Theorem \ref{theorem: grade_of_v} essentially tells us for $k \leq t_{\bm{v}, \bm{A}}$ that $\bm{A}^k \bm{v}$ is linearly independent to $\bm{A}^i \bm{v}$ for $0 \leq i \leq k-1$ meaning $\left\{ \bm{v}, \bm{A} \bm{v}, \bm{A}^2 \bm{v}, \ldots , \bm{A}^{k-1} \bm{v} \right\}$ serves as a basis for $\calK_{k} \left( \bm{A},\bm{v} \right)$ and $\calK_{k-1} \left( \bm{A},\bm{v} \right) \subsetneq \calK_{k} \left( \bm{A},\bm{v} \right)$. Conversely, any new vectors formed beyond $t_{\bm{v}, \bm{A}}$ will be linearly independent meaning $\calK_{k} \left( \bm{A},\bm{v} \right) \subsetneq \calK_{k+1} \left( \bm{A},\bm{v} \right)$ for $k \geq t_{\bm{v}, \bm{A}}$. While $t_{\bm{v}, \bm{A}}$ obviously plays a central role in determining a suitable basis whose span contains $\bm{A}^{-1} \bm{b}$, its importance is made abundantly clear in the following corollary.
\begin{cor} \label{theorem: grade_as_min}
    \[
        t_{\bm{v}, \bm{A}} = \min \left\{k \mid \bm{A}^{-1} \bm{v} \in \calK_{k} \left( \bm{A},\bm{v} \right) \right\}
    \]
\end{cor}
\begin{proof}
    Recall from Cayley-Hamilton (theorem \ref{theorem: cayley_amilton}) that
    \[
        \bm{A}^{-1} \bm{v} = \sum_{i=0}^{n-1} \alpha_{i} \bm{A}^{i} \bm{v}
    \]
    But since $\calK_{k} \left( \bm{A},\bm{v} \right) = \calK_{k+1} \left( \bm{A},\bm{v} \right)$ for $k \geq t_{\bm{v}, \bm{A}}$
    \[
        \bm{A}^{-1} \bm{v} = \sum_{i=0}^{t-1} \beta_{i} \bm{A}^{i} \bm{v}
    \]
    meaing $\bm{A}^{-1} \bm{v} \in \calK_{k} \left( \bm{A},\bm{v} \right)$ for $k \geq t_{\bm{v}, \bm{A}}$. Suppose for the sake of contradiction that this also holds for $k = t_{\bm{v}, \bm{A}} - 1$, that is, $\bm{A}^{-1} \bm{v} = \sum_{i=0}^{t-2} \gamma_{i} \bm{A}^{i} \bm{v}$. However, this gives
    \[
        \bm{v} = \sum_{i=0}^{t-2} \gamma_{i} \bm{A}^{i+1} \bm{v} = \sum_{i=0}^{t-1} \gamma_{i-1} \bm{A}^{i} \bm{v}
    \]
    implying $\left\{ \bm{v}, \bm{A} \bm{v}, \bm{A}^2 \bm{v}, \ldots , \bm{A}^{t-1} \bm{v} \right\}$ are linearly dependent which means that $\operatorname{dim} \left( \calK_{k} \left( \bm{A} , \bm{v} \right) \right) < t$, which provides us with our contradiction.
\end{proof}
This allows us to make a much stronger statement on the where abouts of $\bm{x^{\star}}$ in relation to the Krylov Subspaces.
\begin{cor} \label{theorem: sol_in_krylov}
    For any $\bm{x_0}$ we have
    \[
        \bm{x^{\star}} \in \bm{x_0} + \calK_{t_{\bm{r_0}, \bm{A}}} \left( \bm{A},\bm{r_0} \right)
    \]
    where $\bm{r_0} = \bm{b} - \bm{A} \bm{x_0}$.
\end{cor}