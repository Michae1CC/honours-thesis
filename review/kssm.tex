\subsection{Krylov Subspace Methods}\label{Section1.1}

In this section we will focus on how iterative methods, in particular a class of iterative methods called Krylov Subspace methods, may be used to solve a linear system $\bm{A} \bm{x} = \bm{b}$. While non-iterative methods exist to solve such systems virtually all of them carry an unwieldy runtime of $\mathcal{O} \left( n^3 \right)$ for a system of $n$ parameters. Even for current computer systems, this renders many common matrix problems untractable. Consequently the focus of solving linear systems has shifted towards iterative methods. While iterative methods typically demand certain structural properties of the matrices, such as symmetry and positive definiteness, this generally is not a problem since the majority of large matrix problems that, by mature, endow these systems with the desired properties. For example, in the context of this paper the Gram matrices used to solve linear systems in Gaussian Processes possess both symmetry and positive definiteness. There are also a number of other properties of iterative methods which make them rather attractive to users. To start, iterative Krylov subspace methods are guranteed to converge to an exact solution within a finite number of iterations and even if the method is prematurely stopped before reaching an exact solution, the approximation obtained on the final iteration will in some sense be a good enough estimate of our exact solution. Furthermore, unlike most non-iterative methods, Krylov subspace methods do not require an explicit form of the matrix $\bm{A}$ and instead only requires some routine or process for computing $\bm{A} \bm{x}$.

\subsubsection{Krylov Subspaces}\label{Section1.1.1}

We will motivate the Krylov subspaces by observing their usefullness in solving linear systems. To this end, consider the problem of solving the linear system
\begin{equation}\label{eq:lin_sys_1}
    \bm{A} \bm{x^{\star}} = \bm{b}
\end{equation}
where no explicit form of $\bm{A}$ is available and instead one must draw information from $\bm{A}$ solely through a routine that can evaluate $\bm{A} \bm{v}$ for any $\bm{v}$. How could this routine be utilized in such a manner to provide with a solution to equation \ref{eq:lin_sys_1}? Before answering this, consider the following theorem

\begin{thm} \label{theorem: invert_mat_norm}
    For $\bm{A} \in \KK^{n \times n}$ if $\| \bm{A} \| = q < 1$ then $\Id - \bm{A}$ is invertible and its inverse admits the following representation
    \[
        \left( \Id - \bm{A} \right)^{-1} = \sum_{k=0}^{\infty} \bm{A}^k.
    \]
    \cite{BerezanskyMakarovich1996FaV1}
\end{thm}
Consider a matrix for which $\| \bm{A} \| < 2$, it follows that $\| \Id - \bm{A} \| < 1$ meaning $\Id - \left( \Id - \bm{A} \right)$ is invertible and $\bm{A}^{-1} = \left( \Id - \left( \Id - \bm{A} \right) \right)^{-1} = \sum_{k=0}^{\infty} \left( \Id - \bm{A} \right)^{k}$. Thinking back to equation \ref{eq:lin_sys_1} for any $x_0 \in \KK^{n}$ we have
\begin{align*}
    \bm{x^{\star}} & = \bm{A}^{-1} \bm{b} = \bm{A}^{-1} \left( \bm{A} \bm{x^{\star}} - \bm{A} \bm{x_0} + \bm{A} \bm{x_0} \right) \\
                   & = \bm{x_0} + \bm{A}^{-1} \bm{r_0}                                                                           \\
                   & = \bm{x_0} + \sum_{k=0}^{\infty} \left( \Id - \bm{A} \right)^k
\end{align*}
where $\bm{r_0} = \bm{A} \bm{x^{\star}} - \bm{A} \bm{x_0}$. A natural question that arises is that can we find a closed form solution of the above equation? To answer this question we need to enlist the help of the Cayley-Hamilton theorem.
\begin{thm}[Cayley-Hamilton] \label{theorem: cayley_amilton}
    Let $p_n \left( \lambda \right) = \sum_{i=0}^{n} c_i \lambda^{i}$ be the characteristic polynomial of the matrix $\bm{A} \in \KK^{n \times n}$, then $p_n \left( \bm{A} \right) = \bm{0}$. {\color{red} \textbf{THIS NEEDS A CITATION}}
\end{thm}
The Cayley-Hamilton theorem implies that
\begin{align*}
    0           & = c_0 + c_1 \bm{A} + \ldots + c_{n-1} \bm{A}^{n-1} + c_{n} \bm{A}^{n}           \\
    0           & = \bm{A}^{-1} c_0 + c_1 + \ldots + c_{n-1} \bm{A}^{n-2} + c_{n} \bm{A}^{n-1}    \\
    \bm{A}^{-1} & = \alpha_0 + c_1 + \ldots + \alpha_{n-1} \bm{A}^{n-2} + \alpha_{n} \bm{A}^{n-1}
\end{align*}
where $\alpha_i = -c_i / c_0$. This demonstrates that $\bm{A}^{-1}$ can be represented as a matrix polynomial of degree $n-1$. This means that $\sum_{k=0}^{\infty} \left( \Id - \bm{A} \right)^k$ indeed possess a closed form solution namely
\[
    \bm{x^{\star}} = \bm{x_0} + \bm{A}^{-1} \bm{r_0} = \alpha_0 + c_1 + \ldots + \alpha_{n-1} \bm{A}^{n-2} + \alpha_{n} \bm{A}^{n-1}.
\]
This also shows that $\bm{x^{\star}} \in \operatorname{l.s} \left\{ \bm{r_0}, \bm{A} \bm{r_0}, \bm{A}^2 \bm{r_0}, \ldots , \bm{A}^{n-1} \bm{r_0} \right\}$. One idea for finding a solution to equation \ref{eq:lin_sys_1} is to use our routine for evaluting $\bm{A} \bm{v}$ to iteratively compute new basis elements for the space generated by $\left\{ \bm{r_0}, \bm{A} \bm{r_0}, \bm{A}^2 \bm{r_0}, \ldots , \bm{A}^{n-1} \bm{r_0} \right\}$ and at each step carefully choosing a $\bm{x_k}$ such that $\bm{x_k}$ approaches $\bm{x^{\star}}$, in some form. The subspace constructed using this technique is so important that is has its own name.
\begin{defe}[Krylov Subspace] \label{defe: krylov_subspace}
    The Krylov Subspace of order $k$ generated by the matrix $\bm{A} \in \KK^{n \times n}$ and the vector $\bm{v} \in \KK$ is defined as
    \[
        \calK_{k} \left( \bm{A},\bm{v} \right) = \operatorname{l.s} \left\{ \bm{r_0}, \bm{A} \bm{r_0}, \bm{A}^2 \bm{r_0}, \ldots , \bm{A}^{n-1} \bm{r_0} \right\}
    \]
    for $k \geq 1$ and $\calK_{k} \left( \bm{A},\bm{v} \right) = \left\{ \bm{0} \right\}$.
\end{defe}
For the purposes of solving equation \ref{eq:lin_sys_1} it is of much interest to understand how $\calK_{k} \left( \bm{A},\bm{v} \right)$ grows for larger and larger $k$ since a solution for equation \ref{eq:lin_sys_1} will be present in a Krylov Subspace that cannot be grown any larger. In other words, an exact solution can be constructed once we have extracted all the information from $\bm{A}$ through multiplication of $\bm{r_0}$. The following theorem provides information on how exactly the Krylov Subspace grows as $k$ increases.
\begin{thm} \label{theorem: grade_of_v}
    There is a positive called the grade of $\bm{v}$ with respect to $\bm{A}$, denoted $t_{\bm{v}, \bm{A}}$, where
    \[
        \operatorname{dim} \left( \calK_{k} \left( \bm{A} , \bm{v} \right) \right) = \left\{
        \begin{matrix}
            k, & k \leq t \\
            t, & k \geq t
        \end{matrix}
        \right.
    \]
\end{thm}
Theorem \ref{theorem: grade_of_v} essentially tells us that for $k \leq t_{\bm{v}, \bm{A}}$ that $\bm{A}^k \bm{v}$ is linearly independent to $\bm{A}^i \bm{v}$ for $0 \leq i \leq k-1$ meaning $\left\{ \bm{v}, \bm{A} \bm{v}, \bm{A}^2 \bm{v}, \ldots , \bm{A}^{n-1} \bm{v} \right\}$ serves as a basis for $\calK_{k} \left( \bm{A},\bm{v} \right)$ and that $\calK_{k-1} \left( \bm{A},\bm{v} \right) \subsetneq \calK_{k} \left( \bm{A},\bm{v} \right)$. Conversely, any new vectors formed beyond $t_{\bm{v}, \bm{A}}$ will be linearly independent meaning $\calK_{k} \left( \bm{A},\bm{v} \right) \subsetneq \calK_{k+1} \left( \bm{A},\bm{v} \right)$ for $k \geq t_{\bm{v}, \bm{A}}$. While $t_{\bm{v}, \bm{A}}$ clearly plays a role in determining a suitable basis for which $\bm{A}^{-1} \bm{b}$ lies in its importance is made abundantly clear in the following corollary.
\begin{cor} \label{theorem: grade_as_min}
    \[
        t_{\bm{v}, \bm{A}} = \min \left\{k \mid \bm{A}^{-1} \bm{v} \in \calK_{k} \left( \bm{A},\bm{v} \right) \right\}
    \]
\end{cor}
\begin{proof}
    Recall from Cayley-Hamilton (theorem \ref{theorem: cayley_amilton}) that
    \[
        \bm{A}^{-1} \bm{v} = \sum_{i=0}^{n-1} \alpha_{i} \bm{A}^{i} \bm{v}
    \]
    But since $\calK_{k} \left( \bm{A},\bm{v} \right) = \calK_{k+1} \left( \bm{A},\bm{v} \right)$ for $k \geq t_{\bm{v}, \bm{A}}$
    \[
        \bm{A}^{-1} \bm{v} = \sum_{i=0}^{t-1} \beta_{i} \bm{A}^{i} \bm{v}
    \]
    meaing $\bm{A}^{-1} \bm{v} \in \calK_{k} \left( \bm{A},\bm{v} \right)$ for $k \geq t_{\bm{v}, \bm{A}}$. Suppose for the sake of contradiction that this also holds for $k = t_{\bm{v}, \bm{A}} - 1$, that is, $\bm{A}^{-1} \bm{v} = \sum_{i=0}^{t-2} \gamma_{i} \bm{A}^{i} \bm{v}$. However, this gives
    \[
        \bm{v} = \sum_{i=0}^{t-2} \gamma_{i} \bm{A}^{i+1} \bm{v} = \sum_{i=0}^{t-1} \gamma_{i-1} \bm{A}^{i} \bm{v}
    \]
    implying $\left\{ \bm{v}, \bm{A} \bm{v}, \bm{A}^2 \bm{v}, \ldots , \bm{A}^{t-1} \bm{v} \right\}$ are linearly dependent which means that $\operatorname{dim} \left( \calK_{k} \left( \bm{A} , \bm{v} \right) \right) < t$, which provides us with our contrdiction.
\end{proof}
This machinery allows us to make a much stronger statement on the where abouts of $\bm{x^{\star}}$ in relation to the Krylov Subspaces.
\begin{cor} \label{theorem: sol_in_krylov}
    For any $\bm{x_0}$, we have
    \[
        \bm{x^{\star}} \in \bm{x_0} + \calK_{t_{\bm{r_0}, \bm{A}}} \left( \bm{A},\bm{r_0} \right)
    \]
    where $\bm{r_0} = \bm{b} - \bm{A} \bm{x_0}$.
\end{cor}