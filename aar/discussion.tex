\subsection{Discussion}\label{Section5.3}

\subsubsection{Kernel Matrix approximation}\label{Section5.3.1}

Starting with the performance of the Nystrom methods, assessing Figures \ref{fig: nys-3dsn-froberr} to \ref{fig: nys-wine-abserr}, overall the rls method is certainly the best methods among the sampling distributions as it virtually always is among the top three methods for any combination of dataset, $k$ or $\sigma$. Interestingly, non-uniform sampling techniques performed better with datasets for which the spectrum of the corresponding kernel matrix was also non-uniform. For example the spectrum for the kernel matrices of the magic04 and Stocks dataset (see Figures \ref{fig: magic-spec} and \ref{fig: sm-spec}) are relatively uniform and from Figures \ref{fig: nys-magic-froberr} and \ref{fig: nys-sm-froberr} we find that the non-uniform sampling techniques give rather poor approximations and generally behave just as bad, or even worse, than the naive uniform sampling distribution. In contrast, non-uniform methods really shone for datasets such as the 3DSN and Wine datasets where some of the better methods could provide almost exact kernel matrix approximations (refer to Figures \ref{fig: nys-3dsn-froberr}, \ref{fig: nys-3dsn-abserr}, \ref{fig: nys-wine-froberr} and \ref{fig: nys-wine-abserr}) after a few thousand samples. This phenomena is likely due to the fact that when the spectrum of a matrix is less uniform, it decays faster, meaning that the matrix can be better expressed as a low-rank approximation. Evidently, the majority column sampling are intelligent enough to select columns that are best representative in this low rank approximation, most of the time. What is especially interesting about the Abalone dataset is, despite the rls sampling distribution being almost uniform (compared to other sampling distributions, see Figures \ref{fig: nys-aba-scores} and \ref{fig: rls-aba-scores}), the rls methods seems to perform significantly better than uniform selection, shown in Figure \ref{fig: nys-aba-froberr}. The small amount of variation in the rls sampling distribution presumably provides it with a large enough advantage to outperform the uniform method and select columns that provide more information to the lower rank approximation.

Looking at the RFF methods, we find that the ORF and SORF methods do not really live up to their acclaimed theoretical error bounds. While the ORF and SORF methods do occasionally produce better errors over the standard transformation matrix sampling (see Figures \ref{fig: rff-3dsn-froberr} to \ref{fig: rff-wine-abserr}), it is hard to argue using either of these methods as neither them offer no obvious advantage in terms of time and memory saving (see Figures TODO). This may come as a surprise for most with the SORF method under delivering, even with a JIT compiler was used to speed up various parts of the algorithm. This dissonance between the theoretical bounds presented in \Cref{table: RFF-compare} and these "real world" experiments is that the single line of code to produce the i.i.d. Gaussian matrix for the standard transformation matrix using the numpy python library has been incredibly well engineered and optimized to produce matrix samples brilliantly fast and designed to cope with large scaling. When given large enough datasets, the SORF method likely will overtake the naive sampling method. Unfortunately, due to the memory constraints of the computers used for this project, such large data sets could not be experimented with.

To make a fair comparison in approximation matrix construction between the RFF and Nystrom methods, the time that each method used to build an approximation was graphed along side the corresponding relative error. In this setup, we can think about each method having some sort of time budget and that its task is to produce the best approximation it can within this time limit, regardless of how many samples it requires or how big of a feature space it uses. Comparing the methods in this manner, the Nystrom family is far better at lowering the Frobenius error in the approximations it produces, while the RFF methods provide approximations with smaller Infinity errors (see Figures \ref{fig: cmp-3dsn-froberr} to \ref{fig: cmp-wine-abserr}). In other words, the entries of the approximations produced by Nystrom methods are {\it on average} better than those from RFF methods, while {\it in the worst case} the entries of the approximations produced by RFF are better than those from Nystrom methods. This makes sense since much of the theory in the Nystrom methods was aimed towards lowering the Frobenius errors, while the theory for the RFF methods were aimed towards lowering the Infinity errors. One Final thing worth mentioning is that RFF methods seemed to require much larger amounts of memory to produce their approximations, sometimes double or even triple the amount of memory the Nystrom methods used (refer to Figures TODO).