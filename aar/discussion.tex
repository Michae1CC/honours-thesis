\subsection{Discussion}\label{Section5.3}

\subsubsection{Kernel Matrix Approximation}\label{Section5.3.1}

Starting with the performance of the Nystrom methods, assessing Figures \ref{fig: nys-3dsn-froberr} to \ref{fig: nys-wine-abserr}, overall the rls method is certainly the best methods among the sampling distributions as it virtually always is among the top three methods for any combination of dataset, $k$ or $\sigma$. Interestingly, non-uniform sampling techniques performed better with datasets for which the spectrum of the corresponding kernel matrix was also non-uniform. For example the spectrum for the kernel matrices of the magic04 and Stocks dataset (see Figures \ref{fig: magic-spec} and \ref{fig: sm-spec}) are relatively uniform and from Figures \ref{fig: nys-magic-froberr} and \ref{fig: nys-sm-froberr} we find that the non-uniform sampling techniques give rather poor approximations and generally behave just as bad, or even worse, than the naive uniform sampling distribution. In contrast, non-uniform methods very much shine for datasets such as the 3DSN and Wine datasets where some of the better methods could provide almost exact kernel matrix approximations (refer to Figures \ref{fig: nys-3dsn-froberr}, \ref{fig: nys-3dsn-abserr}, \ref{fig: nys-wine-froberr} and \ref{fig: nys-wine-abserr}) after a few thousand samples. This phenomena is likely due to the fact that when the spectrum of a matrix is less uniform, it decays faster, meaning that the matrix can be better expressed as a low-rank approximation. Evidently, the majority of non-uniform sampling distributions are intelligent enough to select columns that are best representative in this low rank approximation, most of the time. What is especially interesting about the Abalone dataset is, despite the rls sampling distribution being almost uniform (compared to other sampling distributions, see Figures \ref{fig: nys-aba-scores} and \ref{fig: rls-aba-scores}), the rls methods seems to perform significantly better than uniform selection, shown in \Cref{fig: nys-aba-froberr}. The small amount of variation in the rls sampling distribution presumably gives it with a large enough advantage over the uniform method and selects columns that better provides information to the lower rank approximation.

Looking at the RFF methods, we find that the ORF and SORF methods do not really live up to their acclaimed theoretical error bounds. While the ORF and SORF methods do occasionally produce better errors over the standard transformation matrix sampling (see Figures \ref{fig: rff-3dsn-froberr} to \ref{fig: rff-wine-abserr}), it is hard to argue using either of these methods as neither them offer no obvious advantage in terms of time and memory saving (see Figures TODO). This may come as a surprise for most with the SORF method under deliverance, even with a JIT compiler was used to speed up various parts of the algorithm. This dissonance between the theoretical bounds presented in \Cref{table: RFF-compare} and these "real world" experiments is that the single line of code to produce the i.i.d. Gaussian matrix for the standard transformation matrix using the numpy python library has been incredibly well engineered and optimized to produce matrix samples brilliantly fast and is designed to cope with large scaling. When given large enough datasets, the SORF method likely will overtake the naive sampling method. Unfortunately, due to the memory constraints of the computers used for this project, such large data sets could not be experimented with.

To make a fair comparison in kernel matrix approximation between the RFF and Nystrom methods, the time that each method used to build an approximation was graphed along side the corresponding relative error. In this setup, we can think about each method having some sort of time budget and that its task is to produce the best approximation it can within this time limit, regardless of how many samples it requires or how big of a feature space it uses. Comparing the methods in this manner, the Nystrom family is far better at lowering the Frobenius error in the approximations it produces, while the RFF methods provide approximations with smaller infinity errors (see Figures \ref{fig: cmp-3dsn-froberr} to \ref{fig: cmp-wine-abserr}). In other words, the entries of the approximations produced by Nystrom methods are {\it on average} better than those from RFF methods, while {\it in the worst case} the entries of the approximations produced by RFF are better than those from Nystrom methods. This makes sense since much of the theory in the Nystrom methods was aimed towards lowering the Frobenius errors, while the theory behind the RFF methods was aimed towards lowering the infinity errors. One final point worth mentioning is that RFF methods seemed to require much larger amounts of memory to produce their approximations, sometimes double or even triple the amount of memory the Nystrom methods used (refer to Figures TODO).

Next, two methods from the nystrom family and one mehtod from the family that we deemed to be the best were used with Gaussian Process algorithm to help form predictions with the same datasets. The rls and data columns methods were chosen to represent the best of the nystrom family while the Rami and Recht's "plain" RFF method was chosen to represent the best of the RFF techniques. Similar to relative error comparisons, the time that each method used to an approximation was graphed along side the corresponding prediction error. The mean square error was used and classification accuracy where used to measure the success of prediction for both regression and classification tasks, respectively. Looking at the results, no method seemed to be a clear winner. In general the "plain" RFF method usually delivered the best predictions with smaller time budgets were the rls generally caught up and overtook it when a larger time budget was allowed. Moreover, the data columns method, overall, yields the poorest predictions errors among the selected methods. Thus, the RFF method would probably be the best method to use in practice for two main reasons. The first reason is, if one was to select an inexact method for kernel matrix production, it likely means that they do not have a large time budget to begin meaning the RFF ability to produce the best result with small time budgets makes it highly desirable to use. Furthermore, the time taken to construct probability distributions used the nystrom methods art not included in Figure TODO (and can instead be found in ...) making it harder to justify using the rls method if the probability distribution is not already available since a much larger time budget would be required to deliver the same level of prediction accuracy.

\subsubsection{Krylov Subspace Methods}\label{Section5.3.2}

The CG and MINRES methods where also used within the Gaussian Process algorithm to see which one of the two would provide better predictions. The number of iterations that each method used to form an approximation was graphed along side the corresponding prediction error produced. Differences in execution time per iteration were nominal between the two methods. Look at the result in Figure TODO, it seems that the MINRES method performs just as well and, occasionally, even better than CG which is particular apparent in the abalone and 3DSN datasets. In particular, the MINRES method seems to give better results even of the quadratic dataset. The quadratic dataset is an artificial dataset constructed using a Gaussian process with a quadratic mean function $x^2 + y^2$ and some amount of variance. The purpose of this dataset was to check if applied method was prone to overfitting. The fact that, overall, MINRES gave better error indicates that it is also more robust to overfitting than CG. To understand why MINRES may produce better errors for regression tasks, recall that the mean square error is computed in the following manner
\begin{align*}
    \operatorname{MSE} \left( \overline{\bm{f}_{\star}} \right) \
     & = \frac{1}{n} \norm{\overline{\bm{f}_{\star}} - \bm{y}_{\star}}_2^2                                                                                                                                \\
     & = \frac{1}{n} \norm{\bm{K_{X_{\star} X}} \operatorname{lin-solve} \left( \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} , \bm{y} \right) - \bm{y}_{\star}}_2^2                                          \\
     & \leq \frac{1}{n} \left( \norm{\bm{K_{X_{\star} X}}}_2^2 \norm{\operatorname{lin-solve} \left( \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} , \bm{y} \right)}_2^2 + \norm{\bm{y}_{\star}}_2^2 \right).
\end{align*}
From the above result, we find that the mean square error is bounded above by the Euclidean norm of $\operatorname{lin-solve} \left( \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} , \bm{y} \right)$. The MINRES method seeks to solve $\operatorname{lin-solve} \left( \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} , \bm{y} \right)$ by finding vectors that successively minimize the Euclidean-distance that is
\begin{equation*}
    \bm{x}_k = \argmin_{\bm{x} \in \bm{x}_0 + \calK_k} \norm{\left[ \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} \right]^{-1} \bm {x} - \bm{y}}_2.
\end{equation*}
In contrast, CG instead seeks vectors that successively minimize $\| \bm{r}_{k} \|_{\left[ \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} \right]^{-1}}$ at the $k^{th}$, which we are not so interested in when look at test accuracies. The success of MINRES is likely owed to its ability to more directly lower the Euclidean distance of $\operatorname{lin-solve} \left( \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} , \bm{y} \right)$ which bounds the prediction mean square error.