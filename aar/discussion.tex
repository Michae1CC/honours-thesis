\subsection{Discussion}\label{Section5.3}

\subsubsection{Kernel Matrix Approximation}\label{Section5.3.1}

Starting with the performance of the Nystrom methods, assessing Figures \ref{fig: nys-3dsn-froberr} to \ref{fig: nys-wine-abserr}, overall the rls method is superior for sampling distributions as it is virtually always among the top three methods for any combination of dataset, $k$ or $\sigma$. Interestingly, non-uniform sampling techniques performed better with datasets for which the spectrum of the corresponding kernel matrix was also non-uniform. For example, the spectrum for the kernel matrices of the magic04 and Stocks dataset (see Figures \ref{fig: magic-spec} and \ref{fig: sm-spec}) are relatively uniform and from Figures \ref{fig: nys-magic-froberr} and \ref{fig: nys-sm-froberr} we find that the non-uniform sampling techniques give rather poor approximations and generally behave just as poorly, or even worse, than the naive uniform sampling distribution. In contrast, non-uniform methods very much shine in the case of the 3DSN and Wine datasets where some of the better methods could provide almost exact kernel matrix approximations (refer to Figures \ref{fig: nys-3dsn-froberr}, \ref{fig: nys-3dsn-abserr}, \ref{fig: nys-wine-froberr} and \ref{fig: nys-wine-abserr}) after a few thousand samples. A good comparison of these two scenarios is seen in \Cref{fig: nys-uni-vs-nonuni}.

\begin{figure}[ht]
    \centering
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=1]{img/nystrom/abalone/rel_error/nys-sigma=1.0-k=20.png}
        \end{adjustbox}
    }
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=1]{img/nystrom/stock_market/rel_error/nys-sigma=1.0-k=80.png}
        \end{adjustbox}
    }
    \caption{Non-uniform sampling distributions perform much better when the corresponding kernel matrix also has a non-uniform spectrum, as seen in the left panel were various sampling distributions are used to construct the Abalone kernel matrix. In contrast, there is not much difference between sampling distributions for kernel matrices with highly uniform spectrums, as depicted in the right panel.}
    \label{fig: nys-uni-vs-nonuni}
\end{figure}

This phenomena is likely due to the fact that the spectrum of a matrix decays faster when it is less uniform, meaning that the matrix can be better expressed as a low-rank approximation. Evidently, the majority of non-uniform sampling distributions are intelligent enough to select columns that are best representative in this low rank approximation, most of the time. What is especially interesting about the Abalone dataset is, despite the rls sampling distribution being almost uniform (compared to other sampling distributions, see Figures \ref{fig: nys-aba-scores} and \ref{fig: rls-aba-scores}), this method seems to perform significantly better than uniform selection, shown in \Cref{fig: nys-aba-froberr}. The small amount of variation in the rls sampling distribution presumably gives it an edge over the uniform method in selecting columns that better provides information to the lower rank approximation.

Turning our attention to the RFF methods, we find that the ORF and SORF methods do not really live up to their acclaimed theoretical error bounds. While the ORF and SORF methods do occasionally produce better errors over the standard transformation matrix sampling (see Figures \ref{fig: rff-3dsn-froberr}-\ref{fig: rff-wine-abserr}), it is hard to justify using either of these methods as neither offer any obvious advantage in terms of time and memory saving (see Figures \ref{fig: rff-3dsn-times}-\ref{fig: rff-wine-times} and \ref{fig: rff-3dsn-mem}-\ref{fig: rff-wine-mem}). This may come as a surprise given the SORF method, even with a JIT compiler, was used to speed up various parts of the algorithm. This dissonance between the theoretical bounds presented in \Cref{table: RFF-compare} and these "real world" experiments is that generating Gaussian variables is still, in practice, quicker than the asymptotically more efficient construction of SORF's transform matrix. When fed large enough datasets, the SORF method is likely to overtake the "basic" sampling method proposed by Rami and Recht. Unfortunately, due to the memory constraints of the computers used for this project's experimental, using such large data sets could not be conducted to test this hypothesis.

To make a fair comparison in kernel matrix approximation between the RFF and Nystrom methods, the time that each method used to build an approximation was graphed alongside the corresponding relative error. In this setup, we can think about each method as having some sort of time budget and that its task is to produce the best approximation it can within this limit, regardless of how many samples it requires or how big of a feature space it uses. Comparing the methods in this manner the Nystrom family is superior in lowering the Frobenius error in the approximations it produces, while the RFF methods provide approximations with smaller infinity errors (see Figures \ref{fig: cmp-3dsn-froberr}-\ref{fig: cmp-wine-abserr}). In other words, Nystrom methods produce approximations where entries are, {\it on average}, closer to the true matrix compared to RFF methods. RFF methods, on the otherhand, produce approximations where entries are {\it in the worst case} closer to the true matrix compared to the Nystrom methods. This makes sense since much of the theory behind the Nystrom methods is aimed toward lowering Frobenius errors, while RFF methods focuses on lowering infinity errors. An example of this is seen in \Cref{fig: nys-err-vs-rff-err}.

\begin{figure}[H]
    \centering
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=1]{img/kern_time_vs_err/abalone/rel_error/cmp-sigma=1.0-k=20.png}
        \end{adjustbox}
    }
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=1]{img/kern_time_vs_err/abalone/abs_error/cmp-sigma=1.0-k=20.png}
        \end{adjustbox}
    }
    \caption{This gives a comparison of errors for constructing the kernel matrix associated with the Abalone dataset. Nystrom methods (graphed with primary colors) generally provided better Frobenius errors (left panel) while RFF methods (graphed with composite colors) gave better infinity errors (right panel).}
    \label{fig: nys-err-vs-rff-err}
\end{figure}

One final point worth mentioning is that RFF methods seemed to require much larger amounts of memory to produce their approximations, sometimes double or even triple the amount of memory the of Nystrom methods used shown in \Cref{fig: nys-vs-rff-mem} (also see Figures \ref{fig: nys-3dsn-mem-use}-\ref{fig: nys-wine-mem-use} and Figures \ref{fig: rff-3dsn-mem}-\ref{fig: rff-wine-mem}).

\begin{figure}[ht]
    \centering
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=1]{img/nystrom/abalone/add_mem/nys-sigma=1.0-k=10.png}
        \end{adjustbox}
    }
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=1]{img/rff/abalone/add_mem/rff-sigma=1.0.png}
        \end{adjustbox}
    }\\
    \caption{A comparison of memory usage for constructing the kernel matrix associated with the Abalone dataset. RFF methods (right panel) typically had much larger memory footprints compared to Nystrom methods (left panel). Keep in mind that the interpretation of "samples" (on the horizontal axis) changes depending on the method.}
    \label{fig: nys-vs-rff-mem}
\end{figure}

Next, two methods from the Nystrom class and one method from the RFF class, that we deemed to be the best among their respective families, were used within the Gaussian Process algorithm (replacing exact kernel matrix creation) to form predictions. The rls and data columns methods were chosen to represent the best of the Nystrom family while the Rami and Recht's "basic" RFF method was selected to represent the best of the RFF techniques. Similar to relative error comparisons, the time each method required to complete an approximation was graphed along side the corresponding prediction error. The results, suggest that there was no clear winner. Generally speaking, Rami and Recht's RFF method delivered the best predictions with smaller time budgets were the rls generally caught up and overtook it when a larger time budget was allowed. The data columns method, tends to yield the poorest prediction errors among the selected methods. Thus, the RFF method would probably be the best method to use in practice for two main reasons. First, if one was to select an inexact method for kernel matrix production, it most likely means that there is not a large time budget. In this case RFF's ability to produce the best approximation within shorter timeframes makes it the best option. Second, the time taken to construct probability distributions used in the Nystrom methods are not included in Figures \ref{fig: cmp-3dsn-froberr}-\ref{fig: cmp-wine-abserr} (and can instead be found in Figures \ref{fig: prob_times-3dsn-scores}-\ref{fig: prob_times-wine-scores}) making it harder to justify using the rls method if the probability distribution is not already available since a much larger time budget would be required to deliver the same level of prediction accuracy.

\subsubsection{Krylov Subspace Methods}\label{Section5.3.2}

The CG and MINRES methods where also used within the Gaussian Process algorithm to see which one of the two would provide better predictions. Historically CG is preferred over MINRES for Gaussian process prediction since it is typically a superior linear system solver when dealing with PSD matrices. There are, however, a number of circumstances where MINRES can actually outperform CG. Take for example the work of Pleiss {\it et al.} \cite{arxiv200611267} which leverages a variant of MINRES to assist in the computation kernel matrix square roots. We were also wondering if MINRES is better suited as a linear solver within the context of Gaussian processes. In our experimentation, the number of iterations that each method used to form an approximation was graphed along side the corresponding prediction error produced. Differences in execution time per iteration between the two methods were nominal. Looking at the results in Figures \ref{fig: minres-cg-3dsn-ferr}-\ref{fig: minres-cg-Wine_data-ferr}, it appears that the MINRES method performs just as well and, occasionally, even better than CG. This was perhaps most apparent with the prediction errors produced from the abalone and 3DSN datasets, see \Cref{fig: minres-vs-cg-exact}.
\begin{figure}[ht]
    \centering
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=0.39]{img/pred/3D_spatial_network/pred_acc/lin_cmp-sigma=1.0.png}
        \end{adjustbox}
    }
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=0.39]{img/pred/abalone/pred_acc/lin_cmp-sigma=1.0.png}
        \end{adjustbox}
    }\\
    \caption{Prediction errors for the 3DSN (left panel) and Abalone (right panel) datasets make it most apparent that MINRES is the superior solver for GPR.}
    \label{fig: minres-vs-cg-exact}
\end{figure}
These differences became more noticeable when each of these linear solvers where paired with the RFF method (instead of using an exact kernel matrix) to provide predictions shown in \Cref{fig: minres-vs-cg-rff} (also see Figures \ref{fig: 3dsn-dual-cmp}-\ref{fig: Wine_data-dual-cmp}).
\begin{figure}[ht]
    \centering
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=1]{img/pred/3D_spatial_network/pred_acc/dual_cmp_both-sigma=1.0.png}
        \end{adjustbox}
    }
    \subfloat[]{
        \begin{adjustbox}{width=0.45\textwidth}
            \includegraphics[scale=1]{img/pred/abalone/pred_acc/dual_cmp_both-sigma=1.0.png}
        \end{adjustbox}
    }\\
    \caption{The difference in predictive performance between MINRES and CG is even more noticeable when the two methods are paired with a RFF kernel estimate. These difference are best seen when comparing prediction errors provided for the 3DSN (left panel) and Abalone (right panel) datasets.}
    \label{fig: minres-vs-cg-rff}
\end{figure}
In particular, the MINRES method affords better results even on the quadratic dataset. The quadratic dataset is an artificial dataset constructed using a Gaussian process with a quadratic mean function $x^2 + y^2$ and some amount of variance. Its main purpose was to ascertain if the applied method was prone to overfitting. The fact that, overall, MINRES yielded lower errors indicates that it is also more robust against overfitting compared to CG. To understand why MINRES may produce smaller errors for regression tasks, recall that the mean square error of $\bm{\rho}$ is computed as
\begin{align*}
    \norm{\bm{K_{X_{\star} X}} \bm{\rho} - \bm{y}_{\star}}_2^2
\end{align*}
where $\bm{\rho}$ is our best estimate for a solution to the linear system $\left[ \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} \right] \bm{\rho} = \bm{y}$. The MINRES method seeks to solve this linear system by solving an empirical version of the MSE up to a noise value of sigma, that is, $\bm{\rho}$ is chosen so that
\begin{equation*}
    \norm{\left[ \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} \right] \bm{\rho} - \bm{y}}_2^2
\end{equation*}
is minimized. In contrast, CG seeks vectors that successively minimizes the power norm of $\bm{\rho}$; something which we are not so interested in when comparing and contrasting test accuracies across different methods. While both MINRES and CG will eventually yield the same solution, the success of MINRES is likely due to its ability find early estimates of $\bm{\rho}$ that more directly lowers the empirical MSE. For small values of $\sigma_n^2$, will also correspond in a lower true MSE. If the above analysis is correct, then MINRES is really only advantageous when the value of sigma is small so that less weight is placed on the model's prior. Indeed, when the variance was artificially increased for both the abalone and quadratic dataset prediction, the errors between the CG and MINRES where much closer.

\begin{figure}[h]
    \centering
    \subfloat[]{
        \includegraphics[scale=0.35]{img/pred/abalone/pred_acc/dual_cmp_both-sigma=1.0_large_var.png}
    } \qquad
    \subfloat[]{
        \includegraphics[scale=0.35]{img/pred/rastrigin/pred_acc/dual_cmp_both-sigma=2.1_large_var.png}
    }
    \caption{Large variances place greater emphasis on the regression model's prior. This makes the true and empirical MSE more dissimilar causing MINRES to lose its edge over CG. The effect is best seen on the abalone (right panel) and the quadratic dataset (left panel).}
    \label{fig: large-var}
\end{figure}