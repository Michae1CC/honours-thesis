\subsection{Experimental Setup}\label{Section5.2}

Each method was implemented in python3 and run using the python3.8.5 interpreter distributed by the official python website. Various scientific computing python libraries, such as numpy, pandas and scipy, were used to help implement methods to enhance runtime performance. A just-in-time compiler was also used to improve the performance of the FWHT and Krylov-Subspace methods as fast runtime is critically important for the success of both these methods. Experiments were carried out on a variety of different datasets listed in \Cref{table: datasets}.

\begin{longtable}{lp{.50\textwidth}ccc}
    \caption{Descriptions and sources for each of the datasets used in experiments.}
    \label{table: datasets}
    \\\bottomrule
    \hline
    \emph{Name}                                                              & \emph{Description}                                                                                                                                                                   & $n$     & $d$                                                                       & \emph{Source}                                                                                                                 \\\midrule
    3DSN                                                                     & The 3D Spatial Network (3DSN) dataset was constructed by adding elevation information to a 2D road network in North Jutland, Denmark (covering a region of $185 \times 135 {km}^2$). & $2$     & $2000$                                                                    & \href{https://archive.ics.uci.edu/ml/datasets/3D+Road+Network+                                                                %28North+Jutland%2C+Denmark%29}{UCI}            \\
    Abalone                                                                  & Physical measurements of abalones.                                                                                                                                                   & $7$     & $4177$                                                                    & \href{https://archive.ics.uci.edu/ml/datasets/abalone}{UCI}                                                                   \\
    magic04                                                                  & Simulated registration of high energy
    gamma particles in a ground-based atmospheric Cherenkov gamma telescope. & $10$                                                                                                                                                                                 & $19020$ & \href{https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope}{UCI}                                                                                                                                 \\
    Wine                                                                     & Chemical measurements of wine.                                                                                                                                                       & $11$    & $4898$                                                                    & \href{https://archive.ics.uci.edu/ml/datasets/Wine+Quality}{UCI}                                                              \\
    Temp                                                                     & Weather station data from rural Queensland.                                                                                                                                          & $1$     & $11324$                                                                   & \href{https://www.longpaddock.qld.gov.au/}{Qld Gov}                                                                           \\
    Stocks                                                                   & Daily stock prices spanning 2000 to 2019.                                                                                                                                            & $4$     & $4904$                                                                    & \href{https://github.com/tiskw/random-fourier-features/blob/main/dataset/stockprice/download_stockprice_zipped_csv.py}{tiskw}
    \\\bottomrule
    \hline
\end{longtable}

\subsubsection{Kernel Martix Approximation Testing}\label{Section5.2.1}

To test the various kernel matrix approximation techniques, each method was used to construct an estimate of the actual kernel matrix with varying sample sizes. The definition of samples changes depending of which family of approximation technique is considered. For the Nystrom technique, the number of samples refers to $s$, that is the number of columns sampled. For the RFF technique the samples refer to $D$, that being the dimension of the constructed feature space. The Nystrom technique was implemented according to \cite{JMLR:v6:drineas05a} and RFF according to \cite{NIPS2007_013a006f} and \cite{LiuFanghui2021RFfK}. To see how well each technique was at approximating the kernel matrix by itself, each method was used to compute approximations for kernel matrices for each of the above data sets for a fixed $\sigma$ and sample size. This was repeated 30 times for each dataset. For every method the relative Frobenius error $\norm{\bm{K_{XX}} - \widehat{\bm{K_{XX}}}}_{F} / \norm{\bm{K_{XX}}}_{F}$ and relative infinity error $\norm{\bm{K_{XX}} - \widehat{\bm{K_{XX}}}}_{\infty} / \norm{\bm{K_{XX}}}_{\infty} = \norm{\bm{K_{XX}} - \widehat{\bm{K_{XX}}}}_{\infty}$ (in the case of the RBF kernel) was recorded, using the notation that $\widehat{\bm{K_{XX}}}$ represents an approximated kernel matrix. The values of $\sigma$ used were $0.1$, $1.0$ and $10.0$, although a $\sigma$ value of $2.1$ was also used for the Wine data set to compare with \cite{JMLR:v6:drineas05a} results. Additionally, computation time and memory usage for each kernel approximation was also recorded. It should be noted, however, that the time and memory spent on constructing probabilites for the Nystrom methods are not included in the kernel construction time and have instead been recorded separately in TODO. This is because the probabilites are typically known prior to kernel matrix construction and are simply used to provide an approximation to the kernel matrix instead on needing to save the entire data kernel matrix for new prediction. Thus they act as an efficient means to quickly save and rebuild the data kernel matrix. The errors, time and memory were average across the repetitions for each experiment.

To understand how well each kernel matrix approximation method performs in terms of providing predictions, each method was used to train a Gaussian process with ${4/5}^{ths}$ of each data set and required to predict the remaining ${1/5}^{th}$. Predictions for each data set was repeated $15$ times across various sample sizes. The Mean Square Error (MSE) and Classification Error was captured to regression and classification tasks respectively. To make comparisons between different approximation methods as fair as possible, the matrix $\left( \widehat{\bm{K_{XX}}} + \sigma_n^2 \Id_{n \times n} \right)^{-1}$ was directly inverted to eliminate any error an in-exact linear system solver might introduce. To do this efficiently, since both the Nystrom and RFF methods produce decompositions of their approximations in the form $\widehat{\bm{K_{XX}}} = \bm{U} \bm{W}^{\dagger} \bm{U}^{\intercal}$ we can employ the use of the matrix inversion lemma (see \Cref{lemma: mat_inv_lem}) to compute
\begin{align*}
    \left( \widehat{\bm{K_{XX}}} + \sigma_n^2 \Id_{n \times n} \right)^{-1} \
     & = \left( \bm{U} \bm{W} \bm{U}^{\intercal} + \sigma_n^2 \Id_{n \times n} \right)^{-1}                                                                                                                                        \\
     & = \sigma_n^{-2} \Id_{n \times n} + \sigma_n^{-2} \Id_{n \times n} \bm{U} \left( \bm{W}^{\dagger} + \bm{U}^{\intercal} \sigma_n^{-2} \Id_{n \times n} \bm{U} \right)^{-1} \bm{U}^{\intercal} \sigma_n^{-2} \Id_{n \times n}.
\end{align*}
Only a single value of $\sigma$ was used to create predictions for each dataset. The value of $\sigma$ chosen corresponds to the best value of $\sigma$ out of $0.1,1.0$ and $10.0$ (as well as $2.1$ for the Wine dataset) when an exact GP algorithm was used to provide predictions. The time and memory of performing the matrix inverse using the above was recorded. Again, errors, time and memory were average across the repetitions for each different sample.

\subsubsection{Krylov Subspace Methods Approximation Testing}\label{Section5.2.2}

Similar to the kernel matrix approximation setup, to see how well CG and MINRES compare in providing predictions each method was used to solve the linear system
\begin{equation*}
    \left( \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} \right) \bm{\alpha} = \bm{y}
\end{equation*}
within the GPR algorithm and
\begin{equation*}
    \left( \bm{K}_{\bm{X} \bm{X}} + \bm{W}^{-1} \right) \bm{\alpha} = \bm{K}_{\bm{x}_{\star} \bm{X}}^{\intercal}
\end{equation*}
within the GPC algorithm in terms of $\bm{\alpha}$. As before, predictions for each data set was repeated $15$ times across various samples. The Mean Square Error (MSE) and Classification Error was captured to regression and classification tasks respectively. To allow for a fair comparison between the different linear solvers, the kernel matrix was constructed in an exact manner to ensure no error was introduced through any other part of the prediction process. As with the kernel matrix approximation prediction set up, only a single value of $\sigma$ was used to create predictions for each dataset, that being the value of $\sigma$ that provided the best predictions results using exacts methods. Each method was used to train a Gaussian process with ${4/5}^{ths}$ of each data set and required to predict the remaining ${1/5}^{th}$ over all the different datasets and was repeated $15$ times for each dataset with a varying number of maximum iterations. The usual statistics such as prediction error, time and memory usuage were recorded for every experiment and averaged across repetitions for each dataset and method.