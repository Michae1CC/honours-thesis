\subsection{Conclusion}\label{Section5.4}

The aim of this thesis was to explore how various approximation techniques could be used within the Gaussian Process algorithm to speed up prediction time. Chapter 1 focuses on what kernel reproducing hilbert spaces are and how the kernel trick is employed as a clever means to provide non-linear estimations and separation in machine learning models without having to use non-linear solvers. We then went onto show how Gaussian processes where defined and how they could be used a machine learning instrument for both classifications tasks. However, when implemented naively, we found that Gaussian process scale very poorly with the number of samples as a result of the two predominate bottlenecks within the prediction process. The first bottle neck come from having to produce the kernel matrix and the seconds comes from solving linears involving said kernel matrix. The remainder of the thesis focuses on popular estimation techniques to accelrate the prediction process. Chapter 2 looked at the Nystrom method to speed up kernel matrix approximation. Specifically, a common sampling technique variant was studied where various frequently used sampling distributions to provide a sketching matrix such as ridge leverage scores and statistical leverage scores. We also devised our own novel sampling distributions that only depended on the data matrix and did not require any information on the kernel matrix. In Chapter 3, and alternate (and more recent) approach to kernel matrix approximation was reviewed, that being the Random Fourier Feature method. In this chapter various transformation matrices were considered for the construction of a Gaussian random matrix. Finally, in Chapter 4, various Krylov subspace methods were visited to seek alternative ways of solving linear systems within Gaussian process predictions.

Chapter 5, provided a summary of our extensive experimental analysis carried out to determine which methods delivered the best results. For the kernel estimation technique, each method was required to create a its own approximation for the kernel matrices of various datasets where the relative infinity and Frobenius error was captured to measure the quality of approximations. Moreover, selected kernel methods were used within the Gaussian Process algorithm to provided predictions for the same datasets. Similarly, Krylov subspace methods were used in place of the Cholesky decomposition in Algorithm TODO to provided predictions.

When looking at kernel matrix construct in isloation, $5$ different Nystrom column sampling distributions where compared with $3$ RFF transformation matrix construction methods. For Nystrom, it was found that some of the more sophisticated sampling distributions provided far better results for kernel matrices with non-uniform spectrum. Surprisingly, when looking at the various RFF methods, sophisticated techniques used to construct the transformation did not provide much of an advantage over basic methods. In terms of Gaussian process predictions, RFF methods delivered better errors with small time budgets over Nystrom methods.

When comparing CG and MINRES, we found that MINRES was consistently competitive if not better than CG, even when these linear system solvers are paired with RFF for regression tasks. This may seem surprising given the ubiquitous preference of CG over any other linear solver in many Gaussian process software libraries. We believe MINRES's superior performance is owed to its ability to more directly lower the Euclidean distance between the predictions and the true outputs.

Looking forward, many of the datasets used in this thesis were far too small to observe the asymptotic benefits of using approximation methods over most of the exact methods. It would be interesting to see how well these techniques scale on very large datasets, in particular, these methods would probably be of great benefit to agricultural teams as they collect more data over to the years to do crop analysis on. Another, direction we could take future research on this topic is, in many machine learning applications we may want to predict multiple outputs using the same set of inputs. As an example, remote sensing researchers attempt use the intensity of multiple bands of light bouncing off of farm land to give an indication of crop growth. Different versions of the Gaussian processes algorithm exist to predict multiple outputs out once although they also suffer from the same bottlenecks as single output Gaussian processes. It would be interesting to see how the approximation techniques studied in this thesis could help improve the prediction time of multi-output Gaussian Processes.