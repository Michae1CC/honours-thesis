\subsection{Conclusion}\label{Section5.4}

The aim of this thesis was to explore how various approximation techniques could be used within the Gaussian Process algorithm to speed up prediction time. The two predominate bottlenecks of the Gaussian Process algorithm are producing the kernel matrix for the training dataset as well as linear system solve involving said kernel matrix. Two varieties of estimation techniques were used to create the kernel matrix, these being the Random Fourier Feature and Nystrom methods. Techniques used to provide estimates for linear systems solves were two Krylov subspaces methods, namely CG and MINRES. Extensive experimental analysis was conducted to determine which methods delivered the best results. For the kernel estimation technique, each method was required to create a its own approximation for the kernel matrices of various datasets where the relative infinity and Frobenius error was captured to measure the quality of approximations. Moreover, selected kernel methods were used within the Gaussian Process algorithm to provided predictions for the same datasets. Similarly, Krylov subspace methods were used in place of the Cholesky decomposition in Algorithm TODO to provided predictions.

When looking at kernel matrix construct in isloation, for Nystrom methods, it was found that some of the more sophisticated sampling distributions provided far better results for kernel matrices with non-uniform spectrum. Surprisingly, when looking at the various RFF methods, sophisticated techniques used to construct the transformation did not provide much of an advantage over basic methods. In terms of Gaussian process predictions, RFF methods delivered better errors with small time budgets over Nystrom methods.

When comparing CG and MINRES, we found that MINRES was consistently competitive if not better than CG, even when these linear system solvers are paired with RFF for regression tasks. This may seem surprising given the ubiquitous preference of CG over any other linear solver in many Gaussian process software libraries. We believe MINRES's superior performance is owed to its ability to more directly lower the Euclidean distance between the predictions and the true outputs.

Looking forward, many of the datasets used in this thesis were far too small to observe the asymptotic benefits of using approximation methods over most of the exact methods. It would be interesting to see how well these techniques scale on very large datasets, in particular, the methods would be of great benefit to Andries team as they collect more data over to the years to do crop analysis on. Another, direction we could take future research on this topic is, in many machine learning applications we may want to predict multiple outputs using the same set of inputs. As an example, when Andries' team attempts to predict the intensity of multiple bands of light bouncing off of farm land to give an indication of crop growth. Different versions of the Gaussian Processes algorithm exist to predict multiple outputs out once although they also suffer from the same bottlenecks as single output Gaussian processes. It would be interesting to see how the approximation techniques studied in this thesis could help improve the prediction time of multioutput Gaussian Processes.