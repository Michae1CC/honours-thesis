\subsection{Conclusion}\label{Section5.4}

The aim of this thesis was to explore how various approximation techniques could be used within the Gaussian Process algorithm to speed up prediction time. Chapter \ref{Chapter1} focuses on what kernel reproducing Hilbert spaces and how the kernel trick can be employed as a clever means to afford non-linear estimations while avoiding non-linear solvers. Next, Gaussian processes were defined and shown how they could be used as an instrument for machine learning to facilitate classification and regression tasks. However, when implemented naively, Gaussian process scale very poorly with the number of samples on account of two predominate bottlenecks within the prediction process. The first bottleneck arises from having to produce the kernel matrix and the second from solving linears involving said kernel matrix.

The remainder of the thesis focuses on popular estimation techniques to accelerate the prediction process. Chapter \ref{Chapter2} looked at the Nystrom method to speed up kernel matrix approximation. Specifically, a common sampling technique variant was studied in which various sampling distributions where used to construct a sketching matrix. We also devised novel sampling distributions that depended only on the data matrix and did not require any information about the kernel matrix. In Chapter \ref{Chapter3}, another (and more recent) approach to kernel matrix approximation was reviewed, that being the Random Fourier Feature (RFF) method. In this chapter various transformation matrices were considered for the construction of a Gaussian random matrix. Finally, in Chapter \ref{Chapter4}, a number of Krylov subspace methods were surveyed to seek alternative ways of solving linear systems within Gaussian process predictions. The particular Krylov subspace methods studied were the conjugate gradient (CG) and minimum residual (MINRES) algorithms.

Chapter \ref{Chapter5}, summarized our extensive experimental analysis to determine which methods delivered the best results. For the kernel estimation techniques, each method was required to create a its own approximation for the kernel matrices of various datasets where the relative infinity and Frobenius error was captured to measure the quality of approximations. Moreover, selected kernel methods were used within the Gaussian Process algorithm to provided predictions for the same datasets. Similarly, Krylov subspace methods were used in place of the Cholesky decomposition in Algorithm \ref{alg: Unoptimized_GPR} to afford a comparative prediction.

When looking at a kernel matrix construct in isloation, $5$ different Nystrom column sampling distributions where compared with $3$ RFF transformation matrix construction methods. For Nystrom methods, it was found that some of the more sophisticated sampling distributions provided superior results for kernel matrices with non-uniform spectrums. Surprisingly, for RFF methods, the use of sophisticated techniques used to construct the transformation did not provide much of an advantage over basic methods. In terms of Gaussian process predictions, RFF methods delivered better errors with small time budgets over Nystrom methods.

When comparing CG and MINRES, we found that MINRES was consistently more competitive, if not better, than CG, even when paired with RFF for regression tasks. This may seem surprising given the ubiquitous preference of CG over any other linear solver in many Gaussian process software libraries. We believe MINRES's superior performance is due to its ability to more directly lower the Euclidean distance between the predictions and the true outputs.

Looking forward, some of the findings discovered in this thesis have, to our knowledge, not been reported elsewhere. As a result, we intend to publish these findings in a reputable journal given their obvious appeal to the wider scientific computing community. In terms of research, many of the datasets used in this thesis were far too small to observe the asymptotic benefits of using approximation methods over most of the exact methods. It would be interesting to determine how well these techniques scale on very large datasets. In particular, our discoveries are likely to benefit the agricultural sector as more data is collected over the forthcoming years to perform crop analysis on. Another, direction future research could be taken is the application of the approximation techniques applied to multi-output or multi-task Gaussian process models. In many machine learning scenarios we may want to predict multiple outputs using the same set of inputs. As an example, remote sensing researchers attempt to predict the intensity of multiple bands of light reflecting off farm land to give an indication of crop growth. Different versions of the Gaussian processes algorithm exist to predict multiple outputs simultaneously \cite{NIPS2007_66368270}; however, they also suffer from the same bottlenecks as single output Gaussian processes. It would be intriging to learn whether the approximation techniques studied in this thesis could potentially improve the prediction time of multi-task Gaussian Processes.