\subsection{Gaussian Processes Prediction Reviewed}\label{Section5.1}

In Chapter \ref{Chapter1}, a naive implementation for Gaussian Process prediction was presented in \cref{alg: Unoptimized_GPR}. While this does provide a simple and convenient way to produce predictions for a regression task, it is not quite all smooth sailing from here. Unfortunately, there are a number of problems with this algorithm in terms of scalability. For convenience, this algorithm has been restated below but this time with its various bottlenecks highlighted in red.

\setcounter{savecounter}{\value{algocf}}
\setcounter{algocf}{\value{GPRcount}}

{\centering
    \begin{minipage}{.85\linewidth}

        \begin{algorithm*}[H]
            \caption{Unoptimized GPR}
            \SetAlgoLined
            \DontPrintSemicolon
            \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

            \Input{Observations $\bm{X}, \bm{y}$ and a test input $\bm{x}_{\star}$.}
            \Output{A prediction $\overline{f_{\star}} $ with its corresponding variance $ \VV \left[ f_{\star} \right]$.}
            \BlankLine
            \textcolor{red}{$\bm{L} = \operatorname{cholesky} \left( \bm{K_{XX}} + \sigma_n^2 \Id_{n \times n} \right)$}\;
            \textcolor{red}{$\bm{\alpha} = \operatorname{lin-solve} \left( \bm{L}^{\intercal} , \operatorname{lin-solve} \left( \bm{L}, \bm{y} \right) \right)$}\;
            $\overline{f_{\star}} = \bm{K_{x_{\star} X}} \bm{\alpha}$\;
            \textcolor{red}{$\bm{v} = \operatorname{lin-solve} \left( \bm{L}, \bm{K_{x_{\star} X}} \right)$}\;
            $\VV \left[ f_{\star} \right] = \bm{K_{x_{\star} x_{\star}}} - \bm{v}^{\intercal} \bm{v}$\;
            \Return{$\overline{f_{\star}} , \VV \left[ f_{\star} \right]$}
            \BlankLine
        \end{algorithm*}
    \end{minipage}
    \par
}

\setcounter{algocf}{\value{savecounter}}

To start computing the kernel matrix, $\bm{K_{XX}}$, on line $1$ carries an $\mathcal{O} \left( n^2 \right)$ runtime since $\mathcal{O} \left( n^2 \right)$ pairwise kernel evaluations must be made. Moreover, solving linear systems using a cholesky decomposition (seen on lines $1,2$ and $4$) will incur a runtime of $\mathcal{O} \left( n^3 \right)$. While this quadratic scaling may not be much of an issue for smaller datasets, this will scale very poorly where most modern desktops would not be able to feasibly perform training with datasets having more than $10^5$ samples \cite{DBLP:journals/corr/abs-2112-15246}*{page 2}. The Gaussian Process classifier from \cref{alg: Unoptimized_GPC} suffers for the exact same reasons as GPR. To mitigate the computational burden of these bottle necks, we can replace these procedures with their inexact counterparts discussed in Chapters \ref{Chapter2},\ref{Chapter3} and \ref{Chapter4}.

To start, computing the kernel matrix can be done using either the Nystrom method or the RFF technique, both of which provide better asymptotic runtimes. Similarly, the Cholesky decomposition and linear solves can be replaced with either CG or MINRES to, again, improve runtime performance. This especially makes sense approximating the kernel matrix as the Cholesky decomposition will provide an almost-exact solution to solving these linear system, as this is rather counterintuitive if approximations are used earlier on within the algorithm.