\subsection{Theory and Computation}\label{Section3.1}
Contrary to the kernel trick the Random Fouier Features (RFF) technique approximates $\langle \Phi \left( \cdot \right) , \Phi \left( \cdot \right) \rangle_{\RR^N}$ through an explicit feature mapping $\varphi$. The RFF techniques hinges on Bochners theorem stated without proof in theorem \ref{theorem: bochner} which characterises positive definite functions.

\begin{thm}[Bochner's] \label{theorem: bochner}
    A continuous and shift-invariant function $k \left( \bm{x} , \bm{y} \right) = k \left( \bm{x} - \bm{y} \right) = k \left( \Delta \right)$ is positive definite (see definition \ref{defe: PD}) if and only if it can be represented as
    \[
        k \left( \bm{x} - \bm{y} \right) = \int_{\CC^d} \exp \left( i \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right) \mu_k \left( d \bm{\omega} \right)
    \]
    where $\mu_k$ is a positive finite measure on the frequencies of $\bm{\omega}$ \cite{HahnHans1933SBVÃ¼,LiuFanghui2021RFfK}.
\end{thm}

The spectral distribution $\mu_k$ can be represented as finite measure induced by the Fourier transformation. Choosing a kernel for which $k (\bm{0}) = 1$ normalizes $\mu_k$ to a probability distribution $p (\cdot)$. For instance, the spectral distribution of the Gaussian RBF kernel is
\begin{equation} \label{eq: rbf-spec-pdf}
    p \left( \bm{w}\right) = \frac{1}{\sqrt{\left( 2 \pi\right)^{D} \left| \frac{\sigma^2}{2} \Id_{D \times D} \right|}} \exp \left( -\frac{1}{2} \bm{w}^\intercal \left( \frac{\sigma^2}{2} \Id_{D \times D} \right)^{-1} \bm{w} \right)
\end{equation}
\cite{NIPS2007_013a006f}*{page 3}. One caveat in Bochner's theorem is that it requires our kernel to be shift-invariant (sometimes also referred to as stationary) stated in definition \ref{defe: shift-invar}.

\begin{defe}[Shift-Invariant] \label{defe: shift-invar}
    A kernel $k : \RR^N \times \RR^N \to \CC$ is called shift-invariant if $k \left( \bm{x}, \bm{y} \right) = g \left( \bm{x} - \bm{y} \right)$ for some positive definite function $g : \RR^N \to \CC$ \cite{JMLR:v17:14-538}*{page 3}.
\end{defe}

Clearly the Gaussian RBF kernel is shift-invariant since it only relies on the bounding radius of $\bm{x}$ and $\bm{y}$. Thus from Bochners theorem a positive definite shift-invariant kernel with $k(0) = 1$ can be computed as
\begin{equation} \label{eq: rff_boch_int}
    k \left( \bm{x} - \bm{y} \right) = \int_{\CC^d} \exp \left( i \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right) p (\bm{\omega}) \; d \bm{\omega} .
\end{equation}
The main of idea of RFF is to approximate the integral in \ref{eq: rff_boch_int} using the following Monte-Carlo estimate
\begin{align*}
    k \left( \bm{x} - \bm{y} \right)
     & = \int_{\CC^d} \exp \left( i \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right) p (\bm{\omega}) \; d \bm{\omega}                                                                                                            \\
     & = \EE_{\bm{\omega} \sim p (\cdot)} \left( \exp \left( i \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right) \right)                                                                                                          \\
     & \approx \frac{1}{D} \sum_{j=1}^{D} \exp \left( i \langle \bm{\omega}_{j} , \bm{x} - \bm{y} \rangle \right)                                                                                                                     \\
     & = \sum_{j=1}^{D} \left( \frac{1}{\sqrt{D}} \exp \left( i \langle \bm{\omega}_{j} , \bm{x} \rangle \right) \right) \overline{\left( \frac{1}{\sqrt{D}} \exp \left( i \langle \bm{\omega}_{j} , \bm{y} \rangle \right) \right) } \\
     & = \langle \varphi (\bm{x}) , \varphi (\bm{y}) \rangle_{\CC^D}
\end{align*}
where $\bm{\omega}_i \stackrel{\text{iid}}{\sim} p(\cdot)$ using the feature map
\begin{equation}
    \varphi (\bm{x}) = \frac{1}{\sqrt{D}} \left[ z \left( \bm{\omega}_1, \bm{x} \right), z \left( \bm{\omega}_2, \bm{x} \right), \ldots , z \left( \bm{\omega}_D, \bm{x} \right) \right]^{\intercal}
\end{equation}
where for convenience of notation $z \left( \bm{\omega}, \bm{x} \right) = \exp \left( i \langle \bm{\omega} , \bm{x} \rangle \right)$. This allows the Gram matrix to be estimated as $\bm{K} \approx \bm{\widetilde{K}} = \bm{Z} \bm{Z}^{\intercal}$ where $\bm{Z} = \left[ \varphi (\bm{x}_1), \varphi (\bm{x}_2), \ldots \varphi (\bm{x}_D) \right] \in \CC^{n \times D}$ \cite{NIPS2007_013a006f,LiuFanghui2021RFfK,JMLR:v17:14-538}. To simplify computation, in most settings both $p(\cdot)$ and $k(\Delta)$ are real valued functions meaning $\exp \left( i \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right)$ can replaced with its real component $\cos \left( \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right)$. The vast majority of literature uses the embeddings Rahimi and Recht provide for $\cos \left( \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right)$ where $z \left( \bm{\omega}, \bm{x} \right)$ satisfies equation \ref{eq: rff_boch_int}. The first embedding takes the form
\begin{equation} \label{eq: rff_emb}
    z \left( \bm{\omega}, \bm{x} \right) = \left[ \cos \left( \langle \bm{\omega} , \bm{x} \rangle \right) , \sin \left( \langle \bm{\omega} , \bm{x} \rangle \right) \right]^{\intercal}
\end{equation}
which satisfies \ref{eq: rff_boch_int} since
\begin{align*}
     & z \left( \bm{\omega}, \bm{x} \right)^{\intercal} z \left( \bm{\omega}, \bm{y} \right)                                                                                                                                                      \\
     & =
    \left[ \cos \left( \langle \bm{\omega} , \bm{y} \rangle \right), \sin \left( \langle \bm{\omega} , \bm{y} \rangle \right)  \right]
    \begin{bmatrix}
        \cos \left( \langle \bm{\omega} , \bm{x} \rangle \right) \\ \sin \left( \langle \bm{\omega} , \bm{x} \rangle \right)
    \end{bmatrix}                                                                                                                                                                                                                     \\
     & = \cos \left( \langle \bm{\omega} , \bm{x} \rangle \right) \cos \left( \langle \bm{\omega} , \bm{y} \rangle \right) + \sin \left( \langle \bm{\omega} , \bm{x} \rangle \right) \sin \left( \langle \bm{\omega} , \bm{y} \rangle \right)    \\
     & = \frac{1}{2} \left( \cos \left( \langle \bm{\omega} , \bm{x} \rangle + \langle \bm{\omega} , \bm{y} \rangle \right) + \cos \left( \langle \bm{\omega} , \bm{x} \rangle - \langle \bm{\omega} , \bm{y} \rangle \right) \right) +           \\
     & \qquad \qquad \frac{1}{2} \left( \cos \left( \langle \bm{\omega} , \bm{x} \rangle - \langle \bm{\omega} , \bm{y} \rangle \right) - \cos \left( \langle \bm{\omega} , \bm{x} \rangle + \langle \bm{\omega} , \bm{y} \rangle \right) \right) \\
     & = \cos \left( \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right).
\end{align*}
The other embedding Rahimi and Recht give is
\begin{equation} \label{eq: rff_emb_alt}
    z \left( \bm{\omega}, \bm{x} \right) = \sqrt{2} \cos \left(  \langle \bm{\omega} , \bm{x} \rangle + b \right)
\end{equation}
where $b \sim U \left[ 0,2 \pi \right]$. Using a similar argument we can show that this embedding also satisfies \ref{eq: rff_boch_int}. However, Sutherland's and Schneider's paper \cite{sutherland2015error} they argue that the Gaussian RBF kernel is better suited for the embedding given in \ref{eq: rff_emb}. To summarise their argument we denote
\begin{equation} \label{eq: rff_feat_map_1}
    \varphi_1 (\bm{x}) = \sqrt{\frac{2}{D}}
    \begin{bmatrix}
        \cos \left( \langle \bm{\omega}_{1} , \bm{x} \rangle \right)   \\
        \cos \left( \langle \bm{\omega}_{2} , \bm{x} \rangle \right)   \\
        \vdots                                                         \\
        \cos \left( \langle \bm{\omega}_{D/2} , \bm{x} \rangle \right) \\
        \sin \left( \langle \bm{\omega}_{1} , \bm{x} \rangle \right)   \\
        \vdots                                                         \\
        \sin \left( \langle \bm{\omega}_{D/2} , \bm{x} \rangle \right)
    \end{bmatrix}
\end{equation}
to be the feature map corresponding to embedding in equation \ref{eq: rff_emb} and
\begin{equation} \label{eq: rff_feat_map_2}
    \varphi_2 (\bm{x}) = \sqrt{\frac{2}{D}}
    \begin{bmatrix}
        \cos \left( \langle \bm{\omega}_{1} , \bm{x} \rangle + b_1 \right) \\
        \vdots                                                             \\
        \cos \left( \langle \bm{\omega}_{D} , \bm{x} \rangle + b_D \right)
    \end{bmatrix}
\end{equation}
to be the feature map corresponding to equation \ref{eq: rff_emb_alt}. They then show that
\begin{align*}
    \VV \left[ \varphi_1 (\Delta) \right] & = \frac{1}{D} \left( 1 + k (2 \Delta) - 2 {k( \Delta)}^2 \right)           \\
    \VV \left[ \varphi_2 (\Delta) \right] & = \frac{1}{D} \left( 1 + \frac{1}{2} k (2 \Delta) - {k( \Delta)}^2 \right)
\end{align*}
meaning the variance of $\varphi_1$ is smaller whenever
\[
    \VV \left[ \cos \left( \langle \bm{\omega} , \Delta \rangle \right) \right] = \frac{1}{2} + \frac{1}{2} k (2 \Delta) - {k( \Delta)}^2 \leq \frac{1}{2} .
\]
When using the Gaussian kernel,
\[
    \VV \left[ \cos \left( \langle \bm{\omega} , \Delta \rangle \right) \right] = \frac{1}{2} \left( 1 - \exp \left( - \frac{2\norm{\Delta}^2_2}{\sigma^2} \right) \right)^2 \leq \frac{1}{2}
\]
so that $\varphi_1 (\Delta) \leq \varphi_2 (\Delta)$ for any $\Delta \in \RR^d$. This result is indeed consistent with our preliminary experiments. With this in mind, an embedding of $\varphi_1$ is always used for our experimentation.

Another important result Rahimi and Recht show provides a bound on the sup-norm of the difference between a Gram matrix and its RFF approximation stated in proposition \ref{prop: rff_sup-norm_bound_1}.

\begin{prop} \label{prop: rff_sup-norm_bound_1}
    Let $k(\bm{x} , \bm{y}) = k(\bm{x} - \bm{y}) = k( \Delta)$ be a continuous shift-invariant, positive definite function defined on compact subset $\calM \subset \RR^d$ having radius $\ell$ where $k(0) = 1$ such that $\nabla^2 k(0)$ exists. Then for the feature mapping defined in equation \ref{eq: rff_feat_map_1} let $\sigma_p^2 = \EE_{\bm{\omega} \sim p(\cdot)} \norm{\bm{\omega}}^2_2 = \operatorname{tr} \nabla^2 k(0)$ then for any $\varepsilon \in \RR_{>0}, \; \varepsilon \leq \sigma_p \ell$ we have
    \[
        \PP \left[ \sup_{\bm{x},\bm{y} \in \calM} \left| \langle \varphi (\bm{x}) , \varphi (\bm{y}) \rangle_{\RR^D} - k (\bm{x},\bm{y}) \right| \geq \varepsilon \right] \leq \alpha \left( \frac{\sigma_p \ell}{\varepsilon} \right)^2 \exp \left( - \frac{D \varepsilon^2}{8(d+2)} \right)
    \]
    where $\alpha \in \RR_{>0}, \; \alpha < \infty$ does not depend on anything \cite{NIPS2007_013a006f}*{page 3}.
\end{prop}

Rahimi and Recht prove proposition \ref{prop: rff_sup-norm_bound_1} for $\alpha = 2^8$ although Sutherland and Schneider improve this to $\alpha = 66$ \cite{sutherland2015error}*{page 3}. Observe that this bound is somewhat determined by the ratio $D/d$ which is why $D$ is often chosen as a multiple of $d$ in experimentation.

These results justify the RFF procedure seen in algorithm \ref{alg: RFF-algorithm}, which is used to approximate a Gram matrix for the data set $\bm{X}$ using the feature map from \ref{eq: rff_feat_map_1}.

{\centering
\begin{minipage}{.85\linewidth}
    \begin{algorithm}[H]
        \caption{RFF Algorithm}
        \label{alg: RFF-algorithm}
        \SetAlgoLined
        \DontPrintSemicolon
        \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

        \Input{$\bm{X} \in \RR^{n \times d}$, the dimension of the feature space $D$.}
        \Output{$\bm{\widetilde{K}} \approx \bm{K}$ where $\bm{K}$ is the Gram matrix corresponding to $\bm{X}$.}
        \BlankLine
        Construct $\bm{W} \triangleq \left[ \bm{\omega}_1 ,\ldots , \bm{\omega}_D \right]^{\intercal} \in \RR^{D \times d} \;$ where $\; \bm{\omega}_i \stackrel{\text{iid}}{\sim} p(\cdot)$\;
        $\bm{Z} = \frac{1}{\sqrt{D}} \left[ \cos \left( \bm{W} \bm{X}^{\intercal} \right), \sin \left( \bm{W} \bm{X}^{\intercal} \right) \right]^{\intercal}$\;
        $\bm{\widetilde{K}} = \bm{Z}\bm{Z}^{\intercal}$\;
        \Return{$\bm{\widetilde{K}}$}
        \BlankLine
    \end{algorithm}
\end{minipage}
\par
}

Algorithm \ref{alg: RFF-algorithm} of course assumes an appropriate construction of $\bm{W}$, commonly called the transformation matrix, and thus has access to a routine which allows one to sample from $p(\cdot)$. When using the RBF Gaussian kernel, the spectral distribution given in equation \ref{eq: rbf-spec-pdf} corresponds to a multivariate Gaussian distribution with mean $\bm{0}$ and covariance matrix $\left( \frac{\sigma}{\sqrt{2}} \right)^{-2} \Id_{D \times D}$. This means $\bm{W}$ can simply be constructed as $\bm{W} = \left( \frac{\sigma}{\sqrt{2}} \right)^{-1} \left[ \bm{\omega}_1 ,\ldots , \bm{\omega}_D \right]^{\intercal}$ where $\; \bm{\omega}_i \stackrel{\text{iid}}{\sim} \calN \left( \bm{0} , \Id_{D \times D} \right)$. Tranformations matrices constructed in this manner are given the notation $\bm{W}_{\text{RFF}}$. It can be shown that if $\bm{W}_{\text{RFF}}$ is used as the transformation matrix in algorithm \ref{alg: RFF-algorithm} then it produces an unbiased estimate, $\bm{\widetilde{K}}_{\text{RFF}}$, for the Gram matrix. This stated more precisely in lemma \ref{lem: rff-unbiased}.

\begin{lem} \label{lem: rff-unbiased}
    $\bm{\widetilde{K}}_{\text{RFF}}$ is an unbiased estimate of $\bm{K}$, that is
    \[
        \EE \left[ \left( \bm{\widetilde{K}}_{\text{RFF}} \right)_{ij} \right] = \exp \left( \frac{- \norm{\bm{x}_i - {\bm{x}_j}}_{2}^{2}}{\sigma^2} \right)
    \] \cite{YuFelixX2016ORF}*{page 3}.
\end{lem}

Unfortunately, constructing the transformation matrix using $\bm{W}_{\text{RFF}}$ does not scale to well as the dimension of the feature space increases. Thus the focus of the upcoming sections will be to highlight a few of the more popular alternative methods used in literature for the constructing the transformation matrix.