\section{Random Fourier Features}\label{Chapter3}
We saw in section \ref{Chapter1} that GPs relied heavily on the Gram matrix (see definition \ref{defe: Gram_Matrix}) to create predictions based on training data $\calD = \left( \bm{X} , \bm{y} \right)$ where $\bm{X} = \left[ \bm{x}_1 , \bm{x}_2 , \ldots , \bm{x}_n \right]^{\intercal} \in \RR^{n \times d}$ and $\bm{y} = \left[ y_1 , y_2 , \ldots , y_n \right]^{\intercal} \in \RR^{n}$. Unfortunately, the size of the Gram matrix scales quadratically with the number of samples making it difficult to train using data sets with more than $10^5$ samples. Instead the kernel function itself can be factorized allowing one to convert training and kernel evaluation into the corresponding operations of a linear machine by mapping data into a relatively low-dimensional randomized feature space. This idea was first introduced by Rahimi and Recht \cite{NIPS2007_013a006f} where they propose that instead of using a kernel function to implicitly lift data into a higher dimensional feature space, an explicit feature map $\varphi : \RR^d \to \RR^D$ can be used to approximate $k$ as $k \left( \bm{x} , \bm{y} \right) = \langle \Phi (\bm{x}) , \Phi (\bm{y}) \rangle_{\RR^N} \approx \langle \varphi (\bm{x}) , \varphi (\bm{y}) \rangle_{\RR^D}$ where $D$ is chosen so that $n \gg  D$. Thus once $\varphi (\bm{x}_i)$ has been computed for each $\bm{x}_i$, each entry of the Gram matrix can be swiftly approximated as
\[
    \bm{K}_{ij} = \bm{K}_{ji} \approx \langle \varphi (\bm{x}_i) , \varphi (\bm{y}_j) \rangle_{\RR^D}.
\]
Already there have been numerous applications of this technique in GPs that have seen improved time performance with little loss in prediction accuracy \cite{PotapczynskiAndres2021BSGP}.

\input{rff/theory_and_comp.tex}

\input{rff/orf.tex}

\input{rff/rom_and_sorf.tex}