\subsection{Random Ortho-Matrices and Structured Orthogonal Random Matrices}\label{Section3.3}

The second method we shall consider for producing a transformation matrix also originates from Yu's {\it et al.} paper, which Choromanski {\it et al.} \cite{ChoromanskiKrzysztof2017TUEo} generalized as Random Ortho-Matrices (ROM). This second class of methods is underpinned by transformation matrices with the same variance reductions as ORF with the added benefit of time and memory savings. The transformation matrices generated using ROM take the form
\begin{equation} \label{eq: ROM-general}
    \bm{W}_{\text{ROM}} = \sqrt{d} \prod_{i=1}^{k} \bm{S} \bm{D}_{i}
\end{equation}
where $\bm{S} \in \RR^{D \times D}$ has orthogonal rows and $\bm{D} = \operatorname{diag} \left( \delta_1 , \ldots , \delta_D \right) \in \RR^{D \times D}$ where $\delta_i \stackrel{\text{iid}}{\sim} U \left( \left\{ -1, 1 \right\} \right)$. This matrix can be forced into a $\RR^{D \times d}$ sized matrix by simply extracting the first $d$ columns of $\bm{D}_1$. The matrix to take the role of $\bm{S}$ in virtually every application of ROM is the Hadamard matrix, defined in \ref{defe: Hadamard-Matrix}, which facilitates a fast $m \log (n)$ matrix multiplication with a size $m \times n$ and is known as Fast Walsh-Hadamard transform (FWHT) \cite{Fino1976UMTo}.

\begin{defe}[Hadamard Matrix] \label{defe: Hadamard-Matrix}
    The Hadamard matrix $\bm{H}_i \in \RR^{\left( 2^{i-1} \times 2^{i-1} \right)}$ is defined recursively as
    \[
        \bm{H}_i =
        \left\{
        \begin{array}{cc}
            \left[ 1 \right] & , i=1 \\
            \frac{1}{\sqrt{2}}
            \begin{bmatrix}
                \bm{H}_{i-1} & \bm{H}_{i-1}   \\
                \bm{H}_{i-1} & - \bm{H}_{i-1} \\
            \end{bmatrix}
                             & , i>1
        \end{array}
        \right. .
    \]
\end{defe}

Note that while Hadamard matrices are only defined for dimensions of exact powers of 2, although other sizes can be constructed by removing portions of the matrix given in \Cref{defe: Hadamard-Matrix} or by padding with $0$. This provides a concrete means for which one can generate a transformation matrix
\begin{equation} \label{eq: ROM-Hadamard}
    \sqrt{d} \prod_{i=1}^{k} \bm{H} \bm{D}_{i}
\end{equation}
where $\bm{H}$ is an appropriately sized Hadamard matrix. It is easy to check that the matrix generated by equation  \Cref{eq: ROM-Hadamard} shares the same expected rows norm lengths as $\bm{W}_{\text{ORF}}$ and thus enjoys the same variance reduction benefits. Moreover, since matrix multiplication with $\bm{H}$ can be performed in $\calO \left( D \log (d) \right)$ time (using FWHT) and multiplication with $\bm{H}$ can be performed in $\calO (D)$ time, the ROM method has the added benefit of improved run time complexity $\calO \left( D \log (d) \right)$ using only $\calO (D)$ extra memory. Table \ref{table: RFF-compare} gives a comparison of the time and space complexities for the methods mentioned so far.

\begin{table}[h!]
    \centering
    \caption{A comparison of various methods for computing a suitable transformation matrix with the Random Fourier Features paradigm. Typically the dimension of the feature space, $D$, is chosen as some multiple of the dimension of data, $d$.}
    \label{table: RFF-compare}
    \begin{tabular}{*3c}                                                                                                                    \\\bottomrule
        \hline
        \emph{Method}                                                  & \emph{Time}                       & \emph{Extra Space } \\\midrule
        RFF \cite{NIPS2007_013a006f}                                   & $\calO (Dd)$                      & $\calO (Dd)$        \\
        ORF \cite{YuFelixX2016ORF}                                     & $\calO (Dd)$                      & $\calO (Dd)$        \\
        ROM (SORF) \cite{ChoromanskiKrzysztof2017TUEo,YuFelixX2016ORF} & $\calO \left( D \log (d) \right)$ & $\calO (D)$         \\\bottomrule
        \hline
    \end{tabular}
\end{table}

Despite the wide use of the ROM method in various machine learning tasks \cite{ChoromanskiKrzysztof2017TUEo,AndoniAlexandr2015PaOL,ChoromanskiKrzysztof2020RAwP,LiuFanghui2021RFfK} a number of high-interest theoretical properties remain unsolved, leaving many aspects of this method shrouded in mystery. Instead, much of what we understand about ROM's estimate capabilities comes from empirical analysis. Nonetheless, we shall still cover a smaller number of important results that have been established.

Choromanski {\it et al.} \cite{ChoromanskiKrzysztof2017TUEo} show that there are diminishing returns (estimate wise) for choosing larger values of $k$ in equation  \Cref{eq: ROM-Hadamard}. They also show that choosing odd values of $k$ in  \Cref{eq: ROM-Hadamard} provides better estimates then its even-parity $k-1$ and $k+1$ counterparts. For this reason a $k$ value of $3$ is usually chosen which gives rise to the transformation matrix estimate given in equation  \Cref{eq: SORF-tm}. The method for constructing transformation matrices in this manner is referred to as Structured Orthogonal Random Features (SORF).
\begin{equation} \label{eq: SORF-tm}
    \bm{W}_{\text{SORF}} = \sqrt{d} \bm{H} \bm{D}_{3} \bm{H} \bm{D}_{2} \bm{H} \bm{D}_{1}
\end{equation}
This is the same transformation matrix estimate that Yu {\it et al} \cite{YuFelixX2016ORF}. provides. Unfortunately using the SORF method in \Cref{alg: RFF-algorithm} does not produce an unbiased estimate of the Gram matrix; however, it does satisfy an asymptotic unbiased property
\begin{equation*}
    \left| \EE \left[ \left( \bm{\widetilde{K}}_{\text{SORF}} \right)_{ij} \right] - \EE \left[ \left( \bm{\widetilde{K}}_{\text{RFF}} \right)_{ij} \right] \right| \leq \frac{6 \tau}{\sqrt{d}}
\end{equation*}
where $\tau$ is again $\norm{\bm{x}_i - \bm{x}_j}_2 / \frac{\sigma}{\sqrt{2}}$ \cite{LiuFanghui2021RFfK}*{page 8}.

Bojarski {\it et al.} \cite{BojarskiMariusz2016Saar}*{page 4} give an intuitive explanation for the roles of each of the different blocks $\bm{H} \bm{D}_1$, $\bm{H} \bm{D}_2$ and $\bm{H} \bm{D}_3$. The first block can be shown to satisfy
\[
    \PP \left[ \norm{\bm{H}\bm{D}_1 \bm{x}}_{\infty} > \frac{\log D}{\sqrt{D}} \right] \leq 2 d \exp \left( {-\frac{\log^2 D}{8}} \right), \quad \bm{x} \in \RR^D
\]
\cite{LiuFanghui2021RFfK}*{page 8} so that it can be thought as a "balancer" leaving no single dimension bearing too much of the $l^2$ norm. For the second block, the cost of using a structured matrix is the loss of independence. The purpose of the second block is to mitigate this effect by making similar input vectors near-orthogonal. Finally the third block controls the capacity of the entire structure by providing a vector of parameters. Near-independence is now implied by the near-orthogonality (achieved by $\bm{H} \bm{D}_2$) and the fact that the projections of the Gaussian vector or Radamacher vector onto "almost orthogonal directions" are "close to independent". These roles are portrayed visually in \Cref{fig: SORF-vis}.
\begin{figure}[h]
    \centering
    \subfloat[]{
        \begin{adjustbox}{width=0.29\textwidth}
            \begin{tikzpicture}[tdplot_main_coords, scale = 2.25]

                % Create a point (P)
                \coordinate (P) at ({1/sqrt(3)},{1/sqrt(3)},{1/sqrt(3)});

                % Draw shaded circle
                \shade[ball color = SkyBlue,
                    opacity = 0.5
                ] (0,0,0) circle (1cm);

                % draw arcs 
                \tdplotsetrotatedcoords{0}{0}{0};
                \draw[dashed,
                    tdplot_rotated_coords,
                    darkgray
                ] (0,0,0) circle (1);

                % Axes in 3 d coordinate system
                \draw[-stealth] (-1.80,0,0) -- (1.80,0,0)
                node[below left] {$x$};

                \draw[-stealth] (0,-1.30,0) -- (0,1.30,0)
                node[below right] {$y$};

                \draw[-stealth] (0,0,-1.30) -- (0,0,1.30)
                node[above] {$z$};

                % Vector v
                \draw[-stealth] (0,0,0) -- (-0.19588084,  0.97940421,  0.04897021)
                node[above right] {$\bm{v}$};

                % Vector H D_1 v
                \draw[-stealth] (0,0,0) -- (-0.37139068,  0.74278135,  0.55708601)
                node[above right] {$\bm{H} \bm{D}_1 \bm{v}$};

            \end{tikzpicture}
        \end{adjustbox}
    } \qquad
    \subfloat[]{
        \begin{adjustbox}{width=0.29\textwidth}
            \begin{tikzpicture}[tdplot_main_coords, scale = 2.25]

                % Create a point (P)
                \coordinate (P) at ({1/sqrt(3)},{1/sqrt(3)},{1/sqrt(3)});

                % Axes in 3 d coordinate system
                \draw[-stealth] (-1.80,0,0) -- (1.80,0,0)
                node[below left] {$x$};

                \draw[-stealth] (0,-1.30,0) -- (0,1.30,0)
                node[below right] {$y$};

                \draw[-stealth] (0,0,-1.30) -- (0,0,1.30)
                node[above] {$z$};

                % Vector v
                \draw[-stealth] (0,0,0) -- (-0.37139068,  0.74278135,  0.55708601)
                node[above right] {$\bm{v}$};

                % Vector w
                \draw[-stealth] (0,0,0) -- (-0.55708601,  0.37139068,  0.74278135)
                node[above left] {$\bm{w}$};

                % Vector u
                \draw[-stealth] (0,0,0) -- (-0.46423834,  0.55708601,  0.64993368)
                node[above right] {$\bm{u}$};

                % Vector H D_2 v
                \draw[-stealth] (0,0,0) -- (0.83078669, -0.47668089, -0.28734788)
                node[above left] {$\bm{H} \bm{D}_2 \bm{v}$};

                % Vector H D_2 w
                \draw[-stealth] (0,0,0) -- (-0.73,-0.57,-0.38)
                node[above left] {$\bm{H} \bm{D}_2 \bm{w}$};

                % Vector H D_2 u
                \draw[-stealth] (0,0,0) -- (0.22058188, -0.57508846,  0.78779242)
                node[above left] {$\bm{H} \bm{D}_2 \bm{u}$};

            \end{tikzpicture}
        \end{adjustbox}
    }
    \qquad
    \subfloat[] {
        \begin{adjustbox}{width=0.29\textwidth}
            \begin{tikzpicture}[tdplot_main_coords, scale = 2.25]

                % Create a point (P)
                \coordinate (P) at ({1/sqrt(3)},{1/sqrt(3)},{1/sqrt(3)});

                % Draw shaded circle
                \shade[ball color = SkyBlue,
                    opacity = 0.5
                ] (0,0,0) circle (1cm);

                % draw arcs 
                \tdplotsetrotatedcoords{0}{0}{0};
                \draw[dashed,
                    tdplot_rotated_coords,
                    darkgray
                ] (0,0,0) circle (1);

                % Axes in 3 d coordinate system
                \draw[-stealth] (-1.80,0,0) -- (1.80,0,0)
                node[below left] {$x$};

                \draw[-stealth] (0,-1.30,0) -- (0,1.30,0)
                node[below right] {$y$};

                \draw[-stealth] (0,0,-1.30) -- (0,0,1.30)
                node[above] {$z$};

                % Vector r
                \draw[-stealth] (0,0,0) -- (0.44366934, -0.44366934,  0.77866233)
                node[above right] {$\bm{r}$};

                % Vector w
                \draw[-stealth] (0,0,0) -- (-0.69631062, -0.69631062,  0.17407766)
                node[above right] {$\bm{w}$};

                % Vector v
                \draw[-stealth] (0,0,0) -- (0.87287156, -0.43643578,  0.21821789)
                node[above right] {$\bm{v}$};

            \end{tikzpicture}
        \end{adjustbox}
    }
    \caption{A visual representation for the roles of each matrix block in the SORF method. The first block $\bm{H} \bm{D}_1$ rotates $\bm{v}$ so that single dimension bears too much of the $l^2$ norm seen in panel (A). In panel (B) the second block $\bm{H} \bm{D}_2$ transforms vectors so that their image is near-orthogonal. Panel (C) shows that the projection of a random vector $\bm{r}$ onto two near-orthogonal vector $\bm{v},\bm{w}$ yields a near-independent vector.}
    \label{fig: SORF-vis}
\end{figure}
