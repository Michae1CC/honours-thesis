\subsection{Random Ortho-Matrices and Structured Orthogonal Random Matrices}\label{Section3.3}

The second method we shall consider for producing a transformation matrix also originates from Yu's {\it et al.} paper, which Choromanski {\it et al.} \cite{ChoromanskiKrzysztof2017TUEo} generalize as Random Ortho-Matrices (ROM). This second class of methods is motivated by creating transformation matrices with the same variance reductions as ORF with the added benefit time and memory savings. The transformation matrices generated using ROM take the form
\begin{equation} \label{eq: ROM-general}
    \bm{W}_{\text{ROM}} = \sqrt{d} \prod_{i=1}^{k} \bm{S} \bm{D}_{i}
\end{equation}
where $\bm{S} \in \RR^{D \times D}$ has orthogonal rows and $\bm{D} = \operatorname{diag} \left( \delta_1 , \ldots , \delta_D \right) \in \RR^{D \times D}$ where $\delta_i \stackrel{\text{iid}}{\sim} U \left( \left\{ -1, 1 \right\} \right)$, sometimes called the Radamach distribution. This matrix can be forced into a $\RR^{D \times d}$ sized matrix by simply extracting the first $d$ columns of $\bm{D}_1$. The matrix to take the role of $\bm{S}$ in virtually every application of ROM is the Hadamard matrix, defined in \ref{defe: Hadamard-Matrix}, which admits a fast $m \log (n)$ matrix multiplication for a size $m \times n$ Hadamard matrix called the Fast Walsh-Hadamard transform (FWHT) \cite{Fino1976UMTo}.

\begin{defe}[Hadamard Matrix] \label{defe: Hadamard-Matrix}
    The Hadamard matrix $\bm{H}_i \in \RR^{\left( 2^{i-1} \times 2^{i-1} \right)}$ is defined recursively as
    \[
        \bm{H}_i =
        \left\{
        \begin{array}{cc}
            \left[ 1 \right] & , i=1 \\
            \frac{1}{\sqrt{2}}
            \begin{bmatrix}
                \bm{H}_{i-1} & \bm{H}_{i-1}   \\
                \bm{H}_{i-1} & - \bm{H}_{i-1} \\
            \end{bmatrix}
                             & , i>1
        \end{array}
        \right. .
    \]
\end{defe}

Note that while Hadamard matrices are only defined for dimensions of exact powers of 2, other sizes can be constructed by removing portions of the matrix given in definition \ref{defe: Hadamard-Matrix} or by padding by $0$. This gives a concrete means for which one can generate a transformation matrix
\begin{equation} \label{eq: ROM-Hadamard}
    \sqrt{d} \prod_{i=1}^{k} \bm{H} \bm{D}_{i}
\end{equation}
where $\bm{H}$ is an appropriately sized Hadamard matrix. It is easy to check that the matrix generated by \ref{eq: ROM-Hadamard} shares the same expected rows norm lengths as $\bm{W}_{\text{ORF}}$ and thus enjoys the same variance reduction benefits.

Despite the wide use of the ROM method in various machine learning tasks \cite{ChoromanskiKrzysztof2017TUEo,AndoniAlexandr2015PaOL,ChoromanskiKrzysztof2020RAwP} a number of high-interest theoretical properties remain unsolved problems leaving many aspects of this method shrouded in mystery. Instead, much of what we understand about ROM's estimate capabilities comes from empirical analysis, although we will still cover a smaller number of important results that have been established.

Choromanski {\it et al.} \cite{ChoromanskiKrzysztof2017TUEo} show that there is diminishing returns, estimation wise, for choosing larger values of $k$ in equation \ref{eq: ROM-Hadamard}. They also show that choosing odd values of $k$ in \ref{eq: ROM-Hadamard} provide better estimates then its even $k-1$ and $k+1$ counterparts. For this reason a $k$ value of $3$ is usually chosen which yields the transformation matrix estimate given in equation \ref{eq: SORF-tm}. The method for constructing transformation matrices in this manner is referred to as Structured Orthogonal Random Feature (SORF).
\begin{equation} \label{eq: SORF-tm}
    \bm{W}_{\text{SORF}} = \sqrt{d} \bm{H} \bm{D}_{3} \bm{H} \bm{D}_{2} \bm{H} \bm{D}_{1}
\end{equation}
This is the same transformation matrix estimate that Yu {\it et al.} provides. Unfortunately using the SORF method in algorithm \ref{alg: RFF-algorithm} does not produce an unbiased estimate of the Gram matrix. However SORF does satisfy an asymptotic unbiased property
\begin{equation*}
    \left| \EE \left[ \left( \bm{\widetilde{K}}_{\text{SORF}} \right)_{ij} \right] - \EE \left[ \left( \bm{\widetilde{K}}_{\text{RFF}} \right)_{ij} \right] \right| \leq \frac{6 \tau}{\sqrt{d}}
\end{equation*}
where $\tau$ is again $\norm{\bm{x}_i - \bm{x}_j}_2 / \frac{\sigma}{\sqrt{2}}$ \cite{LiuFanghui2021RFfK}*{page 8}.

\begin{tikzpicture}[tdplot_main_coords, scale = 2.5]

    % Create a point (P)
    \coordinate (P) at ({1/sqrt(3)},{1/sqrt(3)},{1/sqrt(3)});

    % Draw shaded circle
    \shade[ball color = SkyBlue,
        opacity = 0.5
    ] (0,0,0) circle (1cm);

    % draw arcs 
    \tdplotsetrotatedcoords{0}{0}{0};
    \draw[dashed,
        tdplot_rotated_coords,
        darkgray
    ] (0,0,0) circle (1);

    % Axes in 3 d coordinate system
    \draw[-stealth] (-1.80,0,0) -- (1.80,0,0)
    node[below left] {$x$};

    \draw[-stealth] (0,-1.30,0) -- (0,1.30,0)
    node[below right] {$y$};

    \draw[-stealth] (0,0,-1.30) -- (0,0,1.30)
    node[above] {$z$};

    % Vector v
    \draw[-stealth] (0,0,0) -- (-0.19588084,  0.97940421,  0.04897021)
    node[above right] {$\bm{v}$};

    % Vector H D_1 v
    \draw[-stealth] (0,0,0) -- (-0.37139068,  0.74278135,  0.55708601)
    node[above right] {$\bm{H} \bm{D}_1 \bm{v}$};

\end{tikzpicture}

\begin{tikzpicture}[tdplot_main_coords, scale = 2.5]

    % Create a point (P)
    \coordinate (P) at ({1/sqrt(3)},{1/sqrt(3)},{1/sqrt(3)});

    % Axes in 3 d coordinate system
    \draw[-stealth] (-1.80,0,0) -- (1.80,0,0)
    node[below left] {$x$};

    \draw[-stealth] (0,-1.30,0) -- (0,1.30,0)
    node[below right] {$y$};

    \draw[-stealth] (0,0,-1.30) -- (0,0,1.30)
    node[above] {$z$};

    % Vector v
    \draw[-stealth] (0,0,0) -- (-0.37139068,  0.74278135,  0.55708601)
    node[above right] {$\bm{v}$};

    % Vector w
    \draw[-stealth] (0,0,0) -- (-0.55708601,  0.37139068,  0.74278135)
    node[above left] {$\bm{w}$};

    % Vector u
    \draw[-stealth] (0,0,0) -- (-0.46423834,  0.55708601,  0.64993368)
    node[above right] {$\bm{u}$};

    % Vector H D_2 v
    \draw[-stealth] (0,0,0) -- (0.86736677,-0.49766946,0)
    node[above left] {$\bm{H} \bm{D}_2 \bm{v}$};

    % Vector H D_2 w
    \draw[-stealth] (0,0,0) -- (-0.73,-0.57,-0.38)
    node[above left] {$\bm{H} \bm{D}_2 \bm{w}$};

    % Vector H D_2 u
    \draw[-stealth] (0,0,0) -- (0.22058188, -0.57508846,  0.78779242)
    node[above left] {$\bm{H} \bm{D}_2 \bm{u}$};

\end{tikzpicture}

Bojarski {\it et al.} \cite{BojarskiMariusz2016Saar}*{page 4} give an intuitive explanation for the roles of each of the different blocks $\bm{H} \bm{D}_1$, $\bm{H} \bm{D}_2$ and $\bm{H} \bm{D}_3$. Since the first block can be shown to satisfy
\[
    \PP \left[ \norm{\bm{H}\bm{D}_1 \bm{x}}_{\infty} > \frac{\log D}{\sqrt{D}} \right] \leq 2 d \exp \left( {-\frac{\log^2 D}{8}} \right), \quad \bm{x} \in \RR^D
\]
\cite{LiuFanghui2021RFfK}*{page 8} so that it can be thought as a "balancer" leaving no single dimension bearingtoo much of the $l^2$ norm. For the secnd block, the cost of using a structured matrix is the loss of independence. The role of the second block is to mitigate this effect by making similar input vectors near-orthogonal. Finally the third block controls the capacity of the entire structure by providing a vector of parameters. Near-independence is now implied by the near-orthogonality achieved by the proceeding block and the fact that the projections of the Gaussian vector or Radamacher vector onto "almost orthogonal directions" and "close to independent".